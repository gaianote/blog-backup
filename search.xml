<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python实现十大经典排序算法]]></title>
    <url>%2F2019%2F10%2F07%2Fpython%2F5.python%E7%AE%97%E6%B3%95%2F1%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[名词解释：n：数据规模k：“桶”的个数In-place：占用常数内存，不占用额外内存Out-place：占用额外内存稳定性：排序后2个相等键值的顺序和排序之前它们的顺序相同 1. 冒泡排序（Bubble Sort） 冒泡排序逻辑： 将第一位与第二位比较大小，如果第一位大于第二位，则第一位与第二位换序；然后将第二位与第三位比较大小，以此类推，直至第n-1位与第n位比较大小。 冒泡排序须知：冒泡排序每次找出一个最大的元素，因此需要遍历 n-1 次。还有一种优化算法，就是立一个flag，当在一趟序列遍历中元素没有发生交换，则证明该序列已经有序。但这种改进对于提升性能来说并没有什么太大作用。 什么时候最快（Best Cases）：当输入的数据已经是正序时。 什么时候最慢（Worst Cases）：当输入的数据是反序时。 冒泡排序动图演示： 冒泡排序 Python 代码实现：123456def bubbleSort(nums): for i in range(len(nums) - 1): # 遍历 len(nums)-1 次 for j in range(len(nums) - i - 1): # 已排好序的部分不用再次遍历 if nums[j] &gt; nums[j+1]: nums[j], nums[j+1] = nums[j+1], nums[j] # Python 交换两个数不用中间变量 return nums 2. 选择排序（Selection Sort） 选择排序须知：选择排序不受输入数据的影响，即在任何情况下时间复杂度不变。选择排序每次选出最小的元素，因此需要遍历 n-1 次。 选择排序动图演示： 选择排序 Python 代码实现：12345678def selectionSort(nums): for i in range(len(nums) - 1): # 遍历 len(nums)-1 次 minIndex = i for j in range(i + 1, len(nums)): if nums[j] &lt; nums[minIndex]: # 更新最小值索引 minIndex = j nums[i], nums[minIndex] = nums[minIndex], nums[i] # 把最小数交换到前面 return nums 3. 插入排序（Insertion Sort） 插入排序须知：插入排序如同打扑克一样，每次将后面的牌插到前面已经排好序的牌中。插入排序有一种优化算法，叫做拆半插入。因为前面是局部排好的序列，因此可以用折半查找的方法将牌插入到正确的位置，而不是从后往前一一比对。折半查找只是减少了比较次数，但是元素的移动次数不变，所以时间复杂度仍为 O(n^2) ！ 插入排序动图演示： 插入排序 Python 代码实现：12345678def insertionSort(nums): for i in range(len(nums) - 1): # 遍历 len(nums)-1 次 curNum, preIndex = nums[i+1], i # curNum 保存当前待插入的数 while preIndex &gt;= 0 and curNum &lt; nums[preIndex]: # 将比 curNum 大的元素向后移动 nums[preIndex + 1] = nums[preIndex] preIndex -= 1 nums[preIndex + 1] = curNum # 待插入的数的正确位置 return nums 希尔排序（Shell Sort） 希尔排序须知：希尔排序是插入排序的一种更高效率的实现。它与插入排序的不同之处在于，它会优先比较距离较远的元素。 【例子】对于待排序列 {44，12，59，36，62，43，94，7，35，52，85}，我们可设定增量序列为 {5，3，1}。 【解析】第一个增量为 5，因此 {44，43，85}、{12，94}、{59，7}、{36，35}、{62，52} 分别隶属于同一个子序列，子序列内部进行插入排序；然后选取第二个增量3，因此 {43，35，94，62}、{12，52，59，85}、{7，44，36} 分别隶属于同一个子序列；最后一个增量为 1，这一次排序相当于简单插入排序，但是经过前两次排序，序列已经基本有序，因此此次排序时间效率就提高了很多。希尔排序过程如下： 希尔排序的核心在于间隔序列的设定。既可以提前设定好间隔序列，也可以动态的定义间隔序列。动态定义间隔序列的算法是《算法（第4版》的合著者 Robert Sedgewick 提出的。在这里，我就使用了这种方法。 希尔排序 Python 代码实现：1234567891011121314def shellSort(nums): lens = len(nums) gap = 1 while gap &lt; lens // 3: gap = gap * 3 + 1 # 动态定义间隔序列 while gap &gt; 0: for i in range(gap, lens): curNum, preIndex = nums[i], i - gap # curNum 保存当前待插入的数 while preIndex &gt;= 0 and curNum &lt; nums[preIndex]: nums[preIndex + gap] = nums[preIndex] # 将比 curNum 大的元素向后移动 preIndex -= gap nums[preIndex + gap] = curNum # 待插入的数的正确位置 gap //= 3 # 下一个动态间隔 return nums 归并排序（Merge Sort） 归并排序须知：作为一种典型的分而治之思想的算法应用，归并排序的实现由两种方法： 自上而下的递归（所有递归的方法都可以用迭代重写，所以就有了第2种方法） 自下而上的迭代 和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是O(n log n）的时间复杂度。代价是需要额外的内存空间。 归并排序动图演示： 归并排序 Python 代码实现：123456789101112131415161718192021def mergeSort(nums): # 归并过程 def merge(left, right): result = [] # 保存归并后的结果 i = j = 0 while i &lt; len(left) and j &lt; len(right): if left[i] &lt;= right[j]: result.append(left[i]) i += 1 else: result.append(right[j]) j += 1 result = result + left[i:] + right[j:] # 剩余的元素直接添加到末尾 return result # 递归过程 if len(nums) &lt;= 1: return nums mid = len(nums) // 2 left = mergeSort(nums[:mid]) right = mergeSort(nums[mid:]) return merge(left, right) 快速排序（Quick Sort） 快速排序须知：又是一种分而治之思想在排序算法上的典型应用。本质上来看，快速排序应该算是在冒泡排序基础上的递归分治法。它是处理大数据最快的排序算法之一，虽然 Worst Case 的时间复杂度达到了 O(n²)，但是在大多数情况下都比平均时间复杂度为 O(n log n) 的排序算法表现要更好，因为 O(n log n) 记号中隐含的常数因子很小，而且快速排序的内循环比大多数排序算法都要短小，这意味着它无论是在理论上还是在实际中都要更快，比复杂度稳定等于 O(n log n) 的归并排序要小很多。所以，对绝大多数顺序性较弱的随机数列而言，快速排序总是优于归并排序。它的主要缺点是非常脆弱，在实现时要非常小心才能避免低劣的性能。 快速排序动图演示： 快速排序 Python 代码实现：1234567891011121314151617181920212223242526272829303132def quickSort(nums): # 这种写法的平均空间复杂度为 O(nlogn) if len(nums) &lt;= 1: return nums pivot = nums[0] # 基准值 left = [nums[i] for i in range(1, len(nums)) if nums[i] &lt; pivot] right = [nums[i] for i in range(1, len(nums)) if nums[i] &gt;= pivot] return quickSort(left) + [pivot] + quickSort(right)'''@param nums: 待排序数组@param left: 数组上界@param right: 数组下界'''def quickSort2(nums, left, right): # 这种写法的平均空间复杂度为 O(logn) # 分区操作 def partition(nums, left, right): pivot = nums[left] # 基准值 while left &lt; right: while left &lt; right and nums[right] &gt;= pivot: right -= 1 nums[left] = nums[right] # 比基准小的交换到前面 while left &lt; right and nums[left] &lt;= pivot: left += 1 nums[right] = nums[left] # 比基准大交换到后面 nums[left] = pivot # 基准值的正确位置，也可以为 nums[right] = pivot return left # 返回基准值的索引，也可以为 return right # 递归操作 if left &lt; right: pivotIndex = partition(nums, left, right) quickSort2(nums, left, pivotIndex - 1) # 左序列 quickSort2(nums, pivotIndex + 1, right) # 右序列 return nums 堆排序（Heap Sort） 堆排序须知：堆排序可以说是一种利用堆的概念来排序的选择排序。分为两种方法： 大根堆：每个节点的值都大于或等于其子节点的值，用于升序排列； 小根堆：每个节点的值都小于或等于其子节点的值，用于降序排列。 如下图所示，首先将一个无序的序列生成一个最大堆，如图（a）所示。接下来我们不需要将堆顶元素输出，只要将它与堆的最后一个元素对换位置即可，如图（b）所示。这时我们确知最后一个元素 99 一定是递增序列的最后一个元素，而且已经在正确的位置上。 现在问题变成了如何将剩余的元素重新生成一个最大堆——也很简单，只要依次自上而下进行过滤，使其符合最大堆的性质。图（c）是调整后形成的新的最大堆。要注意的是，99 已经被排除在最大堆之外，即在调整的时候，堆中元素的个数应该减 1 。结束第 1 轮调整后，再次将当前堆中的最后一个元素 22 与堆顶元素换位，如图（d）所示，再继续调整成新的最大堆……如此循环，直到堆中只剩 1 个元素，即可停止，得到一个从小到大排列的有序序列。 堆排序动图演示： 堆排序 Python 代码实现：123456789101112131415161718192021222324252627282930313233# 大根堆（从小打大排列）def heapSort(nums): # 调整堆 def adjustHeap(nums, i, size): # 非叶子结点的左右两个孩子 lchild = 2 * i + 1 rchild = 2 * i + 2 # 在当前结点、左孩子、右孩子中找到最大元素的索引 largest = i if lchild &lt; size and nums[lchild] &gt; nums[largest]: largest = lchild if rchild &lt; size and nums[rchild] &gt; nums[largest]: largest = rchild # 如果最大元素的索引不是当前结点，把大的结点交换到上面，继续调整堆 if largest != i: nums[largest], nums[i] = nums[i], nums[largest] # 第 2 个参数传入 largest 的索引是交换前大数字对应的索引 # 交换后该索引对应的是小数字，应该把该小数字向下调整 adjustHeap(nums, largest, size) # 建立堆 def builtHeap(nums, size): for i in range(len(nums)//2)[::-1]: # 从倒数第一个非叶子结点开始建立大根堆 adjustHeap(nums, i, size) # 对所有非叶子结点进行堆的调整 # print(nums) # 第一次建立好的大根堆 # 堆排序 size = len(nums) builtHeap(nums, size) for i in range(len(nums))[::-1]: # 每次根结点都是最大的数，最大数放到后面 nums[0], nums[i] = nums[i], nums[0] # 交换完后还需要继续调整堆，只需调整根节点，此时数组的 size 不包括已经排序好的数 adjustHeap(nums, 0, i) return nums # 由于每次大的都会放到后面，因此最后的 nums 是从小到大排列 计数排序（Counting Sort） 计数排序须知：计数排序要求输入数据的范围在 [0,N-1] 之间，则可以开辟一个大小为 N 的数组空间，将输入的数据值转化为键存储在该数组空间中，数组中的元素为该元素出现的个数。它是一种线性时间复杂度的排序。 计数排序动图演示： 计数排序 Python 代码实现：1234567891011def countingSort(nums): bucket = [0] * (max(nums) + 1) # 桶的个数 for num in nums: # 将元素值作为键值存储在桶中，记录其出现的次数 bucket[num] += 1 i = 0 # nums 的索引 for j in range(len(bucket)): while bucket[j] &gt; 0: nums[i] = j bucket[j] -= 1 i += 1 return nums 桶排序（Bucket Sort） 桶排序须知：桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。 为了使桶排序更加高效，我们需要做到这两点： 在额外空间充足的情况下，尽量增大桶的数量 使用的映射函数能够将输入的 N 个数据均匀的分配到 K 个桶中 同时，对于桶中元素的排序，选择何种比较排序算法对于性能的影响至关重要。 什么时候最快（Best Cases）：当输入的数据可以均匀的分配到每一个桶中 什么时候最慢（Worst Cases）：当输入的数据被分配到了同一个桶中 桶排序 Python 代码实现：12345678910111213141516def bucketSort(nums, defaultBucketSize = 5): maxVal, minVal = max(nums), min(nums) bucketSize = defaultBucketSize # 如果没有指定桶的大小，则默认为5 bucketCount = (maxVal - minVal) // bucketSize + 1 # 数据分为 bucketCount 组 buckets = [] # 二维桶 for i in range(bucketCount): buckets.append([]) # 利用函数映射将各个数据放入对应的桶中 for num in nums: buckets[(num - minVal) // bucketSize].append(num) nums.clear() # 清空 nums # 对每一个二维桶中的元素进行排序 for bucket in buckets: insertionSort(bucket) # 假设使用插入排序 nums.extend(bucket) # 将排序好的桶依次放入到 nums 中 return nums 基数排序（Radix Sort） 基数排序须知：基数排序是桶排序的一种推广，它所考虑的待排记录包含不止一个关键字。例如对一副牌的整理，可将每张牌看作一个记录，包含两个关键字：花色、面值。一般我们可以将一个有序列是先按花色划分为四大块，每一块中又再按面值大小排序。这时“花色”就是一张牌的“最主位关键字”，而“面值”是“最次位关键字”。 基数排序有两种方法： MSD （主位优先法）：从高位开始进行排序 LSD （次位优先法）：从低位开始进行排序 LSD基数排序动图演示： 基数排序 Python 代码实现：1234567891011121314151617# LSD Radix Sortdef radixSort(nums): mod = 10 div = 1 mostBit = len(str(max(nums))) # 最大数的位数决定了外循环多少次 buckets = [[] for row in range(mod)] # 构造 mod 个空桶 while mostBit: for num in nums: # 将数据放入对应的桶中 buckets[num // div % mod].append(num) i = 0 # nums 的索引 for bucket in buckets: # 将数据收集起来 while bucket: nums[i] = bucket.pop(0) # 依次取出 i += 1 div *= 10 mostBit -= 1 return nums 补充：外部排序 外部排序是指大文件排序，即待排序的数据记录以文件的形式存储在外存储器上。由于文件中的记录很多、信息容量庞大，所以整个文件所占据的存储单元往往会超过了计算机的内存量，因此，无法将整个文件调入内存中进行排序。于是，在排序过程中需进行多次的内外存之间的交换。在实际应用中，由于使用的外设不一致，通常可以分为磁盘文件排序和磁带文件排序两大类。 外部排序基本上由两个相对独立的阶段组成。首先，按可用内存大小，将外存上含 N 个记录的文件分成若干长度为 L(&lt;N) 的子文件，依次读入内存，利用内部排序算法进行排序。然后，将排序后的文件写入外存，通常将这些文件称为归并段（Run）或“顺串”；对这些归并段进行逐步归并，最终得到整个有序文件。可见外部排序的基本方法是归并排序法，下面的例子给出了一个简单的外部排序解决过程。 【例子】给定磁盘上有6大块记录需要排序，而计算机内存最多只能对3个记录块进行内排序，则外部排序的过程如下图所示。 【解析】首先将连续的3大块记录读入内存，用任何一种内部排序算法完成排序，再写回磁盘。经过2次3大块记录的内部排序，得到上图（a）的结果。然后另用一个可容纳6大块记录的周转盘，辅助最后的归并。方法是将内存分成3块，其中2块用于输入，1块用于输出，指定一个输入块只负责读取一个归并段中的记录，如上图（b）所示。归并步骤为： 当任一输入块为空时，归并暂停，将相应归并段中的一块信息写入内存将内存中2个输入块中的记录逐一归并入输出块当输出块写满时，归并暂停，将输出块中的记录写入周转盘如此可将2个归并段在周转盘上归并成一个有序的归并段。上例的解决方法是最简单的归并法，事实上外部排序的效率还可以进一步提高。要提高外排的效率，关键要解决以下4个问题： 如何减少归并轮数 如何有效安排内存中的输入、输出块，使得机器的并行处理能力被最大限度利用 如何有效生成归并段 如何将归并段进行有效归并 针对这四大问题，人们设计了多种解决方案，例如釆用多路归并取代简单的二路归并，就可以减少归并轮数；例如在内存中划分出2个输出块，而不是只用一个，就可以设计算法使得归并排序不会因为磁盘的写操作而暂停，达到归并和写周转盘同时并行的效果；例如通过一种“败者树”的数据结构，可以一次生成2倍于内存容量的归并段；例如利用哈夫曼树的贪心策略选择归并次序，可以耗费最少的磁盘读写时间等。 其他一些比较： 基数排序 vs 计数排序 vs 桶排序这三种排序算法都利用了桶的概念，但对桶的使用方法上有明显差异：基数排序：根据键值的每位数字来分配桶计数排序：每个桶只存储单一键值桶排序：每个桶存储一定范围的数值 哪些排序算法可以在未结束排序时找出第 k 大元素？冒泡、选择、堆排序、快排（想想为什么？） 总结：本章用 Python3 语言实现了经典的十大排序算法，对它们的优缺点、复杂度等方面进行了详细的比较。最后，还对外部排序进行了简单的介绍。 快排、归并排序、堆排序、计数排序（桶排序）一般是面试中常问的题目，笔者觉得其中比较难的是堆排序，因为涉及建堆、调整堆的过程，手写该算法还是有一定难度的。 笔者在写文章时，难免有些地方会出现一些表述不清的问题，欢迎指正！]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python处理音频文件 - pydub]]></title>
    <url>%2F2019%2F10%2F07%2Fpython%2F3.python%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93%2Fpython%E5%A4%84%E7%90%86%E9%9F%B3%E9%A2%91%E6%96%87%E4%BB%B6-pydub%2F</url>
    <content type="text"><![CDATA[Pydub是一款用来处理音频的，好用的第三方python库。 1. 安装1. 安装依赖ubuntu 123apt-get install libav-tools libavcodec-extra# ORapt-get install ffmpeg libavcodec-extra centos7 1234yum -y install epel-releaserpm --import http://li.nux.ro/download/nux/RPM-GPG-KEY-nux.rorpm -Uvh http://li.nux.ro/download/nux/dextop/el7/x86_64/nux-dextop-release-0-1.el7.nux.noarch.rpmyum -y install ffmpeg ffmpeg-devel 2. 安装库1pip install pydub 2. 打开音频文件可以通过AudioSegment.from_file的方式打开文件并生成实例，由此对音频进行操作 12345678910from pydub import AudioSegmentsong = AudioSegment.from_wav("never_gonna_give_you_up.wav")song = AudioSegment.from_mp3("never_gonna_give_you_up.mp3")song = AudioSegment.from_ogg("never_gonna_give_you_up.ogg")song = AudioSegment.from_flv("never_gonna_give_you_up.flv")mp4= AudioSegment.from_file("never_gonna_give_you_up.mp4", "mp4")wma = AudioSegment.from_file("never_gonna_give_you_up.wma", "wma")aac = AudioSegment.from_file("never_gonna_give_you_up.aiff", "aac") 3. 分割音频可以使用切片的方式切割音频，比如下文的示例，song[:1000*30]表示截取后三十秒的音频 12345from pydub import AudioSegmentsong = AudioSegment.from_wav("never_gonna_give_you_up.wav")slice_song = song[:1000*30]slice_song.export("slice_song.mp3",format="mp3",bitrate="192k") 官方文档]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F10%2F07%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E6%B7%B1%E6%8B%B7%E8%B4%9D%E4%B8%8E%E6%B5%85%E6%8B%B7%E8%B4%9D%2F</url>
    <content type="text"><![CDATA[123title: python深拷贝与浅拷贝date: 2019-09-23 13:58:35tags: python 1. 深拷贝与浅拷贝首先，对赋值操作我们要有以下认识： 赋值是将一个对象的地址赋值给一个变量，让变量指向该地址 修改不可变对象（str、tuple）需要开辟新的空间 修改可变对象（list等）不需要开辟新的空间 其次，赋值与深拷贝浅拷贝的区别如下: 赋值：修改原对象新对象也会跟着变化；新对象与原对象完全一致 浅拷贝：仅仅复制了容器中元素的地址,如果原对象的可变对象子元素发生了改变，由于没有开辟新的空间，新对象的可变对象子元素仍会发生改变 深拷贝: 完全拷贝了一个副本，容器内部元素地址都不一样；新对象与原对象完全无关 1234567891011121314import copya = [1,['a','b']]b = a c = copy.copy(a)d = copy.deepcopy(a)# opreate aa[0] = 2a.append(5)a[1].append('c')# resulta= [2, ['a', 'b', 'c'], 5]b= [2, ['a', 'b', 'c'], 5]c= [1, ['a', 'b', 'c']]d= [1, ['a', 'b']] 2. 传值与传引用在 C/C++ 中，传值和传引用是函数参数传递的两种方式，在Python中参数是如何传递的？回答这个问题前，不如先来看两段代码。 代码段1： 1234567def foo(arg): arg = 2 print(arg)a = 1foo(a) # 输出：2print(a) # 输出：1 看了代码段1的同学可能会说参数是值传递。 代码段2： 123456789def bar(args): args.append(1)b = []print(b)# 输出：[]print(id(b)) # 输出：4324106952bar(b)print(b) ＃ 输出：[1]print(id(b)) # 输出：4324106952 看了代码段2，这时可能又有人会说，参数是传引用，那么问题来了，参数传递到底是传值还是传引用或者两者都不是？为了把这个问题弄清楚，先了解 Python 中变量与对象之间的关系。 1. 变量与对象Python 中一切皆为对象，数字是对象，列表是对象，函数也是对象，任何东西都是对象。而变量是对象的一个引用（又称为名字或者标签），对象的操作都是通过引用来完成的。例如，[]是一个空列表对象，变量 a 是该对象的一个引用 12a = []a.append(1) 在 Python 中，「变量」更准确叫法是「名字」，赋值操作 = 就是把一个名字绑定到一个对象上。就像给对象添加一个标签。 1a = 1 整数 1 赋值给变量 a 就相当于是在整数1上绑定了一个 a 标签。 1a = 2 整数 2 赋值给变量 a，相当于把原来整数 1 身上的 a 标签撕掉，贴到整数 2 身上。 1b = a 把变量 a 赋值给另外一个变量 b，相当于在对象 2 上贴了 a，b 两个标签，通过这两个变量都可以对对象 2 进行操作。 变量本身没有类型信息，类型信息存储在对象中，这和C/C++中的变量有非常大的出入（C中的变量是一段内存区域） 2. 函数参数Python 函数中，参数的传递本质上是一种赋值操作，而赋值操作是一种名字到对象的绑定过程，清楚了赋值和参数传递的本质之后，现在再来分析前面两段代码。 1234567def foo(arg): arg = 2 print(arg)a = 1foo(a) # 输出：2print(a) # 输出：1 在代码段1中，变量 a 绑定了 1，调用函数 foo(a) 时，相当于给参数 arg 赋值 arg=1，这时两个变量都绑定了 1。在函数里面 arg 重新赋值为 2 之后，相当于把 1 上的 arg 标签撕掉，贴到 2 身上，而 1 上的另外一个标签 a 一直存在。因此 print(a) 还是 1。 再来看一下代码段2 123456789def bar(args): args.append(1)b = []print(b)# 输出：[]print(id(b)) # 输出：4324106952bar(b)print(b) ＃ 输出：[1]print(id(b)) # 输出：4324106952 执行 append 方法前 b 和 arg 都指向（绑定）同一个对象，执行 append 方法时，并没有重新赋值操作，也就没有新的绑定过程，append 方法只是对列表对象插入一个元素，对象还是那个对象，只是对象里面的内容变了。因为 b 和 arg 都是绑定在同一个对象上，执行 b.append 或者 arg.append 方法本质上都是对同一个对象进行操作，因此 b 的内容在调用函数后发生了变化（但id没有变，还是原来那个对象） 最后，回到问题本身，究竟是是传值还是传引用呢？说传值或者传引用都不准确。非要安一个确切的叫法的话，叫传对象（call by object）。如果作为面试官，非要考察候选人对 Python 函数参数传递掌握与否，与其讨论字面上的意思，还不如来点实际代码。 show me the code123def bad_append(new_item, a_list=[]): a_list.append(new_item) return a_list 这段代码是初学者最容易犯的错误，用可变(mutable)对象作为参数的默认值。函数定义好之后，默认参数 a_list 就会指向（绑定）到一个空列表对象，每次调用函数时，都是对同一个对象进行 append 操作。因此这样写就会有潜在的bug，同样的调用方式返回了不一样的结果。 1234&gt;&gt;&gt; print bad_append('one')['one']&gt;&gt;&gt; print bad_append('one')['one', 'one'] 而正确的方式是，把参数默认值指定为None 12345def good_append(new_item, a_list=None): if a_list is None: a_list = [] a_list.append(new_item) return a_list 参考：http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html#other-languages-have-variables]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F10%2F07%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F1.python%E4%B8%8E%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Python系列干货之——Python与设计模式阿里云云栖社区 一、创建类设计模式前言 什么样的程序员是一个好的程序员？学会很多门编程语言，就是一个好的程序员了么？事实上，学会一门编程语言不是一件很难的事，而“学会”一门编程语言是非常难的一件事。前一个“会”强调“能”，懂语法，能写简单的程序就算是前者的“会”了；后一个“会”，强调“精”，显然，光能写出“Hello World”并不算是后者的“会”，能熟练应用，并用编程语言解决各种问题，才算是真正的“会”。 点击阅读详情 1、Python与设计模式–单例模式 总线是计算机各种功能部件或者设备之间传送数据、控制信号等信息的公共通信解决方案之一。现假设有如下场景：某中央处理器（CPU）通过某种协议总线与一个信号灯相连，信号灯有64种颜色可以设置，中央处理器上运行着三个线程，都可以对这个信号灯进行控制，并且可以独立设置该信号灯的颜色。抽象掉协议细节（用打印表示），如何实现线程对信号等的控制逻辑。 加线程锁进行控制，无疑是最先想到的方法，但各个线程对锁的控制，无疑加大了模块之间的耦合。下面，我们就用设计模式中的单例模式，来解决这个问题。 什么是单例模式？单例模式是指：保证一个类仅有一个实例，并提供一个访问它的全局访问点。具体到此例中，总线对象，就是一个单例，它仅有一个实例，各个线程对总线的访问只有一个全局访问点，即惟一的实例。 点击阅读详情 2、Python与设计模式–工厂类相关模式 想必大家一定见过类似于麦当劳自助点餐台一类的点餐系统吧。在一个大的触摸显示屏上，有三类可以选择的上餐品：汉堡等主餐、小食、饮料。当我们选择好自己需要的食物，支付完成后，订单就生成了。下面，我们用今天的主角–工厂模式–来生成这些食物的逻辑主体。 点击阅读详情 3、Python与设计模式–建造者模式 今天的例子，还是上一次谈到的快餐点餐系统。只不过，今天我们从订单的角度来构造这个系统。 点击阅读详情 4、Python与设计模式–原型模式 大家如果用过类似于Photoshop的平面设计软件，一定都知道图层的概念。图层概念的提出，使得设计、图形修改等操作更加便利。设计师既可以修改和绘制当前图像对象，又可以保留其它图像对象，逻辑清晰，且可以及时得到反馈。本节内容，将以图层为主角，介绍原型模式。 点击阅读详情 二、结构类设计模式1、Python与设计模式–代理模式 代理模式是一种使用频率非常高的模式，在多个著名的开源软件和当前多个著名的互联网产品后台程序中都有所应用。下面我们用一个抽象化的简单例子，来说明代理模式。 点击阅读详情 2、Python与设计模式–装饰器模式 又提到了那个快餐点餐系统，不过今天我们只以其中的一个类作为主角：饮料类。 除了基本配置，快餐店卖可乐时，可以选择加冰，如果加冰的话，要在原价上加0.3元；卖牛奶时，可以选择加糖，如果加糖的话，要原价上加0.5元。怎么解决这样的问题？可以选择装饰器模式来解决这一类的问题。 点击阅读详情 3、Python与设计模式–适配器模式 假设某公司A与某公司B需要合作，公司A需要访问公司B的人员信息，但公司A与公司B协议接口不同，该如何处理？ 点击阅读详情 4、Python与设计模式–门面模式 门面模式也叫外观模式，定义如下：要求一个子系统的外部与其内部的通信必须通过一个统一的对象进行。门面模式提供一个高层次的接口，使得子系统更易于使用。门面模式注重“统一的对象”，也就是提供一个访问子系统的接口。门面模式与之前说过的模板模式有类似的地方，都是对一些需要重复方法的封装。但从本质上来说，是不同的。模板模式是对类本身的方法的封装，其被封装的方法也可以单独使用；而门面模式，是对子系统的封装，其被封装的接口理论上是不会被单独提出来用的。 点击阅读详情 5、Python与设计模式–组合模式 组合模式也叫作部分-整体模式，其定义如下：将对象组合成树形结构以表示“部分”和“整体”的层次结构，使得用户对单个对象和组合对象的使用具有一致性。 点击阅读详情 6、Python与设计模式–享元模式 享元模式定义如下：使用共享对象支持大量细粒度对象。大量细粒度的对象的支持共享，可能会涉及这些对象的两类信息：内部状态信息和外部状态信息。内部状态信息就是可共享出来的信息，它们存储在享元对象内部，不会随着特定环境的改变而改变；外部状态信息就不可共享的信息了。享元模式中只包含内部状态信息，而不应该包含外部状态信息。这点在设计业务架构时，应该有所考虑。 点击阅读详情 7、Python与设计模式–桥梁模式 桥梁模式又叫桥接模式，定义如下：将抽象与实现解耦（注意此处的抽象和实现，并非抽象类和实现类的那种关系，而是一种角色的关系，这里需要好好区分一下），可以使其独立变化。在形如上例中，Pen只负责画，但没有形状，它终究是不知道要画什么的，所以我们把它叫做抽象化角色；而Shape是具体的形状，我们把它叫做实现化角色。抽象化角色和实现化角色是解耦的，这也就意味着，所谓的桥，就是抽象化角色的抽象类和实现化角色的抽象类之间的引用关系。 点击阅读详情 三、行为类设计模式1、Python与设计模式–策略模式 假设某司维护着一些客户资料，需要在该司有新产品上市或者举行新活动时通知客户。现通知客户的方式有两种：短信通知、邮件通知。应如何设计该系统的客户通知部分？为解决该问题，我们先构造客户类，包括客户常用的联系方式和基本信息，同时也包括要发送的内容。 点击阅读详情 2、Python与设计模式–责任链模式 假设有这么一个请假系统：员工若想要请3天以内（包括3天的假），只需要直属经理批准就可以了；如果想请3-7天，不仅需要直属经理批准，部门经理需要最终批准；如果请假大于7天，不光要前两个经理批准，也需要总经理最终批准。类似的系统相信大家都遇到过，那么该如何实现呢？ 点击阅读详情 3、Python与设计模式–命令模式 又是一个点餐系统（原谅作者的吃货属性）。不过这次的点餐系统是个饭店的点餐系统。饭店的点餐系统有什么不同嘛？大伙想想看，在大多数饭店中，当服务员已经接到顾客的点单，录入到系统中后，根据不同的菜品，会有不同的后台反应。比如，饭店有凉菜间、热菜间、主食间，那当服务员将菜品录入到系统中后，凉菜间会打印出顾客所点的凉菜条目，热菜间会打印出顾客所点的热菜条目，主食间会打印出主食条目。那这个系统的后台模式该如何设计？ 点击阅读详情 4、Python与设计模式–中介者模式 有一个手机仓储管理系统，使用者有三方：销售、仓库管理员、采购。需求是：销售一旦达成订单，销售人员会通过系统的销售子系统部分通知仓储子系统，仓储子系统会将可出仓手机数量减少，同时通知采购管理子系统当前销售订单；仓储子系统的库存到达阈值以下，会通知销售子系统和采购子系统，并督促采购子系统采购；采购完成后，采购人员会把采购信息填入采购子系统，采购子系统会通知销售子系统采购完成，并通知仓库子系统增加库存。 从需求描述来看，每个子系统都和其它子系统有所交流，在设计系统时，如果直接在一个子系统中集成对另两个子系统的操作，一是耦合太大，二是不易扩展。为解决这类问题，我们需要引入一个新的角色-中介者-来将“网状结构”精简为“星形结构”。 点击阅读详情 5、Python与设计模式–模板模式 投资股票是种常见的理财方式，我国股民越来越多，实时查询股票的需求也越来越大。今天，我们通过一个简单的股票查询客户端来认识一种简单的设计模式：模板模式。 点击阅读详情 6、Python与设计模式–迭代器模式 今天的主角是迭代器模式。在python中，迭代器并不用举太多的例子，因为python中的迭代器应用实在太多了（不管是python还是其它很多的编程语言中，实际上迭代器都已经纳入到了常用的库或者包中）。而且在当前，也几乎没有人专门去开发一个迭代器，而是直接去使用list、string、set、dict等python可迭代对象，或者直接使用iter和next函数来实现迭代器。 点击阅读详情 7、Python与设计模式–访问者模式 假设一个药房，有一些大夫，一个药品划价员和一个药房管理员，它们通过一个药房管理系统组织工作流程。大夫开出药方后，药品划价员确定药品是否正常，价格是否正确；通过后药房管理员进行开药处理。该系统可以如何实现？最简单的想法，是分别用一个一个if…else…把划价员处理流程和药房管理流程实现，这样做的问题在于，扩展性不强，而且单一性不强，一旦有新药的加入或者划价流程、开药流程有些变动，会牵扯比较多的改动。今天介绍一种解决这类问题的模式：访问者模式。 点击阅读详情 8、Python与设计模式–观察者模式 在门面模式中，我们提到过火警报警器。在当时，我们关注的是通过封装减少代码重复。而今天，我们将从业务流程的实现角度，来再次实现该火警报警器。 点击阅读详情 9、Python与设计模式–解释器模式 要开发一个自动识别谱子的吉他模拟器，达到录入谱即可按照谱发声的效果。除了发声设备外（假设已完成），最重要的就是读谱和译谱能力了。分析其需求，整个过程大致上分可以分为两部分：根据规则翻译谱的内容；根据翻译的内容演奏。我们用一个解释器模型来完成这个功能。 点击阅读详情 10、Python与设计模式–备忘录模式 打过游戏的朋友一定知道，大多数游戏都有保存进度的功能，如果一局游戏下来，忘保存了进度，那么下次只能从上次进度点开始重新打了。一般情况下，保存进度是要存在可持久化存储器上，本例中先以保存在内存中来模拟实现该场景的情形。 点击阅读详情 11、Python与设计模式–状态模式 电梯在我们周边随处可见，电梯的控制逻辑中心是由电梯控制器实现的。电梯的控制逻辑，即使简单点设计，把状态分成开门状态，停止状态和运行状态，操作分成开门、关门、运行、停止，那流程也是很复杂的。首先，开门状态不能开门、运行、停止；停止状态不能关门，停止；运行状态不能开门、关门、运行。要用一个一个if…else…实现，首先代码混乱，不易维护；二是不易扩展。至于各种设计原则什么的…… 那该如何实现？在上边的逻辑中，每个操作仅仅是一个操作，状态切换与操作是分离的，这也造成后来操作和状态“相互配合”的“手忙脚乱”。如果把状态抽象成一个类，每个状态为一个子类，每个状态实现什么操作，不实现什么操作，仅仅在这个类中具体实现就可以了。 点击阅读详情 更多技术干货敬请关注云栖社区知乎机构号：阿里云云栖社区 - 知乎]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F10%2F07%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F%E5%B8%B8%E8%A7%81%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F10%2F07%2Fpython%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Python实现十大经典排序算法- 简书]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F10%2F07%2Fpython%2F%E6%8A%93%E5%8C%85%2F</url>
    <content type="text"><![CDATA[https://www.wireshark.org/]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F10%2F07%2Fpython%2F5.python%E7%AE%97%E6%B3%95%2F%E7%AE%97%E6%B3%95%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E4%B8%8E%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[https://zhuanlan.zhihu.com/p/50479555]]></content>
  </entry>
  <entry>
    <title><![CDATA[REST接口设计规范]]></title>
    <url>%2F2019%2F09%2F27%2F%E5%89%8D%E7%AB%AF%2FREST%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[1. 理解RESTful架构越来越多的人开始意识到，网站即软件，而且是一种新型的软件。 这种”互联网软件”采用客户端/服务器模式，建立在分布式体系上，通过互联网通信，具有高延时（high latency）、高并发等特点。 网站开发，完全可以采用软件开发的模式。但是传统上，软件和网络是两个不同的领域，很少有交集；软件开发主要针对单机环境，网络则主要研究系统之间的通信。互联网的兴起，使得这两个领域开始融合，现在我们必须考虑，如何开发在互联网环境中使用的软件。 RESTful架构，就是目前最流行的一种互联网软件架构。它结构清晰、符合标准、易于理解、扩展方便，所以正得到越来越多网站的采用。 但是，到底什么是RESTful架构，并不是一个容易说清楚的问题。下面，我就谈谈我理解的RESTful架构。 1. 起源REST这个词，是Roy Thomas Fielding在他2000年的博士论文中提出的。 Fielding是一个非常重要的人，他是HTTP协议（1.0版和1.1版）的主要设计者、Apache服务器软件的作者之一、Apache基金会的第一任主席。所以，他的这篇论文一经发表，就引起了关注，并且立即对互联网开发产生了深远的影响。 他这样介绍论文的写作目的： “本文研究计算机科学两大前沿—-软件和网络—-的交叉点。长期以来，软件研究主要关注软件设计的分类、设计方法的演化，很少客观地评估不同的设计选择对系统行为的影响。而相反地，网络研究主要关注系统之间通信行为的细节、如何改进特定通信机制的表现，常常忽视了一个事实，那就是改变应用程序的互动风格比改变互动协议，对整体表现有更大的影响。我这篇文章的写作目的，就是想在符合架构原理的前提下，理解和评估以网络为基础的应用软件的架构设计，得到一个功能强、性能好、适宜通信的架构。“ 2. 名称Fielding将他对互联网软件的架构原则，定名为REST，即Representational State Transfer的缩写。我对这个词组的翻译是”表现层状态转化”。 如果一个架构符合REST原则，就称它为RESTful架构。 要理解RESTful架构，最好的方法就是去理解Representational State Transfer这个词组到底是什么意思，它的每一个词代表了什么涵义。如果你把这个名称搞懂了，也就不难体会REST是一种什么样的设计。 3. 资源（Resources）REST的名称”表现层状态转化”中，省略了主语。”表现层”其实指的是”资源”（Resources）的”表现层”。 所谓”资源”，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。你可以用一个URI（统一资源定位符）指向它，每种资源对应一个特定的URI。要获取这个资源，访问它的URI就可以，因此URI就成了每一个资源的地址或独一无二的识别符。 所谓”上网”，就是与互联网上一系列的”资源”互动，调用它的URI。 4. 表现层（Representation）“资源”是一种信息实体，它可以有多种外在表现形式。我们把”资源”具体呈现出来的形式，叫做它的”表现层”（Representation）。 比如，文本可以用txt格式表现，也可以用HTML格式、XML格式、JSON格式表现，甚至可以采用二进制格式；图片可以用JPG格式表现，也可以用PNG格式表现。 URI只代表资源的实体，不代表它的形式。严格地说，有些网址最后的”.html”后缀名是不必要的，因为这个后缀名表示格式，属于”表现层”范畴，而URI应该只代表”资源”的位置。它的具体表现形式，应该在HTTP请求的头信息中用Accept和Content-Type字段指定，这两个字段才是对”表现层”的描述。 5. 状态转化（State Transfer）访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。 互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生”状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是”表现层状态转化”。 客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。 6. 综述综合上面的解释，我们总结一下什么是RESTful架构： （1）每一个URI代表一种资源； （2）客户端和服务器之间，传递这种资源的某种表现层； （3）客户端通过四个HTTP动词，对服务器端资源进行操作，实现”表现层状态转化”。 2. RESTful API 设计指南网络应用程序，分为前端和后端两个部分。当前的发展趋势，就是前端设备层出不穷（手机、平板、桌面电脑、其他专用设备……）。 因此，必须有一种统一的机制，方便不同的前端设备与后端进行通信。这导致API构架的流行，甚至出现“API First”的设计思想。RESTful API是目前比较成熟的一套互联网应用程序的API设计理论。我以前写过一篇《理解RESTful架构》，探讨如何理解这个概念。 今天，我将介绍RESTful API的设计细节，探讨如何设计一套合理、好用的API。我的主要参考了两篇文章（1，2）。 1. 协议API与用户的通信协议，总是使用HTTPs协议。 2. 域名应该尽量将API部署在专用域名之下。 1https://api.example.com 如果确定API很简单，不会有进一步扩展，可以考虑放在主域名下。 1https://example.org/api/ 3. 版本（Versioning）应该将API的版本号放入URL。 1https://api.example.com/v1/ 另一种做法是，将版本号放在HTTP头信息中，但不如放入URL方便和直观。Github采用这种做法。 4. 路径（Endpoint）路径又称”终点”（endpoint），表示API的具体网址。 在RESTful架构中，每个网址代表一种资源（resource），所以网址中不能有动词，只能有名词，而且所用的名词往往与数据库的表格名对应。一般来说，数据库中的表都是同种记录的”集合”（collection），所以API中的名词也应该使用复数。 举例来说，有一个API提供动物园（zoo）的信息，还包括各种动物和雇员的信息，则它的路径应该设计成下面这样。 https://api.example.com/v1/zoos https://api.example.com/v1/animals https://api.example.com/v1/employees 5. HTTP动词对于资源的具体操作类型，由HTTP动词表示。 常用的HTTP动词有下面五个（括号里是对应的SQL命令）。 GET（SELECT）：从服务器取出资源（一项或多项）。 POST（CREATE）：在服务器新建一个资源。 PUT（UPDATE）：在服务器更新资源（客户端提供改变后的完整资源）。 PATCH（UPDATE）：在服务器更新资源（客户端提供改变的属性）。 DELETE（DELETE）：从服务器删除资源。 还有两个不常用的HTTP动词。 HEAD：获取资源的元数据。 OPTIONS：获取信息，关于资源的哪些属性是客户端可以改变的。 下面是一些例子。 GET /zoos：列出所有动物园 POST /zoos：新建一个动物园 GET /zoos/ID：获取某个指定动物园的信息 PUT /zoos/ID：更新某个指定动物园的信息（提供该动物园的全部信息） PATCH /zoos/ID：更新某个指定动物园的信息（提供该动物园的部分信息） DELETE /zoos/ID：删除某个动物园 GET /zoos/ID/animals：列出某个指定动物园的所有动物 DELETE /zoos/ID/animals/ID：删除某个指定动物园的指定动物 6. 过滤信息（Filtering）如果记录数量很多，服务器不可能都将它们返回给用户。API应该提供参数，过滤返回结果。 下面是一些常见的参数。 ?limit=10：指定返回记录的数量 ?offset=10：指定返回记录的开始位置。 ?page=2&amp;per_page=100：指定第几页，以及每页的记录数。 ?sortby=name&amp;order=asc：指定返回结果按照哪个属性排序，以及排序顺序。 ?animal_type_id=1：指定筛选条件 参数的设计允许存在冗余，即允许API路径和URL参数偶尔有重复。比如，GET /zoo/ID/animals 与 GET /animals?zoo_id=ID 的含义是相同的。 7. 状态码（Status Codes）服务器向用户返回的状态码和提示信息，常见的有以下一些（方括号中是该状态码对应的HTTP动词）。 200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。 201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。 202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务） 204 NO CONTENT - [DELETE]：用户删除数据成功。 400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。 401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。 403 Forbidden - [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。 404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。 406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。 410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。 422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。 500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。 状态码的完全列表参见这里。 8. 错误处理（Error handling）如果状态码是4xx，就应该向用户返回出错信息。一般来说，返回的信息中将error作为键名，出错信息作为键值即可。 123&#123; error: "Invalid API key"&#125; 9. 返回结果针对不同操作，服务器向用户返回的结果应该符合以下规范。 GET /collection：返回资源对象的列表（数组） GET /collection/resource：返回单个资源对象 POST /collection：返回新生成的资源对象 PUT /collection/resource：返回完整的资源对象 PATCH /collection/resource：返回完整的资源对象 DELETE /collection/resource：返回一个空文档 10. Hypermedia APIRESTful API最好做到Hypermedia，即返回结果中提供链接，连向其他API方法，使得用户不查文档，也知道下一步应该做什么。 比如，当用户向api.example.com的根目录发出请求，会得到这样一个文档。 123456&#123;"link": &#123; "rel": "collection https://www.example.com/zoos", "href": "https://api.example.com/zoos", "title": "List of zoos", "type": "application/vnd.yourformat+json"&#125;&#125; 上面代码表示，文档中有一个link属性，用户读取这个属性就知道下一步该调用什么API了。rel表示这个API与当前网址的关系（collection关系，并给出该collection的网址），href表示API的路径，title表示API的标题，type表示返回类型。 Hypermedia API的设计被称为HATEOAS。Github的API就是这种设计，访问api.github.com会得到一个所有可用API的网址列表。 12345&#123; "current_user_url": "https://api.github.com/user", "authorizations_url": "https://api.github.com/authorizations", // ...&#125; 从上面可以看到，如果想获取当前用户的信息，应该去访问api.github.com/user，然后就得到了下面结果。 1234&#123; "message": "Requires authentication", "documentation_url": "https://developer.github.com/v3"&#125; 上面代码表示，服务器给出了提示信息，以及文档的网址。 11.其他（1）API的身份认证应该使用OAuth 2.0框架。 （2）服务器返回的数据格式，应该尽量使用JSON，避免使用XML。 3. RESTful API 最佳实践RESTful 是目前最流行的 API 设计规范，用于 Web 数据接口的设计。 它的大原则容易把握，但是细节不容易做对。本文总结 RESTful 的设计细节，介绍如何设计出易于理解和使用的 API。 1. URL 设计1.1 动词 + 宾语RESTful 的核心思想就是，客户端发出的数据操作指令都是”动词 + 宾语”的结构。比如，GET /articles这个命令，GET是动词，/articles是宾语。 动词通常就是五种 HTTP 方法，对应 CRUD 操作。 GET：读取（Read） POST：新建（Create） PUT：更新（Update） PATCH：更新（Update），通常是部分更新 DELETE：删除（Delete） 根据 HTTP 规范，动词一律大写。 1.2 动词的覆盖有些客户端只能使用GET和POST这两种方法。服务器必须接受POST模拟其他三个方法（PUT、PATCH、DELETE）。 这时，客户端发出的 HTTP 请求，要加上X-HTTP-Method-Override属性，告诉服务器应该使用哪一个动词，覆盖POST方法。 12POST /api/Person/4 HTTP/1.1X-HTTP-Method-Override: PUT 上面代码中，X-HTTP-Method-Override指定本次请求的方法是PUT，而不是POST。 1.3 宾语必须是名词宾语就是 API 的 URL，是 HTTP 动词作用的对象。它应该是名词，不能是动词。比如，/articles这个 URL 就是正确的，而下面的 URL 不是名词，所以都是错误的。 /getAllCars /createNewCar /deleteAllRedCars 1.4 复数 URL既然 URL 是名词，那么应该使用复数，还是单数？ 这没有统一的规定，但是常见的操作是读取一个集合，比如GET /articles（读取所有文章），这里明显应该是复数。 为了统一起见，建议都使用复数 URL，比如GET /articles/2要好于GET /article/2。 1.5 避免多级 URL常见的情况是，资源需要多级分类，因此很容易写出多级的 URL，比如获取某个作者的某一类文章。 1GET /authors/12/categories/2 这种 URL 不利于扩展，语义也不明确，往往要想一会，才能明白含义。 更好的做法是，除了第一级，其他级别都用查询字符串表达。 1GET /authors/12?categories=2 下面是另一个例子，查询已发布的文章。你可能会设计成下面的 URL。 1GET /articles/published 查询字符串的写法明显更好。 1GET /articles?published=true 2. 状态码2.1 状态码必须精确客户端的每一次请求，服务器都必须给出回应。回应包括 HTTP 状态码和数据两部分。 HTTP 状态码就是一个三位数，分成五个类别。 1xx：相关信息 2xx：操作成功 3xx：重定向 4xx：客户端错误 5xx：服务器错误 这五大类总共包含100多种状态码，覆盖了绝大部分可能遇到的情况。每一种状态码都有标准的（或者约定的）解释，客户端只需查看状态码，就可以判断出发生了什么情况，所以服务器应该返回尽可能精确的状态码。 API 不需要1xx状态码，下面介绍其他四类状态码的精确含义。 2.2 2xx 状态码200状态码表示操作成功，但是不同的方法可以返回更精确的状态码。 GET: 200 OK POST: 201 Created PUT: 200 OK PATCH: 200 OK DELETE: 204 No Content 上面代码中，POST返回201状态码，表示生成了新的资源；DELETE返回204状态码，表示资源已经不存在。 此外，202 Accepted状态码表示服务器已经收到请求，但还未进行处理，会在未来再处理，通常用于异步操作。下面是一个例子。 12345678HTTP/1.1 202 Accepted&#123; "task": &#123; "href": "/api/company/job-management/jobs/2130040", "id": "2130040" &#125;&#125; 2.3 3xx 状态码API 用不到301状态码（永久重定向）和302状态码（暂时重定向，307也是这个含义），因为它们可以由应用级别返回，浏览器会直接跳转，API 级别可以不考虑这两种情况。 API 用到的3xx状态码，主要是303 See Other，表示参考另一个 URL。它与302和307的含义一样，也是”暂时重定向”，区别在于302和307用于GET请求，而303用于POST、PUT和DELETE请求。收到303以后，浏览器不会自动跳转，而会让用户自己决定下一步怎么办。下面是一个例子。 12HTTP/1.1 303 See OtherLocation: /api/orders/12345 2.4 4xx 状态码4xx状态码表示客户端错误，主要有下面几种。 400 Bad Request：服务器不理解客户端的请求，未做任何处理。 401 Unauthorized：用户未提供身份验证凭据，或者没有通过身份验证。 403 Forbidden：用户通过了身份验证，但是不具有访问资源所需的权限。 404 Not Found：所请求的资源不存在，或不可用。 405 Method Not Allowed：用户已经通过身份验证，但是所用的 HTTP 方法不在他的权限之内。 410 Gone：所请求的资源已从这个地址转移，不再可用。 415 Unsupported Media Type：客户端要求的返回格式不支持。比如，API 只能返回 JSON 格式，但是客户端要求返回 XML 格式。 422 Unprocessable Entity ：客户端上传的附件无法处理，导致请求失败。 429 Too Many Requests：客户端的请求次数超过限额。 2.5 5xx 状态码5xx状态码表示服务端错误。一般来说，API 不会向用户透露服务器的详细信息，所以只要两个状态码就够了。 500 Internal Server Error：客户端请求有效，服务器处理时发生了意外。 503 Service Unavailable：服务器无法处理请求，一般用于网站维护状态。 3. 服务器回应3.1 不要返回纯本文API 返回的数据格式，不应该是纯文本，而应该是一个 JSON 对象，因为这样才能返回标准的结构化数据。所以，服务器回应的 HTTP 头的Content-Type属性要设为application/json。 客户端请求时，也要明确告诉服务器，可以接受 JSON 格式，即请求的 HTTP 头的ACCEPT属性也要设成application/json。下面是一个例子。 12GET /orders/2 HTTP/1.1Accept: application/json 3.2 发生错误时，不要返回 200 状态码有一种不恰当的做法是，即使发生错误，也返回200状态码，把错误信息放在数据体里面，就像下面这样。 123456789HTTP/1.1 200 OKContent-Type: application/json&#123; "status": "failure", "data": &#123; "error": "Expected at least two items in list." &#125;&#125; 上面代码中，解析数据体以后，才能得知操作失败。 这张做法实际上取消了状态码，这是完全不可取的。正确的做法是，状态码反映发生的错误，具体的错误信息放在数据体里面返回。下面是一个例子。 123456789HTTP/1.1 400 Bad RequestContent-Type: application/json&#123; "error": "Invalid payoad.", "detail": &#123; "surname": "This field is required." &#125;&#125; 3.3 提供链接API 的使用者未必知道，URL 是怎么设计的。一个解决方法就是，在回应中，给出相关链接，便于下一步操作。这样的话，用户只要记住一个 URL，就可以发现其他的 URL。这种方法叫做 HATEOAS。 举例来说，GitHub 的 API 都在 api.github.com 这个域名。访问它，就可以得到其他 URL。 123456789&#123; ... "feeds_url": "https://api.github.com/feeds", "followers_url": "https://api.github.com/user/followers", "following_url": "https://api.github.com/user/following&#123;/target&#125;", "gists_url": "https://api.github.com/gists&#123;/gist_id&#125;", "hub_url": "https://api.github.com/hub", ...&#125; 上面的回应中，挑一个 URL 访问，又可以得到别的 URL。对于用户来说，不需要记住 URL 设计，只要从 api.github.com 一步步查找就可以了。 HATEOAS 的格式没有统一规定，上面例子中，GitHub 将它们与其他属性放在一起。更好的做法应该是，将相关链接与其他属性分开。 12345678910HTTP/1.1 200 OKContent-Type: application/json&#123; "status": "In progress", "links": &#123;[ &#123; "rel":"cancel", "method": "delete", "href":"/api/status/12345" &#125; , &#123; "rel":"edit", "method": "put", "href":"/api/status/12345" &#125; ]&#125;&#125;[参考文档](https://www.jianshu.com/p/2bcd5836dcdb) 参考文档]]></content>
      <tags>
        <tag>设计规范</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[os.walk 目录遍历器]]></title>
    <url>%2F2019%2F09%2F20%2Fpython%2F2.python%E6%A0%87%E5%87%86%E5%BA%93%2Fos%2Fos.walk_%E7%9B%AE%E5%BD%95%E9%81%8D%E5%8E%86%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1. 概述os.walk() 方法用于通过在目录树中游走输出在目录中的文件名，向上或者向下。 os.walk()方法是一个简单易用的文件、目录遍历器，可以帮助我们高效的处理文件、目录方面的事情。 在Unix，Windows中有效。 2. 语法walk()方法语法格式如下： 1os.walk(top[, topdown=True[, onerror=None[, followlinks=False]]]) 通过从自顶向下或自底向上走树来生成目录树中的文件名。对于树中根目录为 top （包括 top 本身）的每个目录，它产生一个3元组 (root, dirs, files) root 是一个字符串，是目录的路径。 dirs是 root中子目录的名称列表（不包括 ‘.’ 和 ‘..’）。 files是 root中非目录文件的名称列表。 如果可选参数 topdown 是 True 或未指定，则在其任何子目录（目录自上而下生成）的三元组之前生成目录的三元组。如果 topdown 是 False，则目录的三元组在其所有子目录的三元组之后生成（目录是从下到上生成的）。不管 topdown 的值，在生成目录及其子目录的元组之前检索子目录的列表。 当 topdown 是 True 时，呼叫者可以就地修改 dirnames 列表（可能使用 del 或片段分配），并且 walk() 将只递归到其名称保留在 dirnames 中的子目录中；这可以用于修剪搜索，施加特定的访问顺序，或甚至在再次恢复 walk() 之前通知 walk() 关于呼叫者创建或重命名的目录。当 topdown 是 False 时修改 dirnames 对行走的行为没有影响，因为在自底向上模式中，dirnames 中的目录是在 dirpath 自身生成之前生成的。 默认情况下，来自 listdir() 调用的错误将被忽略。如果指定可选参数 onerror，它应该是一个函数；它将使用一个参数（一个 OSError 实例）进行调用。它可以报告错误以继续步行，或提出异常以中止步行。请注意，文件名可用作异常对象的 filename 属性。 默认情况下，walk() 不会走向解析到目录的符号链接。将 followlinks 设置为 True 以访问由符号链接指向的目录，在支持它们的系统上。 注解 请注意，如果链接指向自身的父目录，将 followlinks 设置为 True 可能导致无限递归。 walk() 不跟踪它访问过的目录。 注解 如果传递相对路径名，请不要在walk()的恢复选项之间更改当前工作目录。 walk() 从不更改当前目录，并假定其调用者也不会。 基本示例简单实现 shutil.rmtree()，从下到上走树是必不可少的，rmdir() 不允许在目录为空之前删除目录: 12345678import osfor root, dirs, files in os.walk(top, topdown=False): # 1.当前路径的所有文件 for name in files: os.remove(os.path.join(root, name)) # 1.当前路径的所有文件 for name in dirs: os.rmdir(os.path.join(root, name)) 12]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux权限管理 - chmod命令]]></title>
    <url>%2F2019%2F09%2F11%2Flinux%2F1.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86-chmod%2F</url>
    <content type="text"><![CDATA[chmod命令用来变更文件或目录的权限。在UNIX系统家族里，文件或目录权限的控制分别以读取、写入、执行3种一般权限来区分，另有3种特殊权限可供运用。用户可以使用chmod指令去变更文件与目录的权限，设置方式采用文字或数字代号皆可。符号连接的权限无法变更，如果用户对符号连接修改权限，其改变会作用在被连接的原始文件。 1. linux 权限介绍Linux用 户分为：拥有者、组群(Group)、其他（other），Linux系统中，预设的情況下，系统中所有的帐号与一般身份使用者，以及root的相关信 息， 都是记录在/etc/passwd文件中。每个人的密码则是记录在/etc/shadow文件下。 此外，所有的组群名称记录在/etc/group內！ 2. 语法介绍1. 参数介绍12chmod [-cfvR] [--help] [--version] mode file...` 说明 : Linux/Unix 的档案存取权限分为三级 : 档案拥有者、群组、其他。利用 chmod 可以藉以控制档案如何被他人所存取。 u : 表示该档案的拥有者，g 表示与该档案的拥有者属于同一个群体(group)者，o 表示其他以外的人，a 表示这三者皆是。 + : 表示增加权限、- 表示取消权限、= 表示唯一设定权限。 r : 表示可读取，w 表示可写入，x 表示可执行，X 表示只有当该档案是个子目录或者该档案已经被设定过为可执行。 -c : 若该档案权限确实已经更改，才显示其更改动作 -f : 若该档案权限无法被更改也不要显示错误讯息 -v : 显示权限变更的详细资料 -R : 对目前目录下的所有档案与子目录进行相同的权限变更(即以递回的方式逐个变更) --help : 显示辅助说明 --version : 显示版本 2. 示例将档案 file1.txt 设为所有人皆可读取 : 1chmod ugo+r file1.txt 将档案 file1.txt 设为所有人皆可读取 :1chmod a+r file1.txt 将档案 file1.txt 与 file2.txt 设为该档案拥有者，与其所属同一个群体者可写入，但其他以外的人则不可写入 : 1chmod ug+w,o-w file1.txt file2.txt 将 ex1.py 设定为只有该档案拥有者可以执行 : 1chmod u+x ex1.py 将目前目录下的所有档案与子目录皆设为任何人可读取 : 1chmod -R a+r * 3. 通过数字设置权限1. 语法说明此外chmod也可以用数字来表示权限如chmod 777 file 语法为： 1chmod ugo file 其中u,g,o各为一个数字，分别表示User、Group、及Other的权限。 1r=4，w=2，x=1 若要rwx属性则4+2+1=7； 若要rw-属性则4+2=6； 若要r-x属性则4+1=5。 12chmod a=rwx file 和chmod 777 file效果相同chmod ug=rwx,o=x file和chmod 771 file效果相同 755代表用户对该文件拥有读，写，执行的权限，同组其他人员拥有执行和读的权限，没有写的权限，其他用户的权限和同组人员权限一样。 777代表，user,group ,others ,都有读写和可执行权限。 12# 任何人对mydir文件夹和里面的内容具有完全的权限$ chmod -R 777 mydir 2. chmod 4755与chmod 755 的区别chmod 4755与chmod 755 的区别在于开头多了一位，这个4表示其他用户执行文件时，具有与所有者相当的权限。 例如：root用户创建了一个上网认证程序netlogin，如果其他用户要上网也要用到这个程序，那就需要root用户运行chmod 755 netlogin命令使其他用户也能运行netlogin。 但是netlogin执行时可能需要访问一些只有root用户才有权访问的文件，那么其他用户执行netlogin时可能因为权限不够还是不能上网。 这种情况下，就可以用 chmod 4755 netlogin 设置其他用户在执行netlogin也有root用户的权限，从而顺利上网。]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[curl的用法指南]]></title>
    <url>%2F2019%2F09%2F10%2Flinux%2F1.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F01.curl%E7%9A%84%E7%94%A8%E6%B3%95%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[1. 简介curl 是常用的命令行工具，用来请求 Web 服务器。它的名字就是客户端（client）的 URL 工具的意思。它的功能非常强大，命令行参数多达几十种。如果熟练的话，完全可以取代 Postman 这一类的图形界面工具。 本文介绍它的主要命令行参数，作为日常的参考，方便查阅。内容主要翻译自《curl cookbook》。为了节约篇幅，下面的例子不包括运行时的输出，初学者可以先看我以前写的《curl 初学者教程》。 2. GET 请求不带有任何参数时，curl 就是发出 GET 请求。 1$ curl https://www.example.com 上面命令向www.example.com发出 GET 请求，服务器返回的内容会在命令行输出。 3. 指定用户代理 -A-A参数指定客户端的用户代理标头，即User-Agent。curl 的默认用户代理字符串是curl/[version]。 1$ curl -A 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36' https://google.com 上面命令将User-Agent改成 Chrome 浏览器。 1$ curl -A '' https://google.com 上面命令会移除User-Agent标头。 也可以通过-H参数直接指定标头，更改User-Agent。 1$ curl -H 'User-Agent: php/1.0' https://google.com 4. 指定cookie -b-b参数用来向服务器发送 Cookie。 1$ curl -b 'foo=bar' https://google.com 上面命令会生成一个标头Cookie: foo=bar，向服务器发送一个名为foo、值为bar的 Cookie。 1$ curl -b 'foo1=bar' -b 'foo2=baz' https://google.com 上面命令发送两个 Cookie。 1$ curl -b cookies.txt https://www.google.com 上面命令读取本地文件cookies.txt，里面是服务器设置的 Cookie（参见-c参数），将其发送到服务器。 5. 记录cookie -c-c参数将服务器设置的 Cookie 写入一个文件。 1$ curl -c cookies.txt https://www.google.com 上面命令将服务器的 HTTP 回应所设置 Cookie 写入文本文件cookies.txt。 6. post数据体 -d-d参数用于发送 POST 请求的数据体。 123$ curl -d'login=emma＆password=123'-X POST https://google.com/login# 或者$ curl -d 'login=emma' -d 'password=123' -X POST https://google.com/login 使用-d参数以后，HTTP 请求会自动加上标头Content-Type : application/x-www-form-urlencoded。并且会自动将请求转为 POST 方法，因此可以省略-X POST。 -d参数可以读取本地文本文件的数据，向服务器发送。 1$ curl -d '@data.txt' https://google.com/login 上面命令读取data.txt文件的内容，作为数据体向服务器发送。 --data-urlencode参数等同于-d，发送 POST 请求的数据体，区别在于会自动将发送的数据进行 URL 编码。 1$ curl --data-urlencode 'comment=hello world' https://google.com/login 上面代码中，发送的数据hello world之间有一个空格，需要进行 URL 编码。 7. 设置 HTTP 的标头Referer-e-e参数用来设置 HTTP 的标头Referer，表示请求的来源。 1curl -e 'https://google.com?q=example' https://www.example.com 上面命令将Referer标头设为https://google.com?q=example。 -H参数可以通过直接添加标头Referer，达到同样效果。 1curl -H 'Referer: https://google.com?q=example' https://www.example.com 8. 上传二进制文件 -F-F参数用来向服务器上传二进制文件。 1$ curl -F 'file=@photo.png' https://google.com/profile 上面命令会给 HTTP 请求加上标头Content-Type: multipart/form-data，然后将文件photo.png作为file字段上传。 -F参数可以指定 MIME 类型。 1$ curl -F 'file=@photo.png;type=image/png' https://google.com/profile 上面命令指定 MIME 类型为image/png，否则 curl 会把 MIME 类型设为application/octet-stream。 -F参数也可以指定文件名。 1$ curl -F 'file=@photo.png;filename=me.png' https://google.com/profile 上面命令中，原始文件名为photo.png，但是服务器接收到的文件名为me.png。 9.构造 URL 的查询字符串 -G-G参数用来构造 URL 的查询字符串。 1$ curl -G -d 'q=kitties' -d 'count=20' https://google.com/search 上面命令会发出一个 GET 请求，实际请求的 URL 为https://google.com/search?q=kitties&amp;count=20。如果省略--G，会发出一个 POST 请求。 如果数据需要 URL 编码，可以结合--data--urlencode参数。 1$ curl -G --data-urlencode 'comment=hello world' https://www.example.com 10. 添加 HTTP 请求的标头 -H-H参数添加 HTTP 请求的标头。 1$ curl -H 'Accept-Language: en-US' https://google.com 上面命令添加 HTTP 标头Accept-Language: en-US。 1$ curl -H 'Accept-Language: en-US' -H 'Secret-Message: xyzzy' https://google.com 上面命令添加两个 HTTP 标头。 1$ curl -d '&#123;"login": "emma", "pass": "123"&#125;' -H 'Content-Type: application/json' https://google.com/login 上面命令添加 HTTP 请求的标头是Content-Type: application/json，然后用-d参数发送 JSON 数据。 11. 打印出服务器回应的 HTTP 标头 -i-i参数打印出服务器回应的 HTTP 标头。 1$ curl -i https://www.example.com 上面命令收到服务器回应后，先输出服务器回应的标头，然后空一行，再输出网页的源码。 12. 打印 服务器返回的 HTTP 标头 -I-I参数向服务器发出 HEAD 请求，然会将服务器返回的 HTTP 标头打印出来。 1$ curl -I https://www.example.com 上面命令输出服务器对 HEAD 请求的回应。 --head参数等同于-I。 1$ curl --head https://www.example.com 13. 过 SSL 检测 -k-k参数指定跳过 SSL 检测。 1$ curl -k https://www.example.com 上面命令不会检查服务器的 SSL 证书是否正确。 14. 跟随服务器的重定向 -L-L参数会让 HTTP 请求跟随服务器的重定向。curl 默认不跟随重定向。 1$ curl -L -d 'tweet=hi' https://api.twitter.com/tweet 15. 限制带宽 --limit-rate--limit-rate用来限制 HTTP 请求和回应的带宽，模拟慢网速的环境。 1$ curl --limit-rate 200k https://google.com 上面命令将带宽限制在每秒 200K 字节。 16. 将回应保存为文件 -o-o参数将服务器的回应保存成文件，等同于wget命令。 1$ curl -o example.html https://www.example.com 上面命令将www.example.com保存成example.html。 17. 将回应保存为文件 -O-O参数将服务器回应保存成文件，并将 URL 的最后部分当作文件名。 1$ curl -O https://www.example.com/foo/bar.html 上面命令将服务器回应保存成文件，文件名为bar.html。 18. 不输出错误和进度信息 -s-s参数将不输出错误和进度信息。 1$ curl -s https://www.example.com 上面命令一旦发生错误，不会显示错误信息。不发生错误的话，会正常显示运行结果。 如果想让 curl 不产生任何输出，可以使用下面的命令。 1$ curl -s -o /dev/null https://google.com 19. 只输出错误信息 -S-S参数指定只输出错误信息，通常与-s一起使用。 1$ curl -sS -o /dev/null https://google.com 上面命令没有任何输出，除非发生错误。 20. 用户名与密码 -u-u参数用来设置服务器认证的用户名和密码。 1$ curl -u 'bob:12345' https://google.com/login 上面命令设置用户名为bob，密码为12345，然后将其转为 HTTP 标头Authorization: Basic Ym9iOjEyMzQ1。 curl 能够识别 URL 里面的用户名和密码。 1$ curl https://bob:12345@google.com/login 上面命令能够识别 URL 里面的用户名和密码，将其转为上个例子里面的 HTTP 标头。 1$ curl -u 'bob' https://google.com/login 上面命令只设置了用户名，执行后，curl 会提示用户输入密码。 21. 输出详情 -v-v参数输出通信的整个过程，用于调试。 1$ curl -v https://www.example.com --trace参数也可以用于调试，还会输出原始的二进制数据。 1$ curl --trace - https://www.example.com 22. 指定代理 -x-x参数指定 HTTP 请求的代理。 1$ curl -x socks5://james:cats@myproxy.com:8080 https://www.example.com 上面命令指定 HTTP 请求通过myproxy.com:8080的 socks5 代理发出。 如果没有指定代理协议，默认为 HTTP。 1$ curl -x james:cats@myproxy.com:8080 https://www.example.com 上面命令中，请求的代理使用 HTTP 协议。 23. 指定请求方法 -X-X参数指定 HTTP 请求的方法。 1$ curl -X POST https://www.example.com 上面命令对https://www.example.com发出 POST 请求。 参考连接curl 的用法指南]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python一行命令启动http webserver]]></title>
    <url>%2F2019%2F08%2F29%2Fpython%2F4.python%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%2F%E4%B8%80%E5%8F%A5%E8%AF%9D%E5%90%AF%E5%8A%A8httpwebserver%2F</url>
    <content type="text"><![CDATA[在python3中,可以使用如下命令快速启动一个简易的web服务： 123$ python3 -m http.server# 指定端口python3 -m http.server 80 如果希望后台运行 1$ python3 -m http.server &amp; 如果希望保持服务，忽略所有挂断信号 1$ nohub python3 -m http.server 示例： 12$ python3 -m http.server 80Serving HTTP on 0.0.0.0 port 80 (http://0.0.0.0:80/)]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用WolCmd.exe工具远程唤醒主机]]></title>
    <url>%2F2019%2F08%2F27%2Fwindows%2F%E4%BD%BF%E7%94%A8WolCmd.exe%E5%B7%A5%E5%85%B7%E8%BF%9C%E7%A8%8B%E5%94%A4%E9%86%92%E4%B8%BB%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[wolcmd.exe是一款应用于windows上的wake on lan的工具，通过它，我们可以轻易的唤醒远程主机。 1. 进入的WolCmd.exe的同级目录,键入WolCmd.exe，可以看到它的语法结构12345678910λ WolCmd.exeWake On Lan Command Line...Usage: wolcmd [mac address] [ipaddress] [subnet mask] [port number]i.e. wolcmd 009027a322fc 195.188.159.20 255.255.255.0 7or wolcmd 009027a322fc depicus.com 255.255.255.0 7Copyright www.depicus.com (Brian Slack) 1966-2005 2. 根据实际情况传入参数即可比如我的主机mac地址是08CA45D60A80，ip地址是192.168.72.107，子网掩码是255.255.255.0，端口号是8900 1WolCmd.exe 08CA45D60A80 192.168.72.107 255.255.255.0 8900 3. 点击下载WolCmd]]></content>
  </entry>
  <entry>
    <title><![CDATA[git pull 详解]]></title>
    <url>%2F2019%2F08%2F24%2Fgit%E5%AD%A6%E4%B9%A0%2Fgit-pull%2F</url>
    <content type="text"><![CDATA[git pull 详解git pull命令的作用是，取回远程主机某个分支的更新，再与本地的指定分支合并。它的完整格式稍稍有点复杂。 1$ git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt; 比如，取回origin主机的next分支，与本地的master分支合并，需要写成下面这样。 1$ git pull origin next:master 如果远程分支是与当前分支合并，则冒号后面的部分可以省略。 1$ git pull origin next 上面命令表示，取回origin/next分支，再与当前分支合并。实质上，这等同于先做git fetch，再做git merge。 12$ git fetch origin$ git merge origin/next 在某些场合，Git会自动在本地分支与远程分支之间，建立一种追踪关系(tracking)。比如，在git clone的时候，所有本地分支默认与远程主机的同名分支，建立追踪关系，也就是说，本地的master分支自动”追踪”origin/master分支。 Git也允许手动建立追踪关系。 1git branch --set-upstream master origin/next 上面命令指定master分支追踪origin/next分支。 如果当前分支与远程分支存在追踪关系，git pull就可以省略远程分支名。 1$ git pull origin 上面命令表示，本地的当前分支自动与对应的origin主机”追踪分支”(remote-tracking branch)进行合并。 如果当前分支只有一个追踪分支，连远程主机名都可以省略。 1git pull]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git命令清单]]></title>
    <url>%2F2019%2F07%2F24%2Fgit%E5%AD%A6%E4%B9%A0%2Fgit%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95%2F</url>
    <content type="text"><![CDATA[git 常用命令清单下面是我整理的常用 Git 命令清单。几个专用名词的译名如下。 1234Workspace：工作区Index / Stage：暂存区Repository：仓库区（或本地仓库）Remote：远程仓库 新建代码库12345678# 在当前目录新建一个Git代码库$ git init# 新建一个目录，将其初始化为Git代码库$ git init [project-name]# 下载一个项目和它的整个代码历史$ git clone [url] 配置Git的设置文件为.gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。 123456789# 显示当前的Git配置$ git config --list# 编辑Git配置文件$ git config -e [--global]# 设置提交代码时的用户信息$ git config [--global] user.name "[name]"$ git config [--global] user.email "[email address]" 增加/删除文件123456789101112131415161718192021# 添加指定文件到暂存区$ git add [file1] [file2] ...# 添加指定目录到暂存区，包括子目录$ git add [dir]# 添加当前目录的所有文件到暂存区$ git add .# 添加每个变化前，都会要求确认# 对于同一个文件的多处变化，可以实现分次提交$ git add -p# 删除工作区文件，并且将这次删除放入暂存区$ git rm [file1] [file2] ...# 停止追踪指定文件，但该文件会保留在工作区$ git rm --cached [file]# 改名文件，并且将这个改名放入暂存区$ git mv [file-original] [file-renamed] 代码提交123456789101112131415161718# 提交暂存区到仓库区$ git commit -m [message]# 提交暂存区的指定文件到仓库区$ git commit [file1] [file2] ... -m [message]# 提交工作区自上次commit之后的变化，直接到仓库区$ git commit -a# 提交时显示所有diff信息$ git commit -v# 使用一次新的commit，替代上一次提交# 如果代码没有任何新变化，则用来改写上一次commit的提交信息$ git commit --amend -m [message]# 重做上一次commit，并包括指定文件的新变化$ git commit --amend [file1] [file2] ... 标签1234567891011121314151617181920212223242526# 列出所有tag$ git tag# 新建一个tag在当前commit$ git tag [tag]# 新建一个tag在指定commit$ git tag [tag] [commit]# 删除本地tag$ git tag -d [tag]# 删除远程tag$ git push origin :refs/tags/[tagName]# 查看tag信息$ git show [tag]# 提交指定tag$ git push [remote] [tag]# 提交所有tag$ git push [remote] --tags# 新建一个分支，指向某个tag$ git checkout -b [branch] [tag] 查看信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 显示有变更的文件$ git status# 显示当前分支的版本历史$ git log# 显示commit历史，以及每次commit发生变更的文件$ git log --stat# 搜索提交历史，根据关键词$ git log -S [keyword]# 显示某个commit之后的所有变动，每个commit占据一行$ git log [tag] HEAD --pretty=format:%s# 显示某个commit之后的所有变动，其"提交说明"必须符合搜索条件$ git log [tag] HEAD --grep feature# 显示某个文件的版本历史，包括文件改名$ git log --follow [file]$ git whatchanged [file]# 显示指定文件相关的每一次diff$ git log -p [file]# 显示过去5次提交$ git log -5 --pretty --oneline# 显示所有提交过的用户，按提交次数排序$ git shortlog -sn# 显示指定文件是什么人在什么时间修改过$ git blame [file]# 显示暂存区和工作区的差异$ git diff# 显示暂存区和上一个commit的差异$ git diff --cached [file]# 显示工作区与当前分支最新commit之间的差异$ git diff HEAD# 显示两次提交之间的差异$ git diff [first-branch]...[second-branch]# 显示今天你写了多少行代码$ git diff --shortstat "@&#123;0 day ago&#125;"# 显示某次提交的元数据和内容变化$ git show [commit]# 显示某次提交发生变化的文件$ git show --name-only [commit]# 显示某次提交时，某个文件的内容$ git show [commit]:[filename]# 显示当前分支的最近几次提交$ git reflog 远程同步1234567891011121314151617181920212223# 下载远程仓库的所有变动$ git fetch [remote]# 显示所有远程仓库$ git remote -v# 显示某个远程仓库的信息$ git remote show [remote]# 增加一个新的远程仓库，并命名$ git remote add [shortname] [url]# 取回远程仓库的变化，并与本地分支合并$ git pull [remote] [branch]# 上传本地指定分支到远程仓库$ git push [remote] [branch]# 强行推送当前分支到远程仓库，即使有冲突$ git push [remote] --force# 推送所有分支到远程仓库$ git push [remote] --all 撤销12345678910111213141516171819202122232425262728293031# 恢复暂存区的指定文件到工作区$ git checkout [file]# 恢复某个commit的指定文件到暂存区和工作区$ git checkout [commit] [file]# 恢复暂存区的所有文件到工作区$ git checkout .# 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变$ git reset [file]# 重置暂存区与工作区，与上一次commit保持一致$ git reset --hard# 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变$ git reset [commit]# 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致$ git reset --hard [commit]# 重置当前HEAD为指定commit，但保持暂存区和工作区不变$ git reset --keep [commit]# 新建一个commit，用来撤销指定commit# 后者的所有变化都将被前者抵消，并且应用到当前分支$ git revert [commit]# 暂时将未提交的变化移除，稍后再移入$ git stash$ git stash pop 其他12# 生成一个可供发布的压缩包$ git archive]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[创建git仓库管理代码]]></title>
    <url>%2F2019%2F07%2F24%2Fgit%E5%AD%A6%E4%B9%A0%2F2.%E5%88%9B%E5%BB%BAgit%E4%BB%93%E5%BA%93%E7%AE%A1%E7%90%86%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[如果我们希望提供一台服务器用例管理git仓库，并提供远程访问，需要以下操作: 安装git： 1$ sudo apt-get install git 创建一个git用户，用来运行git服务： 1$ sudo adduser git 创建证书登录： 收集所有需要登录的用户的公钥，就是他们自己的id_rsa.pub文件，把所有公钥导入到/home/git/.ssh/authorized_keys文件里，一行一个 或赋予git密码 1$ passwd git 初始化Git仓库： 先选定一个目录作为Git仓库，假定是/srv/sample.git，在/srv目录下输入命令： 1$ sudo git init --bare sample.git 把owner改为git 1chown -R git:git sample.git 至此，git仓库已经初始化完成了，我们在其它拥有权限的主机上尝试clone Git仓库： 1$ git clone git@192.168.72.192:/srv/sample.git 如果操作正确，可以发现，仓库已经被clone到本地了。]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitnore]]></title>
    <url>%2F2019%2F07%2F22%2Fgit%E5%AD%A6%E4%B9%A0%2Fgitnore%2F</url>
    <content type="text"><![CDATA[语法规范 空行或是以#开头的行即注释行将被忽略。 可以在前面添加正斜杠/来避免递归,下面的例子中可以很明白的看出来与下一条的区别。 可以在后面添加正斜杠/来忽略文件夹，例如build/即忽略build文件夹。 可以使用!来否定忽略，即比如在前面用了*.apk，然后使用!a.apk，则这个a.apk不会被忽略。 *用来匹配零个或多个字符，如*.[oa]忽略所有以”.o”或”.a”结尾，*~忽略所有以~结尾的文件（这种文件通常被许多编辑器标记为临时文件）；[]用来匹配括号内的任一字符，如[abc]，也可以在括号内加连接符，如[0-9]匹配0至9的数；?用来匹配单个字符。看了这么多，还是应该来个栗子： 123456789101112# 忽略 .a 文件*.a# 但否定忽略 lib.a, 尽管已经在前面忽略了 .a 文件!lib.a# 仅在当前目录下忽略 TODO 文件， 但不包括子目录下的 subdir/TODO/TODO# 忽略 build/ 文件夹下的所有文件build/# 忽略 doc/notes.txt, 不包括 doc/server/arch.txtdoc/*.txt# 忽略所有的 .pdf 文件 在 doc/ directory 下的doc/**/*.pdf Github给出的Android开发中使用的.gitignore模板 12345678910111213141516171819202122232425262728# Built application files*.apk*.ap_# Files for the Dalvik VM*.dex# Java class files*.class# Generated filesbin/gen/out/# Gradle files.gradle/build/# Local configuration file (sdk path, etc)local.properties# Proguard folder generated by Eclipseproguard/# Log Files*.log# Android Studio Navigation editor temp files.navigation/# Android Studio captures foldercaptures/# Intellij*.iml# Keystore files*.jks]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux显示进度条]]></title>
    <url>%2F2019%2F07%2F16%2Flinux%2F1.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F%E4%B8%80%E4%B8%AA%E5%8F%AF%E4%BB%A5%E6%98%BE%E7%A4%BALinux%E5%91%BD%E4%BB%A4%E8%BF%90%E8%A1%8C%E8%BF%9B%E5%BA%A6%E7%9A%84%E4%BC%9F%E5%A4%A7%E5%B7%A5%E5%85%B7-CV%2F</url>
    <content type="text"><![CDATA[progress前用名Coreutils Viewer，是使用C语言开发的，用来显示Linux命令执行进度的工具支持cp, mv, tar, dd, gzip/gunzip, cat, grep等coreutils基本命令。 https://github.com/Xfennec/progress Linux安装progressprogress依赖libncurses库显示进度条；安装依赖： 123456# CentOSyum install ncurses-devel -y# Fedora 22dnf install ncurses-devel# Ubuntusudo apt-get install libncurses5-dev 下载源码，编译安装： 1git clone https://github.com/Xfennec/progress;cd progress;make;make install 安装教程]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建gitlab管理git仓库]]></title>
    <url>%2F2019%2F06%2F27%2Fgit%E5%AD%A6%E4%B9%A0%2F3.%E6%90%AD%E5%BB%BAgitlab%E7%AE%A1%E7%90%86git%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[1. 安装并启动gitlab服务假设，我们拥有一台包含docker运行环境并且可以连接网络的主机192.168.71.104，执行以下命令，用于安装并启动gitlab服务 拉取gitlab的docker镜像 1$ docker pull gitlab/gitlab-ce:latest 启动gitlab服务 1234567$ docker run -d \ -p 8443:443 -p 8888:80 -p 2222:22 \ --name gitlab \ -v /root/gitlab/config:/etc/gitlab \ -v /root/gitlab/logs:/var/log/gitlab \ -v /root/gitlab/data:/var/opt/gitlab \ gitlab/gitlab-ce:latest 2. GitLab试用1. 打开首页在上面的示例中，我们将宿主主机的8888端口映射到了docker容器的80端口，等待服务启动后，访问192.168.71.104:8888，即可打开gitlab的web页面 2. 设置管理员密码首先根据提示输入管理员密码，这个密码是管理员用户的密码。对应的用户名是root，用于以管理员身份登录Gitlab,密码设置为了password 3. 创建账号设置好密码后去注册一个普通账号 4. 创建项目注册成功后会跳到首页，我们创建一个项目，名字大家随意 5. 添加ssh-key到gitlab项目建好了之后，我们需要将客户端的公钥添加到gitlab，用于免密登陆。 设置地址在右上角用户头像 &gt; Settings &gt; SSH Keys 添加方法如下： 获取需要对git仓库进行操作的客户端公钥 123456#先看看是不是已经有啦，如果有内容就直接copy贴过去就行啦$ cat ~/.ssh/id_rsa.pub#如果上一步没有这个文件 我们就创建一个，运行下面命令（邮箱改成自己的哦），一路回车就好了$ ssh-keygen -t rsa -C "youremail@example.com"$ cat ~/.ssh/id_rsa.pub 将公钥内容复制到上图的ssh-rsa输入框中 3. 开始使用git假如之前我创建用户名为liyp，仓库名称为ztest，输入以下命令将仓库clone到本地,创建readme并上传 123456$ git clone ssh://git@192.168.71.104:2222/liyp/ztest.git $ cd ztest $ echo hello gitlab ! &gt; README.md $ git add . $ git commit -m "first commit" $ git push 进到入gitlab中，可以看刚才的README文档已经上传成功了。至此，我们可以通过gitlab来管理我们的git仓库了。]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[grep使用说明]]></title>
    <url>%2F2019%2F06%2F26%2Flinux%2F1.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2Fgrep%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[grep的正则表达式生效，需要&#39;&#39; 括起来。 1234# zfs list | grep '/shares/.*/' | wc -l 132 # zfs list | grep /shares/.*/ | wc -l 0]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git分支管理]]></title>
    <url>%2F2019%2F06%2F24%2Fgit%E5%AD%A6%E4%B9%A0%2F1.git%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[1. 列出分支12345678# 列出所有本地分支$ git branch# 列出所有远程分支$ git branch -r# 列出所有本地分支和远程分支$ git branch -a 2. 新建分支123456789101112# 新建一个分支，但依然停留在当前分支$ git branch [branch-name]# 新建一个分支，并切换到该分支$ git checkout -b [branch]# 新建一个分支，指向指定commit$ git branch [branch] [commit]# 新建一个分支，与指定的远程分支建立追踪关系$ git branch --track [branch] [remote-branch] 3. 切换分支12345# 切换到指定分支，并更新工作区$ git checkout [branch-name]# 切换到上一个分支$ git checkout - 4. 追踪分支12# 建立追踪关系，在现有分支与指定的远程分支之间$ git branch --set-upstream [branch] [remote-branch] 5. 合并分支12345# 合并指定分支到当前分支$ git merge [branch]# 选择一个commit，合并进当前分支$ git cherry-pick [commit] 6. 删除分支123456# 删除分支$ git branch -d [branch-name]# 删除远程分支$ git push origin --delete [branch-name]$ git branch -dr [remote/branch]]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh远程执行命令]]></title>
    <url>%2F2019%2F06%2F23%2Flinux%2Fssh%E8%BF%9C%E7%A8%8B%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[1. 远程执行命令如果我们要查看一下某台主机的磁盘使用情况，是不是必须要登录到目标主机上才能执行 df 命令呢？当然不是的，我们可以使用 ssh 命令在远程的主机上执行 df 命令，然后直接把结果显示出来。整个过程就像是在本地执行了一条命令一样： 12345$ ssh root@192.168.199.199 &quot;df -h&quot;Filesystem Size Used Avail Use% Mounted on/dev/mapper/centos-root 17G 5.3G 12G 31% /devtmpfs 470M 0 470M 0% /dev... 那么如何一次执行多条命令呢？其实也很简单，使用分号把不同的命令隔起来就 OK 了： 12$ ssh root@192.168.199.199 &quot;cd /home;ls&quot;admin 注意，当命令多于一个时用引号括起来，否则在有的系统中除了第一个命令，其它都是在本地执行的。 2. 远程执行需要交互的命令通常情况下，当你通过 ssh 在远程主机上执行命令时，并不会为这个远程会话分配 TTY。此时 ssh 会立即退出远程主机，所以需要交互的命令也随之结束。好在我们可以通过-t 参数显式的告诉 ssh，我们需要一个 TTY 远程 shell 进行交互！ 1234$ ssh -t root@192.168.199.199 &quot;top&quot;PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 128580 5000 2908 S 0.0 0.5 0:16.97 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.25 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:38.07 ksoftirqd/0 ... 上面的情况-t参数是必须的，否则终端会提示TERM environment variable not set.错误 12ssh root@192.168.199.199 &quot;top&quot;TERM environment variable not set. 3. 远程执行脚本对于要完成一些复杂功能的场景，如果是仅仅能执行几个命令的话，简直是弱爆了。我们可能需要写长篇累牍的 shell 脚本去完成某项使命！此时 SSH 依然是不辱使命的好帮手(哈哈，前面的内容仅仅是开胃菜啊！)。 1. 执行本地的脚本我们在本地创建一个脚本文件 test.sh，内容为： 12lspwd 然后运行下面的命令： 12$ ssh root@192.168.199.199 &lt; test.sh/root 要想在这种情况下(远程执行本地的脚本)执行带有参数的脚本，需要为 bash 指定-s参数： 1$ ssh root@192.168.199.199 'bash -s' &lt; test.sh hello 2. 执行远程服务器上的脚本除了执行本地的脚本，还有一种情况是脚本文件存放在远程服务器上，而我们需要远程的执行它！此时在远程服务器上用户 nick 的家目录中有一个脚本 test.sh。文件的内容如下： 12lspwd 执行下面的命令： 1$ ssh root@192.168.199.199 "/home/nick/test.sh" 注意，此时需要指定脚本的绝对路径！ 下面我们也尝试为脚本传递参数。在远程主机上的 test.sh 文件的末尾添加两行： 1$ echo $0 然后尝试执行下面的命令： 1$ ssh root@192.168.199.199 /home/nick/test.sh hello 程序可以正常执行]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh远程执行命令]]></title>
    <url>%2F2019%2F06%2F13%2Flinux%2F1.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2Fssh%E8%BF%9C%E7%A8%8B%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[ssh-keygen命令详解同一ip地址更换主机，导致ssh无法连接，使用以下方法删除密钥后解决: 1ssh-keygen -f &quot;/root/.ssh/known_hosts&quot; -R 172.10.16.5 -f filename 指定密钥文件名 -R hostname 从 known_hosts 文件中删除所有属于 hostname的密钥。 1. 远程执行命令如果我们要查看一下某台主机的磁盘使用情况，是不是必须要登录到目标主机上才能执行 df 命令呢？当然不是的，我们可以使用 ssh 命令在远程的主机上执行 df 命令，然后直接把结果显示出来。整个过程就像是在本地执行了一条命令一样： 12345$ ssh root@192.168.199.199 &quot;df -h&quot;Filesystem Size Used Avail Use% Mounted on/dev/mapper/centos-root 17G 5.3G 12G 31% /devtmpfs 470M 0 470M 0% /dev... 那么如何一次执行多条命令呢？其实也很简单，使用分号把不同的命令隔起来就 OK 了： 12$ ssh root@192.168.199.199 &quot;cd /home;ls&quot;admin 注意，当命令多于一个时用引号括起来，否则在有的系统中除了第一个命令，其它都是在本地执行的。 2. 远程执行需要交互的命令通常情况下，当你通过 ssh 在远程主机上执行命令时，并不会为这个远程会话分配 TTY。此时 ssh 会立即退出远程主机，所以需要交互的命令也随之结束。好在我们可以通过-t 参数显式的告诉 ssh，我们需要一个 TTY 远程 shell 进行交互！ 1234$ ssh -t root@192.168.199.199 &quot;top&quot;PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 128580 5000 2908 S 0.0 0.5 0:16.97 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.25 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:38.07 ksoftirqd/0 ... 上面的情况-t参数是必须的，否则终端会提示TERM environment variable not set.错误 12ssh root@192.168.199.199 &quot;top&quot;TERM environment variable not set. 3. 远程执行脚本对于要完成一些复杂功能的场景，如果是仅仅能执行几个命令的话，简直是弱爆了。我们可能需要写长篇累牍的 shell 脚本去完成某项使命！此时 SSH 依然是不辱使命的好帮手(哈哈，前面的内容仅仅是开胃菜啊！)。 1. 执行本地的脚本我们在本地创建一个脚本文件 test.sh，内容为： 12lspwd 然后运行下面的命令： 12$ ssh root@192.168.199.199 &lt; test.sh/root 要想在这种情况下(远程执行本地的脚本)执行带有参数的脚本，需要为 bash 指定-s参数： 1$ ssh root@192.168.199.199 &apos;bash -s&apos; &lt; test.sh hello 2. 执行远程服务器上的脚本除了执行本地的脚本，还有一种情况是脚本文件存放在远程服务器上，而我们需要远程的执行它！此时在远程服务器上用户 nick 的家目录中有一个脚本 test.sh。文件的内容如下： 12lspwd 执行下面的命令： 1$ ssh root@192.168.199.199 "/home/nick/test.sh" 注意，此时需要指定脚本的绝对路径！ 下面我们也尝试为脚本传递参数。在远程主机上的 test.sh 文件的末尾添加两行： 1echo $0 然后尝试执行下面的命令： 1$ ssh root@192.168.199.199 /home/nick/test.sh hello 程序可以正常执行]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins与docker]]></title>
    <url>%2F2019%2F06%2F12%2Fjenkins%2F2.jenkins%E4%B8%8EPipeline%2F12.jenkins%E4%B8%8Edocker%2F</url>
    <content type="text"><![CDATA[1. 在jenkins中使用docker容器1. 修改操作权限jenkins可能没有足够的权限操作dokcer，使用此命令获取权限 1chmod 777 /var/run/docker.sock 2 . 一个基本的例子12345678910111213141516171819202122232425pipeline &#123; agent &#123; docker &#123; image 'ztest' args "--entrypoint='' -v ztest:/workspace" label '71.192' &#125; &#125; environment &#123; LC_ALL = 'en_US.UTF-8' LANG = 'zh_CN.UTF-8' LANGUAGE = 'en_US.UTF-8' &#125; stages &#123; stage('Test') &#123; steps &#123; sh 'rm -rf *' sh 'ztest init' sh 'ls' sh 'ztest start demo' &#125; &#125; &#125;&#125; 3. 遇到的问题在jenkins中执行docker，提示以下错误： 1234+ docker inspect -f . ztest:latestFailed to run top '06112963e90f9baf22eb5fd24ef7b872daa704d27ac4135ff12305a9a422e650'. Error: Error response from daemon: Container 06112963e90f9baf22eb5fd24ef7b872daa704d27ac4135ff12305a9a422e650 is not running 通过添加docker参数--entrypoint解决 123456... docker &#123; image 'ztest' args "--entrypoint=''" &#125;... 4. docker volume如何在jenkins中使用docker voulume中的挂载文件呢？只需要cd到挂载的目录中即可 在jenkins的pipeline中启用docker 环境 1234docker &#123; image &apos;192.168.71.104:5000/tytest&apos; args &quot;--entrypoint=&apos;&apos; --privileged -v tytest:/workspace&quot;&#125; 上面将名为tytest的volume挂载到了容器的/workspace目录，此时只需要cd到此目录即可 12345stage(&apos;DEV&apos;) &#123; steps &#123; sh &quot;cd /workspace;ls&quot; # 会列出volume的所有文件 &#125;&#125;]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[删除命令rm]]></title>
    <url>%2F2019%2F06%2F12%2Flinux%2F1.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F1.%E5%88%A0%E9%99%A4%E5%91%BD%E4%BB%A4-rm%2F</url>
    <content type="text"><![CDATA[删除数量巨大的文件，同时显示详细信息(-v) 1sudo rm -rfv bolands-mills-mhcpt 您可以使用rm -v具有rm打印每删除的文件一行。通过这种方式，您可以看到rm确实正在删除文件。但是如果你有数十亿个文件，那么你将会看到它rm仍然有效。您将不知道已删除了多少文件以及剩下多少文件。 该工具pv可以帮助您进行进度估算。 http://www.ivarch.com/programs/pv.shtml 这里是你将如何调用rm与pv使用示例输出 $ rm -rv dirname | pv -l -s 1000 &gt; logfile562 0:00:07 [79,8 /s] [====================&gt; ] 56% ETA 0:00:05在这个人为的例子中，我告诉我pv有1000文件。输出pv显示562已被删除，经过时间为7秒，估计完成时间为5秒。 一些解释： pv -l使得pv通过换行，而不是字节数pv -s number告诉pv总数是什么，它可以给你一个估计。最后的重定向logfile是清洁输出。否则，状态行将与来自pv的输出混淆rm -v。额外：您将拥有已删除内容的日志文件。但要注意文件会变得很大。/dev/null如果您不需要日志，也可以重定向到。要获取文件数，可以使用此命令： $ find dirname | wc -l如果有数十亿个文件，这也可能需要很长时间。您也可以pv在这里使用它来查看它的数量 $ find dirname | pv -l | wc -l278k 0:00:04 [56,8k/s] [ &lt;=&gt; ]278044这里说它花了4秒钟来计算278k文件。end（278044）的确切计数是来自的输出wc -l。 如果您不想等待计数，那么您可以猜测文件数量或使用pv而无需估算： $ rm -rv dirname | pv -l &gt; logfile像这样你将没有任何估计完成，但至少你会看到已经删除了多少文件。/dev/null如果您不需要日志文件，请重定向到。 鸡蛋里挑骨头： 你真的需要sudo吗？通常rm -r足以递归删除。没必要rm -f。 快速删除数以亿计的文件在业务运行时，没有制定日志清除规则，导致在日志目录下保存了大量的日志文件。在使用rm -rf $dir删除旧的日志时，会提示-bash: /bin/rm: Argument list too long,通过ls |xargs rm -rf也可以进行删除，但是会耗费大量的时间。在网上找到一种快速删除大量文件的方法rsync，有点类似MySQL的truncate table。 具体操作方法12345# 先创建一个空目录# 注意：最好是用和被清空目录的所有者(用户)去创建这个空目录，使用的命令会将空目录的权限带过去mkdir /tmp/empty# 清除目标目录的文件,不要忘记目录后面的`/`rsync --delete-before -av /tmp/empty/ /var/log/target/ 选项说明123-delete-before 接收者在传输之前进行删除操作-a 归档模式，表示以递归方式传输文件，并保持所有文件属性-v 详细输出模式 rsync快的原因12rm删除内容时，将目录的每一个条目逐个删除(unlink)，需要循环重复遍历很多次；rsync删除内容时，建立好新的空目录，替换掉老目录，不需要进行大量的遍历操作。]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux查看文件大小的几种方法]]></title>
    <url>%2F2019%2F06%2F12%2Flinux%2F1.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2FLinux%E6%9F%A5%E7%9C%8B%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1. stat命令12345678910stat filepathxanarry@ThinkPad:/$ stat ~/Downloads/jdk-8u60-linux-x64.tar.gz File: '/home/xanarry/Downloads/jdk-8u60-linux-x64.tar.gz' Size: 181238643 Blocks: 353984 IO Block: 4096 regular fileDevice: 808h/2056d Inode: 261742 Links: 1Access: (0666/-rw-rw-rw-) Uid: ( 1000/ xanarry) Gid: ( 1000/ xanarry)Access: 2017-02-01 17:36:43.177892508 +0800Modify: 2015-10-02 12:43:29.853291000 +0800Change: 2016-12-26 23:33:34.619480450 +0800 2. wc命令wc -c filename 参数-c表示统计字符, 因为一个字符一个字节, 所以这样得到字节数 12xanarry@ThinkPad:/$ wc -c ~/Downloads/jdk-8u60-linux-x64.tar.gz181238643 /home/xanarry/Downloads/jdk-8u60-linux-x64.tar.gz 3. du命令1234du -b filepath 参数-b表示以字节计数xanarry@ThinkPad:/$ du -b ~/Downloads/jdk-8u60-linux-x64.tar.gz181238643 /home/xanarry/Downloads/jdk-8u60-linux-x64.tar.gz 或者 du -h filepath 直接得出人好识别的文件大小 12xanarry@ThinkPad:/$ du -h ~/Downloads/jdk-8u60-linux-x64.tar.gz173M /home/xanarry/Downloads/jdk-8u60-linux-x64.tar.gz 4. ls命令ls -l filepath 第五列为文件字节数 12$ ls -l ~/Downloads/jdk-8u60-linux-x64.tar.gz-rw-rw-rw- 1 xanarry xanarry 181238643 10月 2 2015 /home/xanarry/Downloads/jdk-8u60-linux-x64.tar.gz ls -lh filepath h表示human, 加-h参数得到人好读的文件大小 12$ ls -lh ~/Downloads/jdk-8u60-linux-x64.tar.gz-rw-rw-rw- 1 xanarry xanarry 173M 10月 2 2015 /home/xanarry/Downloads/jdk-8u60-linux-x64.tar.gz]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sed 字符的查找与替换]]></title>
    <url>%2F2019%2F06%2F10%2Flinux%2F1.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F02.sed-%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9F%A5%E6%89%BE%E4%B8%8E%E6%9B%BF%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[sed是一种流编辑器，它是文本处理中非常中的工具，能够完美的配合正则表达式使用，功能不同凡响。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有 改变，除非你使用重定向存储输出。Sed主要用来自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等。 1. 参数与操作1. 参数 -e：执行命令行中的指令，例如：sed -e &#39;command&#39; file(s) [默认值] -f：执行一个 sed 脚本文件中的指令，例如： sed -f scriptfile file(s) -i：与-e的区别在于：当使用-e 时，sed 执行指令并不会修改原输入文件的内容，只会显示在 bash 中，而使用-i 选项时，sed 执行的指令会直接修改原输入文件。 -n：读取下一行到 pattern space。 2. 操作 a ：在行后新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～ c ：取代行，c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！ d ：删除行，所以 d 后面通常不接参数； i ：在行前插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)； p ：打印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～ s ：取代，可以直接进行取代的工作,通常这个s的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！ 2. 按照行号新增/删除行1. 以行为单位的新增 a在第二行后加上『drink tea?』字样: 123456$ nl /etc/passwd | sed '2a drink tea'1 root:x:0:0:root:/root:/bin/bash2 bin:x:1:1:bin:/bin:/sbin/nologindrink tea3 daemon:x:2:2:daemon:/sbin:/sbin/nologin..... 如果是要增加两行以上，在第二行后面加入两行字，例如 Drink tea or ….. 与 drink beer?,每一行之间都必须要以反斜杠\来进行新行的添加 12345678$ nl /etc/passwd | sed '2a Drink tea or ......\&gt; drink beer ?'1 root:x:0:0:root:/root:/bin/bash2 bin:x:1:1:bin:/bin:/sbin/nologinDrink tea or ......drink beer ?3 daemon:x:2:2:daemon:/sbin:/sbin/nologin.....(后面省略)..... 2. 以行为单位的删除 d将 /etc/passwd 的内容列出并且列印行号，同时，请将第 2~5 行删除！ 12345$ nl /etc/passwd | sed '2,5d'1 root:x:0:0:root:/root:/bin/bash6 sync:x:5:0:sync:/sbin:/bin/sync7 shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown..... 只要删除第 2 行 1nl /etc/passwd | sed '2d' 要删除第 3 到最后一行 1nl /etc/passwd | sed '3,$d' 3. 以行为单位的插入 i如果在第二行前插入内容 1234567$ nl /etc/passwd | sed '2i drink tea' 1 root:x:0:0:root:/root:/bin/bashdrink tea2 bin:x:1:1:bin:/bin:/sbin/nologin3 daemon:x:2:2:daemon:/sbin:/sbin/nologin...... 4. 以行为单位的替换 c将第2-5行的内容取代成为『No 2-5 number』呢？ 12345[root@www ~]# nl /etc/passwd | sed &apos;2,5c No 2-5 number&apos;1 root:x:0:0:root:/root:/bin/bashNo 2-5 number6 sync:x:5:0:sync:/sbin:/bin/sync.....(后面省略)..... 透过这个方法我们就能够将数据整行取代了！ 4. 以行为单位的列出 p仅列出 /etc/passwd 文件内的第 5-7 行 1234[root@www ~]# nl /etc/passwd | sed -n &apos;5,7p&apos;5 lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin6 sync:x:5:0:sync:/sbin:/bin/sync7 shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown 可以透过这个 sed 的以行为单位的显示功能， 就能够将某一个文件内的某些行号选择出来显示。 3. 数据搜索1. 数据的搜寻并显示搜索 /etc/passwd有root关键字的行 12345678$ nl /etc/passwd | sed '/root/p'1 root:x:0:0:root:/root:/bin/bash1 root:x:0:0:root:/root:/bin/bash2 daemon:x:1:1:daemon:/usr/sbin:/bin/sh3 bin:x:2:2:bin:/bin:/bin/sh4 sys:x:3:3:sys:/dev:/bin/sh5 sync:x:4:65534:sync:/bin:/bin/sync..... 如果root找到，除了输出所有行，还会输出匹配行。 使用-n的时候将只打印包含模板的行。 12$ nl /etc/passwd | sed -n '/root/p'1 root:x:0:0:root:/root:/bin/bash 2. 数据的搜寻并删除删除/etc/passwd所有包含root的行，其他行输出 12345$ nl /etc/passwd | sed '/root/d'2 daemon:x:1:1:daemon:/usr/sbin:/bin/sh3 bin:x:2:2:bin:/bin:/bin/sh....下面忽略#第一行的匹配root已经删除了 3. 数据的搜寻并执行命令搜索/etc/passwd,找到root对应的行，执行后面花括号中的一组命令，每个命令之间用分号分隔，这里把bash替换为blueshell，再输出这行： 12$ nl /etc/passwd | sed -n '/root/&#123;s/bash/blueshell/;p;q&#125;' 1 root:x:0:0:root:/root:/bin/blueshell 最后的q是退出。 4. 数据的搜寻并替换除了整行的处理模式之外， sed 还可以用行为单位进行部分数据的搜寻并取代。基本上 sed 的搜寻与替代的与 vi 相当的类似！他有点像这样： 1sed &apos;s/要被取代的字串/新的字串/g&apos; 先观察原始信息，利用 /sbin/ifconfig 查询 IP 123456$ ifconfig eth0eth0 Link encap:Ethernet HWaddr 00:90:CC:A6:34:84inet addr:192.168.1.100 Bcast:192.168.1.255 Mask:255.255.255.0inet6 addr: fe80::290:ccff:fea6:3484/64 Scope:LinkUP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1.....(以下省略)..... 本机的ip是192.168.1.100。 将 IP 前面的部分予以删除 12$ ifconfig eth0 | grep 'inet addr' | sed 's/^.*addr://g'192.168.1.100 Bcast:192.168.1.255 Mask:255.255.255.0 接下来则是删除后续的部分，亦即： 192.168.1.100 Bcast:192.168.1.255 Mask:255.255.255.0 将 IP 后面的部分予以删除 12$ /sbin/ifconfig eth0 | grep 'inet addr' | sed 's/^.*addr://g' | sed 's/Bcast.*$//g'192.168.1.100 4. 多点编辑一条sed命令，删除/etc/passwd第三行到末尾的数据，并把bash替换为blueshell 123nl /etc/passwd | sed -e &apos;3,$d&apos; -e &apos;s/bash/blueshell/&apos;1 root:x:0:0:root:/root:/bin/blueshell2 daemon:x:1:1:daemon:/usr/sbin:/bin/sh -e表示多点编辑，第一个编辑命令删除/etc/passwd第三行到末尾的数据，第二条命令搜索bash替换为blueshell。 5. 直接修改文件内容(危险动作)sed 可以直接修改文件的内容 在整行范围内把 journal=recover 替换为 journal=yes。如果没有 g 标记，则只有每行第一个匹,如果没有 -i选项，不会立即生效 1sed -i 's/journal=recover/journal=yes/g' nas_vdbench_conf_file 6. 替换分隔符当要替换的字符串出现\,可以使用:替换默认分隔符\ 1sed -i 's:\"/mnt/.*\":\"/mnt/&#123;&#125;\":g' remoteIO/test_conf 7. 批量处理多个文件12345678# 1.列出包含PASS字符串的文件路径$ grep -lr PASS /home/liyp/home/liyp/file_a/home/liyp/file_b# 2.使用sed将所有文件的PASS替换为pass$ sed 's/PASS/pass/g' `grep -lr PASS /home/liyp`passpass 7. 参考教程 Linux命令大全-sed命令]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins编码问题]]></title>
    <url>%2F2019%2F06%2F05%2Fjenkins%2F2.jenkins%E4%B8%8EPipeline%2F13.jenkins%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1. 编码问题导致无法启动使用jenkins调用测试程序，提示编码问题: 1UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-4: ordinal not in range(128) 通过设置环境变量解决此问题 123456789101112131415pipeline &#123; agent &#123;label 'node name'&#125; environment &#123; LC_ALL = 'en_US.UTF-8' LANG = 'en_US.UTF-8' LANGUAGE = 'en_US.UTF-8' &#125; stages &#123; stage ('XXXX') &#123; steps &#123; echo 'Hello' &#125; &#125; &#125;&#125; 2. 控制台输出乱码Jenkins控制台中文乱码问题 理解环境变量JAVA_TOOL_OPTIONS 12$ export JAVA_TOOL_OPTIONS="-Dfile.encoding=UTF-8" $ java -jar jenkins.war --httpPort=8080]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git常见问题解决方案]]></title>
    <url>%2F2019%2F05%2F24%2Fgit%E5%AD%A6%E4%B9%A0%2Fgit%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[1. 查看并恢复已删除的某个文件1234# 1. 获得所有的已删除的文件$ git log --diff-filter=D --summary# 2. 恢复删除的文件,$commit为文件被删除的提交hash，比如2682452$ git checkout $commit~1 path/to/deleted_file 示例： 1234567891011$ git log --diff-filter=D --summarycommit 2682452a0ae587480b4c96a4f9d5045d371a0e04 (HEAD -&gt; master)Author: gaianote &lt;gaianote@163.com&gt;Date: Sat Apr 13 00:28:12 2019 +0800 3 delete mode 100644 test.py$ git checkout 268245~1 test.py]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在pipeline拉取git或svn]]></title>
    <url>%2F2019%2F05%2F22%2Fjenkins%2F2.jenkins%E4%B8%8EPipeline%2F10.%E5%9C%A8pipeline%E6%8B%89%E5%8F%96git%E6%88%96svn%2F</url>
    <content type="text"><![CDATA[1. 生成checkout代码 在Sample Step输入框输入： 填写项目地址和用户名密码 svn: git: 点击Generate Pipeline Script按钮，会自动生成相应的代码 将步骤3复制的代码粘贴到checkout的位置 1234567891011121314151617181920pipeline &#123; agent any stages &#123; stage('Build') &#123; steps &#123; checkout(...) //将步骤3复制的代码粘贴到这里 sh 'pwd' //这个是Linux的执行 sh 'ls' &#125; &#125; stage('Test') &#123; steps &#123; // bat 'dir' // 如果jenkins安装在windows并执行这部分代码 sh 'echo $&#123;JAVA_HOME&#125;' //这个是Linux的执行 &#125; &#125; &#125;&#125; 2. dir 工作目录：如果文件夹不存在，dir命令会创建文件夹，下面操作会将git down到gui文件夹，如果没有dir命令，则会直接下载到workspace根目录 123456789101112131415161718pipeline &#123; agent any stages &#123; stage('Build') &#123; steps &#123; dir('gui')&#123; checkout(...) sh 'pwd' //这个是Linux的执行 sh 'ls' &#125; · dir('')&#123; //do your job out of git folder sh 'pwd' &#125; &#125; &#125; &#125;&#125; 使用dir后效果如下：123456# pwd/root/.jenkins/workspace/svntest# lsgui gui@tmp# cd gui &amp;&amp; lsbusiness data doc drivers lib __main__.py]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多个stage的关系-顺序和并行]]></title>
    <url>%2F2019%2F05%2F20%2Fjenkins%2F2.jenkins%E4%B8%8EPipeline%2F9.%E5%A4%9A%E4%B8%AAstage%E7%9A%84%E5%85%B3%E7%B3%BB-%E9%A1%BA%E5%BA%8F%E5%92%8C%E5%B9%B6%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[1. stage嵌套stage是支持嵌套的，可以在stage中嵌套新的stages，看看下面这个顺序嵌套例子代码 123456789101112131415161718192021222324252627282930313233pipeline &#123; agent none stages &#123; stage('Non-Sequential Stage') &#123; agent &#123; label 'for-non-sequential' &#125; steps &#123; echo "On Non-Sequential Stage" &#125; &#125; stage('Sequential') &#123; agent &#123; label 'for-sequential' &#125; environment &#123; FOR_SEQUENTIAL = "some-value" &#125; stages &#123; stage('In Sequential 1') &#123; steps &#123; echo "In Sequential 1" &#125; &#125; stage('In Sequential 2') &#123; steps &#123; echo "In Sequential 2" &#125; &#125; &#125; &#125; &#125;&#125; 2. stage并行执行并行stage{...}需要用到指令paraller, 有一个paraller{...} 里面包含多个stage{...},最后一个stage{...}内部支持嵌套多个stages{...}。在paraller{...}如果要设置只要里面有一个stage{...}运行失败就强制停止，可以使用表达式failFast true 来条件控制。 123456789101112131415161718192021222324252627282930313233343536373839pipeline &#123; agent any stages &#123; stage('Non-Parallel Stage') &#123; steps &#123; echo 'This stage will be executed first.' &#125; &#125; stage('Parallel Stage') &#123; failFast true parallel &#123; stage('并行一') &#123; steps &#123; echo "并行一" &#125; &#125; stage('并行二') &#123; steps &#123; echo "并行二" &#125; &#125; stage('并行三') &#123; stages &#123; stage('Nested 1') &#123; steps &#123; echo "In stage Nested 1 within Branch C" &#125; &#125; stage('Nested 2') &#123; steps &#123; echo "In stage Nested 2 within Branch C" &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 正常构建的结果： 如果设置了failFast true ，当某一个并行stage出现错误时，其它并行的stage都会停止构建 如果不设置failFast true ，当某一个并行stage出现错误时，其它并行的stage会继续构建]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline语法详解之input与when]]></title>
    <url>%2F2019%2F05%2F19%2Fjenkins%2F2.jenkins%E4%B8%8EPipeline%2F8.pipeline%E8%AF%AD%E6%B3%95%E8%AF%A6%E8%A7%A3%E4%B9%8Binput%E4%B8%8Ewhen%2F</url>
    <content type="text"><![CDATA[1. input该input指令允许在一个stage{…}显示提示输入等待。在input{…}写一些条件，传递一些变量等 下面解释input{…}里面支持写那些option。 message ：这个option是必须的，这个message会在用户提交构建的页面显示，提示用户提交相关的input条件。 id ：这个id是一个可选的option，可以作为这个input的标记符，默认的标记符是这个stage的名称。 ok ：这个ok也是一个可选的option, 主要是在ok按钮上显示一些文本，在input表单里。 submitter ： 这个submitter也是一个可选的option，里面可以写多个用户名称或者组名称，用逗号隔开。意思就是，只有这写名称的对应用户登陆jenkins，才能提交这个input动作，如果不写，默认是任何人都可以提交input。 parameter ：这个也是一个可选的option, 和我们前面学的parameters没有区别，就是定义一些参数的地方。 下面是一个简单的示例: 123456789101112131415161718pipeline &#123; agent any stages &#123; stage('Example') &#123; input &#123; message "Should we continue?" ok "Yes, we should." submitter "admin,anthony" parameters &#123; string(name: 'PERSON', defaultValue: 'Mr Anthony', description: 'Who should I say hello to?') &#125; &#125; steps &#123; echo "Hello, $&#123;PERSON&#125;, nice to meet you." &#125; &#125; &#125;&#125; 执行以上pipeline脚本，job会持续等待，直到你点击YES为止，你输入的变量，会传递给steps中的${PERSON} ， 即parameters中定义的name参数 构建成功后，输入如下 2. when通过验证expression表达式返回的值，来决定当前的stage是否会被执行。如果when指令包含多个expression件，则所有子expression必须为stage执行返回true。这与子条件嵌套在一个allOf条件中相同。 branch ：当正在构建的分支与给出的分支模式匹配时执行阶段，例如：when { branch &#39;master&#39; }。请注意，这仅适用于多分支Pipeline。 environment ：当指定的环境变量设置为给定值时执行阶段，例如：when { environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39; } expression ：当指定的Groovy表达式求值为true时执行阶段，例如： when { expression { return params.DEBUG_BUILD } } not ：当嵌套条件为false时执行阶段。必须包含一个条件。例如：when { not { branch &#39;master&#39; } } allOf ：当所有嵌套条件都为真时，执行舞台。必须至少包含一个条件。例如：when { allOf { branch &#39;master&#39;; environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39; } } anyOf ：当至少一个嵌套条件为真时执行舞台。必须至少包含一个条件。例如：when { anyOf { branch &#39;master&#39;; branch &#39;staging&#39; } } 下面是一个简单的示例： 12345678910111213141516171819202122232425pipeline &#123; agent any environment &#123; quick_test = false &#125; stages &#123; stage('Example Build') &#123; steps &#123; script &#123; echo 'Hello World' &#125; &#125; &#125; stage('Example Deploy') &#123; when &#123; expression &#123; return (quick_test == "true") &#125; &#125; steps &#123; echo 'Deploying' &#125; &#125; &#125;&#125; 点击构建后，运行结果如上图，可以看出，构建跳过了Deploy阶段,而当我们将environment中的quick_test改为true时，Deploy阶段可以正常构建]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline指令triggers与corn语法解析]]></title>
    <url>%2F2019%2F05%2F17%2Fjenkins%2F2.jenkins%E4%B8%8EPipeline%2F7.pipeline%E6%8C%87%E4%BB%A4triggers%E4%B8%8Ecorn%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本篇继续学习Declarative Pipeline的相关指令，本篇学习三个指令，分别是触发器（triggers），stage和tool。触发器主要是方便周期控制自动化提交，stage是在stages{…}下的一个指令，我们平台的大部分代码都在stage内完成，tool主要就是为了三大工具（jdk, gradle,maven）提供环境变量服务的。 1. 指令1：triggers该triggers指令定义了Pipeline应重新触发的自动化方式。对于与源代码集成的Pipeline，如GitHub或BitBucket，triggers可能不需要基于webhook的集成可能已经存在。目前有三个可用的触发器是cron和pollSCM 和 upstream。 这个triggers是触发器的意思，所以这块是设置什么条件下触发pipeline代码执行，以及触发的频率。看到这里，如果你学习过Jenkins UI功能部分，你应该记得有下面这个图，用来控制轮询频率的，特别适合周期的自动化提交。 这篇介绍的触发器知识点就和这个有关。先记住这个点，在一个pipeline{…}代码中，只运行出现一次triggers{…},而且这个指令不是必须存在的。 1. cron接受一个cron风格的字符串来定义Pipeline应重新触发的常规间隔，例如：triggers { cron(&#39;H 4/* 0 0 1-5&#39;) } 2. pollSCM接受一个cron风格的字符串来定义Jenkins应该检查新的源更改的常规间隔。如果存在新的更改，则Pipeline将被重新触发。例如：triggers { pollSCM(&#39;H 4/* 0 0 1-5&#39;) } 3. upstream接受逗号分隔的作业字符串和阈值。 当字符串中的任何作业以最小阈值结束时，将重新触发pipeline。例如：triggers { upstream(upstreamProjects: &#39;job1,job2&#39;, threshold: hudson.model.Result.SUCCESS) } 举例一个可能利用scm的场景，如果一个公司做到了很好的代码覆盖测试，一般都会，如果监控到有人提交代码，就会自动化触发启动相关的单元测试。这个场景就是适合在pipeline代码里使用triggers指令，下面代码举例一个pollSCM的基本使用。1234567891011121314pipeline &#123; agent any triggers &#123; pollSCM (‘H H(9-16)/2 * * 1-5)’) &#125; stages &#123; stage('Example') &#123; steps &#123; echo 'Hello World' &#125; &#125; &#125;&#125; 2. corn语法解析该字段遵循cron的语法（略有不同）。具体来说，每行包含由TAB或空格分隔的5个字段： MINUTE HOUR DOM MONTH DOW MINUTE Minutes within the hour (0–59)HOUR The hour of the day (0–23)DOM The day of the month (1–31)MONTH The month (1–12)DOW The day of the week (0–7) where 0 and 7 are Sunday. 要为一个字段指定多个值，可以使用以下运算符。按优先顺序排列 * specifies all valid valuesM-N specifies a range of valuesM-N/X or*/X steps by intervals of X through the specified range or whole valid rangeA,B,…,Z enumerates multiple values 示例： 1.每十五分钟一次（也许在：07：，22，：37，：52） 1H/15 * * * * 2.每小时上半场每十分钟一次（三次，也许是：04，：14，：24） 1H(0-29)/10 * * * * 3.每相隔两个小时，在45分执行一次，从上午9:45开始，每个工作日下午3:45结束。 145 9-16/2 * * 1-5 4.每个工作日上午9点到下午5点每两小时一次（可能是上午10点38分，下午12点38分，下午2点38分，下午4点38分） 1H H(9-16)/2 * * 1-5 5.每月1日和15日每天一次，12月除外 1H H 1,15 1-11 * H表示对该段不做限制，对比例3和例4 1properties([pipelineTriggers([pollSCM('H * * * *')])]) 1234567891011121314pipeline &#123; stages &#123; stage('Initialize') &#123; steps &#123; //enable remote triggers script &#123; properties([pipelineTriggers([pollSCM('')])]) &#125; //define scm connection for polling git branch: BRANCH_NAME, credentialsId: 'my-credentials', url: 'ssh://git@stash.server.fqdn/stash/my-project.git' &#125; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline语法environment_options_parameters]]></title>
    <url>%2F2019%2F05%2F16%2Fjenkins%2F2.jenkins%E4%B8%8EPipeline%2F6.pipeline%E8%AF%AD%E6%B3%95environment_options_parameters%2F</url>
    <content type="text"><![CDATA[1. 指令1：environment该environment指令指定一系列键值对，这些对值将被定义为所有步骤的环境变量或阶段特定步骤，具体取决于environment指令位于Pipeline中的位置。 解释一下什么意思，environment{…}, 大括号里面写一些键值对，也就是定义一些变量并赋值，这些变量就是环境变量。环境变量的作用范围，取决你environment{…}所写的位置，你可以写在顶层环境变量，让所有的stage下的step共享这些变量，也可以单独定义在某一个stage下，只能供这个stage去调用变量，其他的stage不能共享这些变量。一般来说，我们基本上上定义全局环境变量，如果是局部环境变量，我们直接用def关键字声明就可以，没必要放environment{…}里面。 123456789101112131415Pipeline &#123; agent any environment &#123; unit_test = true &#125; stages &#123; stage('Example') &#123; steps &#123; if(unit_test == true) &#123; // call run unit test methods &#125; &#125; &#125; &#125;&#125; 指令2：options该options指令允许在Pipeline本身内配置Pipeline专用选项。Pipeline提供了许多这些选项，例如buildDiscarder，但它们也可能由插件提供，例如 timestamps。 这个options{…}不是一个必须的指令，我几乎没有用到过这个，如果要用，一个pipeline{…}内只运行出现一次options{…}, 下面看一个下这个retry的使用。12345678910111213Pipeline &#123; agent any options &#123; retry(3) &#125; stages &#123; stage('Example') &#123; steps &#123; // call some method &#125; &#125; &#125;&#125; 上面的整个pipeline{…}, 如果在jenkins上job执行失败，会继续执行，如果再遇到失败，继续执行一次，总共执行三次。这种任务场景，一般是在夜间执行，无人值守的时候。例如，如果要下班前，要提交一个jenkins job，跑一下测试，由于之前经验告诉我，跑这些测试很不稳定，我就可以通过上面代码，让pipeline代码失败的时候还可以再尝试运行两次，第二天来看结果。总体来说，这个指令不是必须要有的，所以不是必须要掌握学习好的。 上面是把options{…}放在顶层里，也可以放在具体的某一个stage下，意味这这个stage下所有代码，如果遇到失败，最多执行三次。]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在CentOS上使用Zsh提高生产力]]></title>
    <url>%2F2019%2F05%2F16%2Flinux%2F1.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F1.%E4%BD%BF%E7%94%A8zsh%E6%8F%90%E9%AB%98%E7%94%9F%E4%BA%A7%E5%8A%9B%2F</url>
    <content type="text"><![CDATA[传统的bash和zsh相比相差甚远，感谢社区的贡献，让我们可以通过简单的两条命令，即可安装并启用zsh 1. 安装 zsh1yum -y update &amp;&amp; yum -y install zsh 2. 安装 oh-my-zsh1sh -c "$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)" 3. oh-my-zsh效果查看：]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline语法posts]]></title>
    <url>%2F2019%2F05%2F15%2Fjenkins%2F2.jenkins%E4%B8%8EPipeline%2F5.pipeline%E8%AF%AD%E6%B3%95posts%2F</url>
    <content type="text"><![CDATA[Declarative Pipeline语法-post上一篇学习了Agent的这个指令和相关参数的基本使用，基本上掌握了node这个可选参数就好。这一篇，来学习post和stages指令。 1. post指令基本概述post部分定义将在Pipeline运行或阶段结束时运行的操作。一些条件后 的块的内支持post：部分 always，changed，failure，success，unstable，和aborted。这些块允许在Pipeline运行或阶段结束时执行步骤，具体取决于Pipeline的状态。 简单来说，post可以放在顶层，也就是和agent{…}同级，也可以放在stage里面。一般放顶层的比较多。而且pipeline代码中post代码块不是必须的，使用post的场景基本上执行完一个构建，进行发送消息通知，例如构建失败会发邮件通知 基本代码布局： 123456789101112131415161718192021222324252627pipeline &#123; agent any stages &#123; stage('Build') &#123; steps &#123; println "Build" &#125; &#125; stage('Test') &#123; steps &#123; println "Test" &#125; &#125; stage('Deploy') &#123; steps &#123; println "Deploy" &#125; &#125; &#125; Post &#123; always &#123; script &#123; //写相关清除/恢复环境等操作代码 &#125; &#125; &#125;&#125; 2. 不同post条件的基本用法在post代码块区域，支持多种条件指令，这些指令有always，changed，failure，success，unstable，和aborted。下面分别来介绍这些条件的基本用法。 条件1：always作用：无论Pipeline运行的完成状态如何都会执行这段代码 基本代码：123456789101112131415161718192021pipeline &#123; agent &#123; node &#123; label ‘xxx-agent-机器’ customWorkspace "$&#123;env.JOB_NAME&#125;/$&#123;env.BUILD_NUMBER&#125;" &#125; &#125; stages &#123; stage (‘Build’) &#123; bat “dir” // 如果jenkins安装在windows并执行这部分代码 sh “pwd” //这个是Linux的执行 &#125; &#125; Post &#123; always &#123; script &#123; //写相关清除/恢复环境等操作代码 &#125; &#125; &#125;&#125; 这个always场景，很容易想到的场景就是，事后清理环境。例如测试完了，对数据库进行恢复操作，恢复到测试之前的环境。 条件2：changed作用：只有当前Pipeline运行的状态与先前完成的Pipeline的状态不同时，才能触发运行。 基本代码：123456789101112131415161718192021pipeline &#123; agent &#123; node &#123; label ‘xxx-agent-机器’ customWorkspace "$&#123;env.JOB_NAME&#125;/$&#123;env.BUILD_NUMBER&#125;" &#125; &#125; stages &#123; stage (‘Build’) &#123; bat “dir” // 如果jenkins安装在windows并执行这部分代码 sh “pwd” //这个是Linux的执行 &#125; &#125; Post &#123; changed &#123; script &#123; // 例如发邮件代码 &#125; &#125; &#125;&#125; 这个场景，大部分是写发邮件状态。例如，你最近几次构建都是成功，突然变成不是成功状态，里面就触发发邮件通知。当然，使用changed这个指令没success和failure要频率高。 条件3：failure作用：只有当前Pipeline运行的状态与先前完成的Pipeline的状态不同时，才能触发运行。 基本代码：123456789101112131415161718192021pipeline &#123; agent &#123; node &#123; label ‘xxx-agent-机器’ //customWorkspace "$&#123;env.JOB_NAME&#125;/$&#123;env.BUILD_NUMBER&#125;" &#125; &#125; stages &#123; stage (‘Build’) &#123; bat “dir” // 如果jenkins安装在windows并执行这部分代码 sh “pwd” //这个是Linux的执行 &#125; &#125; post &#123; failure &#123; script &#123; // 例如发邮件代码 &#125; &#125; &#125;&#125; 这个failure条件一般来说，百分百会写到Pipeline代码中，内容无非就是发邮件通知，或者发微信群，钉钉机器人，还有国外的slack聊天群组等。 剩下的三个条件：success, unstable, aborted 我就不写代码介绍，和上面一样的代码结构，简单介绍下这三个条件的含义。Success和上面failure都是只Jenkins job的执行结果。是成功，Jenkins UI这次构建会显示绿色图标；如果是失败，就显示红色图标；如果是取消，也就是aborted状态，就显示灰色图标。还有一个是不稳定状态，叫unstable，这几个单词都是和Jenkins job构建结果的概念，如果你熟悉Jenkins，那么这些概念很简单。本篇要求掌握，always和failure和success的基本使用代码。]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline parameters 参数化构建]]></title>
    <url>%2F2019%2F05%2F15%2Fjenkins%2F2.jenkins%E4%B8%8EPipeline%2F11.pipeline%E8%AF%AD%E6%B3%95%E4%B9%8B%E5%8F%82%E6%95%B0%E5%8C%96%E6%9E%84%E5%BB%BAparametters%2F</url>
    <content type="text"><![CDATA[parameters是参数的意思，parameters指令提供用户在触发Pipeline时应提供的参数列表。这些用户指定的参数的值通过该params对象可用于Pipeline步骤。 我们很多人听过参数化构建，也可能知道如何在一个jenkins job上，通过UI创建不同的参数，例如有字符串参数，布尔选择参数，下拉多选参数等。这些参数即可以通过UI点击创建，也可以通过pipeline代码去写出来。我们先来看看了解有那些具体参数类型，然后挑选几个，分别用UI和代码方式去实现创建这些参数。 1. 通过代码方式创建1. 字符串参数就是定义一个字符串参数，用户可以在Jenkins UI上输入字符串，常见使用这个参数的场景有，用户名，收件人邮箱，文件网络路径，主机名称的或者url等 字符串参数 就是定义一个字符串参数，用户可以在Jenkins UI上输入字符串，常见使用这个参数的场景有，用户名，收件人邮箱，文件网络路径，主机名称的或者url等 代码举例：123456Pipeline &#123; agent any parameters &#123; string(name: 'DEPLOY_ENV', defaultValue: 'staging', description: '') &#125;&#125; 使用 1echo "$DEPLOY_ENV" 2. 布尔值参数就是定义一个布尔类型参数，用户可以在Jenkins UI上选择是还是否，选择是表示代码会执行这部分，如果选择否，会跳过这部分。一般需要使用布尔值的场景有，执行一些特定集成的脚本或则工作，或者事后清除环境，例如清楚Jenkins的workspace这样的动作。 代码举例：123456Pipeline &#123; agent any parameters &#123; booleanParam(name: 'DEBUG_BUILD', defaultValue: true, description: '') &#125;&#125; 3. 文本参数文本（text）的参数就是支持写很多行的字符串，这个变量我好像没有使用过，例如想给发送一段欢迎的消息，你可以采用text的参数。 代码举例：1234567Pipeline &#123; agent any parameters &#123; text(name: 'Welcome_text', defaultValue: 'One\nTwo\nThree\n', description: '') &#125;&#125; 上面的\n表示换行，上面写了三行的text。 4. 选择参数选择（choice）的参数就是支持用户从多个选择项中，选择一个值用来表示这个变量的值。工作中常用的场景，有选择服务器类型，选择版本号等。 代码举例：1234567Pipeline &#123; agent any parameters &#123; choice(name: 'ENV_TYPE', choices: ['test', 'dev', 'product'], description: 'test means test env,….') &#125;&#125; 5. 文件参数文件（file）参数就是在Jenkins 参数化构建UI上提供一个文件路径的输入框，Jenkins会自动去你提供的网络路径去查找并下载。一般伴随着还有你需要在Pipleline代码中写解析文件。也有这样场景，这个构建job就是把一个war包部署到服务器上特定位置，你可以使用这个文件参数。 1234567Pipeline &#123; agent any parameters &#123; file(name: 'FILE', description: 'Some file to upload') &#125;&#125; 6. 密码参数密码（password）参数就是在Jenkins 参数化构建UI提供一个暗文密码输入框，例如，我需要在一些linux机器上做自动化操作，需要提供机器的用户名和密码，由于密码涉及安全问题，一般都采用暗文显示，这个时候你就不能用string类型参数，就需要使用password参数类型。 代码举例：1234567Pipeline &#123; agent any parameters &#123; password(name: 'PASSWORD', defaultValue: 'SECRET', description: 'A secret password') &#125;&#125; 123456789101112131415161718pipeline&#123; agent any parameters &#123; string(name: 'userName', defaultValue: 'Anthony', description: 'please give a name') choice(name: 'version', choices: ['1.1', '1.2', '1.3'], description: 'select the version to test') booleanParam(name: 'is_boy', defaultValue: true, description: 'you is boy or not') &#125; stages &#123; stage('test') &#123; steps&#123; script &#123; sh "java -version" &#125; &#125; &#125; &#125;&#125; 菜单是Build now，而并不是Build with Parameters，这个是正常，你先点击Build now，先完成第一个构建，Jenkins第二个构建才会显示代码中的三个参数。刷新之后，就可以看到参数化构建这个菜单。 2. 通过UI创建]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline语法agent与节点管理]]></title>
    <url>%2F2019%2F05%2F14%2Fjenkins%2F2.jenkins%E4%B8%8EPipeline%2F4.pipeline%E8%AF%AD%E6%B3%95agent%E4%B8%8E%E8%8A%82%E7%82%B9%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[这篇开始跟着官网Pipeline文章来具体学习Pipeline语法知识。我们先从Declarative 模式开始，当然，以后我个人主推使用这个模式，前面已经说了原因。这里再说下Declarative的特点，Declarative Pipeline是Jenkins Pipeline 的一个相对较新的补充， 它在Pipeline子系统之上提出了一种更为简化和有意义的语法。我看到有人把Declarative这个单词，翻译成声明，这没毛病，我还是采用英文，不去翻译这个单词。 1.Pipeline语法引用官网地址接下来的Pipeline语法和部分练习代码都来着官网，地址是：https://jenkins.io/doc/book/pipeline/syntax/ 我个人认为官网的文章组织不适合初学者，有些Pipeline代码穿插了很多docker的知识进去，但是我们没有学docker，这样给初学者造成很大的学习压力和负担。 2.Declarative Pipeline概述所有有效的Declarative Pipeline必须包含在一个pipeline块内，例如： 12345pipeline &#123; /* insert Declarative Pipeline here */ &#125; Declarative Pipeline中有效的基本语句和表达式遵循与Groovy语法相同的规则 ，但有以下例外： Pipeline的顶层必须是块，具体来说是：pipeline { } 没有分号作为语句分隔符。每个声明必须在自己的一行 块只能包含章节， 指令，步骤或赋值语句。 属性引用语句被视为无参数方法调用。所以例如，input被视为input（） 第一点，前面文章解释过，就是一个代码块范围的意思，很好理解。第二个以后可能经常会犯这个，分号写了也是多余的。Groovy代码还可以写分号，Jenkins Pipeline代码就不需要，每行只写一个声明语句块或者调用方法语句。第三点，只能包含Sections, Directives, Steps或者赋值语句，其中的Sections 和Directives后面语法会解释。指令和步骤，前面文章我介绍过，例如steps, stage, agent等。最后一句话，我也不确定，没有理解透彻。 3. sectionsDeclarative Pipeline 代码中的Sections指的是必须包含一个或者多个指令或者步骤的代码区域块。Sections不是一个关键字或者指令，只是一个逻辑概念。 4.agent该agent部分指定整个Pipeline或特定阶段将在Jenkins环境中执行的位置，具体取决于该agent 部分的放置位置。该部分必须在pipeline块内的顶层定义 ，但阶段级使用是可选的。 简单来说，agent部分主要作用就是告诉Jenkins，选择那台节点机器去执行Pipeline代码。这个指令是必须要有的，也就在你顶层pipeline {…}的下一层，必须要有一个agent{…},agent这个指令对应的多个可选参数，本篇文章会一一介绍。这里注意一点，在具体某一个stage {…}里面也可以使用agent指令。这种用法不多，一般我们在顶层使用agent，这样，接下来的全部stage都在一个agent机器下执行代码。 为了支持Pipeline作者可能拥有的各种用例，该agent部分支持几种不同类型的参数。这些参数可以应用于pipeline块的顶层，也可以应用在每个stage指令内。 为了支持写Pipeline代码的人可能遇到的各种用例场景，agent部分支持几种不同类型的参数。这些参数可以应用于pipeline块的顶层，也可以应用在每个stage指令内。 参数1：any作用：在任何可用的代理上执行Pipeline或stage。 代码示例 12345pipeline &#123; agent any&#125; 上面这种是最简单的，如果你Jenkins平台环境只有一个master，那么这种写法就最省事情. 需要注意的是，如果设置了any，stage里面设置的agent是无效的，依然会随机分配节点执行任务 参数2：none作用：当在pipeline块的顶层应用时，将不会为整个Pipeline运行分配全局代理，并且每个stage部分将需要包含其自己的agent部分。 代码示例： 12345678910pipeline &#123; agent none stages &#123; stage(‘Build’)&#123; agent &#123; label ‘具体的节点名称’ &#125; &#125; &#125;&#125; 参数3：label作用：使用提供的标签在Jenkins环境中可用的代理机器上执行Pipeline或stage内执行。label后面需要跟上节点的名称，如下图所示 代码示例： 12345pipeline &#123; agent &#123; label ‘具体一个节点label名称’ &#125;&#125; 参数4：node作用：和上面label功能类似，但是node运行其他选项，例如customWorkspace 代码示例： 12345678pipeline &#123; agent &#123; node &#123; label ‘xxx-agent-机器’ customWorkspace "$&#123;env.JOB_NAME&#125;/$&#123;env.BUILD_NUMBER&#125;" &#125; &#125;&#125; 目前来说，这种node类型的agent代码块，在实际工作中使用可能是最多的一个场景。我建议你分别测试下有和没有customWorkspace的区别，前提你要有自己Jenkins环境，能找到”${env.JOB_NAME}/${env.BUILD_NUMBER}”这个具体效果。 一个示例其实agent相关的还有两个可选参数，分别是docker和dockerfile。目前，我不想把docker加入进来，给我们学习Pipeline增加复杂度。但是docker又是很火的一个技术栈，以后如果你项目中需要docker，请去官网找到这两个参数的基本使用介绍。 代码如下： 12345678910111213141516171819pipeline &#123; agent any stages &#123; stage('Build') &#123; steps &#123; // bat 'dir' // 如果jenkins安装在windows并执行这部分代码 sh 'pwd' //这个是Linux的执行 &#125; &#125; stage('Test') &#123; steps &#123; // bat 'dir' // 如果jenkins安装在windows并执行这部分代码 sh 'echo $&#123;JAVA_HOME&#125;' //这个是Linux的执行 &#125; &#125; &#125;&#125; 拷贝上面代码在Jenkins job的pipeline设置页面，保存，启动测试。]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline脚本练习]]></title>
    <url>%2F2019%2F05%2F13%2Fjenkins%2F2.jenkins%E4%B8%8EPipeline%2F3.%E7%AC%AC%E4%B8%80%E4%B8%AAPipeline%E4%BB%A3%E7%A0%81%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A%2F</url>
    <content type="text"><![CDATA[前面一篇，我用Jenkins支持的脚本输入框执行构建，还用了github上拉取代码下来进行构建项目。这篇，就来详细解释下每行代码的含义，还有就是复习一下第一篇提到的几个关键字，这样的基础中的基础知识。 1.Declarative代码如下1234567891011121314151617181920pipeline &#123; agent any stages &#123; stage('Build') &#123; steps &#123; println "Build" &#125; &#125; stage('Test') &#123; steps &#123; println "Test" &#125; &#125; stage('Deploy') &#123; steps &#123; println "Deploy" &#125; &#125; &#125;&#125; 上面是一个Declarative类型的Pipeline，这个，我前面说过，基本上实际开发都采用这个。虽然Scripted模式的Pipeline代码行数精简，很短，上面Declarative有20行代码，如果用Scripted模式，就10行代码。但是Scripted脚本很灵活，不好写，也不好读，维护起来相等困难。我们先来学习Declartive里面代码含义，有些基础知识在Scripted也有，有些却没有。 1）第一行是小写pipeline，然后一对大括{}，学习过代码的人都知道，大括号里面就是代码块，用来和别的代码块隔离出来。pipeline是一个语法标识符，前面我叫关键字。如果是Declarative类型，一定是pipeline {}这样起头的。当然脚本文件，pipeline不要求一定是第一行代码。也就是说pipeline前面可以有其他代码，例如导入语句，和其他功能代码。pipeline是一个执行pipeline代码的入口，jenkins可以根据这个入门开始执行里面不同stage 2）第二行agent any，agent是一个语法关键字，any是一个option类型。agent是代理的意思，这个和选择用jenkins平台上那一台机器去执行任务构建有关。当然jenkins目前只有一个master节点，没有添加第二个节点机器，后面文章，等我们专门学习agent这个指令的时候，再来介绍如何添加一个节点。等添加了新节点，我们这个地方就可以选择用master还是一个从节点机器来执行任务，所以any是指任意一个可用的机器，当然我环境就是master。 3）第三行stages{}, stages是多个stage的意思，也就是说一个stages可以包含多个stage，从上面代码结果你也可以看出来。上面写了三个stage，根据你任务需要，你可以写十多个都可以。 4）第四行stage(&#39;Build&#39;) {}, 这个就是具体定义一个stage,一般一个stage就是指完成一个业务场景。‘Build’是认为给这个任务取一个名字。这个名称可以出现在Jenkins任务的页面上，在我前面一篇文章结尾处的图片可以显示着三个stage的名称，分别是Build,Test，和Deploy。 5）第五行steps{},字面意思就是很多个步骤的意思。这里提一下，看到了steps，当然还有step这个指令。一般来说，一个steps{}里面就写几行代码，或者一个try catch语句。 6）第六行，这个地方可以定义变量，写调用模块代码等。这里，我就用Groovy语法，写了一个打印语句。如果你机器没有安装groovy，你安装了python，你可以写python的打印语句，或者用linux的shell，例如sh &quot;echo $JAVA_HOME&quot; 后面的stage含义就是一样的，上面写了三个state,描述了三个业务场景，例如打包build,和测试Test,以及部署，这三个串联起来就是一个典型的CD Pipeline流程。实际的肯定不是这么写，因为Test就包含很多个阶段，和不同测试类型。这些不同测试类型，都可以细分成很多个stage去完成。 在Declarative 模式中，只支持steps，不支持在steps {…} 里面嵌套写step{…}。一个stage 下至少有一个steps，一般也就是一个steps。我们可以在一个steps下写调用一个或者几个方法，也就是两三行代码。stages下可以包含多个stage, 在一个Declarative Pipeline脚本中，只允许出现一次stages。 2.Scripted模式代码1234567891011node &#123; stage('Build') &#123; // &#125; stage('Test') &#123; // &#125; stage('Deploy') &#123; // &#125;&#125; 这个代码，有两点和上面不同。第一个是Scripted模式是node{}开头，并没有pipeline{},这个区别好知道。第二个要指出的是，scripted模式下没有stages这个关键字或者指令，只有stage。上面其实可以node(&#39;Node name&#39;) {}来开头，Node name就是从节点或master节点的名称。 基本代码含义就讲解到这里，很简单，需要把这几个常见的指令熟记就行。不管哪种模式，你都要注意一对{}，特别是多层嵌套，不要丢了或者少了一些结束大括号。再提一个注释语法，由于pipeline是采用groovy语言设计的，而groovy是依赖java的，所以上面//表示注释的意思。]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux解压缩命令]]></title>
    <url>%2F2019%2F05%2F12%2Flinux%2F2.%E8%A7%A3%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[tar.gz和tar.bz2解压命令网络上下载到linux源码包主要是tar.gz和tar.bz2压缩格式的，有一部分是zip 解压tar.gz命令是 1tar -zxvf xx.tar.gz 解压tar.bz2的命令是 1tar -jxvf xx.tar.bz2 解压zip则使用unzip工具]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipeline脚本练习]]></title>
    <url>%2F2019%2F05%2F11%2Fjenkins%2F2.jenkins%E4%B8%8EPipeline%2F2.pipeline%E8%84%9A%E6%9C%AC%E7%BB%83%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[1. 在Jenkins上创建一个Pipeline项目这个可能很多人没有创建过这个类型，基本上大家之前创建的项目都是自由风格的。参考下面这个图，创建一个Pipeline Job.(注：安装Jenkins的向导过程中，选择默认的插件安装，就会有Pipeline组件) 2. 不集成Github的Pipeline代码这种方式，直接把写好的Pipeline代码拷贝到Pipeline对应的Jenkins页面上，经常用这个方式来进行本地Debug和单元测试。选择上面创建好的这个Pipeline Job，点击Confige, 到达配置界面，点击Pipeline,然后把代码帖进去，点击保存，具体参考下图。 1234567891011121314151617181920pipeline &#123; agent any stages &#123; stage('Build') &#123; steps &#123; println "Build" &#125; &#125; stage('Test') &#123; steps &#123; println "Test" &#125; &#125; stage('Deploy') &#123; steps &#123; println "Deploy" &#125; &#125; &#125;&#125; 上面这个Pipeline模式就是一个典型的Declarative类型，先不管上面具体语法，我们点击保存，然后点击Build Now，看看控制台日志，会发生什么。 3. 集成Github，把Pipeline代码放到Jenkinsfile文件中这种方式才是开发中使用的场景，任何Pipeline和业务代码一样需要添加到代码仓库。这里我们模仿git，只写Declarative的模式，以后我们都使用Declarative模式的Pipeline代码。 我准备好的github项目地址：https://github.com/gaianote/learnJenkins.git 项目中只有一个Jenkinsfile文本文件，里面写的是Declarative模式的Pipeline代码。下面，继续使用上面创建好的Job，到Configure页面，选择如下图的git拉取Pipeline代码。 点击保存，然后点击Build Now, 我们会发现jenkins会自动从github上拉取代码，然后开始构建，这种代码拉取，执行文件的方式，运行结果也是成功的，以后我们基本上都是使用这个方式。]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pipeline基本介绍]]></title>
    <url>%2F2019%2F05%2F10%2Fjenkins%2F2.jenkins%E4%B8%8EPipeline%2F1.Pipeline%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[这篇是Pipeline学习的Hello World篇，任何新知识开始学习都比较困难。耐心一点，坚持多看一些官网文档。概念积累多了，我们才能运用和尝试一些具体的实战练习。这篇主要介绍，Pipeline的基本概念，包括，什么是Pipeline, Pipeline脚本类型划分，为什么要选择Pipeline，以及Pipeline一些大的概念，细节的知识，我们后续文章会慢慢学习。 1. 什么是Pipeline Jenkins Pipeline是一组插件，支持在Jenkins上实现和集成持续交付的管道。Pipeline这个单词是水管的意思。我以后可能会翻译成管道或者流水线，我建议大家不要翻译，就写Pipeline。这里持续集成（CI）和持续交付（CD），我们在DevOps基础扫盲篇介绍过，以后多经常用到这两个单词缩写。 Jenkins为了更好支持CI和CD，通过Groovy语言这么DSL（动态描述语言）来开发Pipeline组件。在Jenkins中有一句话，Pipeline as code，Pipeline是Jenkins中最优雅的存在。之前Jenkins上UI操作动作，都可以在Pipeline中代码实现，主要你对Jenkins和Groovy语言有足够多掌握。 以后我们经常说CI Pipeline和CD Pipeline，你现在大致可以理解为，要实现CD，先要实现CI。CD Pipeline就是一个代码文件，里面把你项目业务场景都通过Groovy代码和Pipeline语法实现，一个一个业务串联起来，全部实现自动化，从代码仓库到生产环境完成部署的自动化流水线。这个过程就是一个典型的CD Pipeline 官网建议我们把Pipeline代码放在一个名称为Jenkinsfile的文本文件中，并且把这个文件放在你项目代码的根目录，采用版本管理工具管理。Jenkinsfile我后面会具体例子来介绍。当然，我们也可以把Pipeline代码用一个Hello.groovy这样的文件去保存在代码库，这也是没问题的。 2. Pipeline代码分类和两者区别一个Jenkinsfile或者一个Pipeline代码文件，我们可以使用两个脚本模式去写代码，这两种分类叫：Declarative Pipeline 和 Scripted Pipeline. 现在来介绍下两者脚本模式的区别，Declarative相对于Scripted有两个优点。第一个是提供更丰富的语法功能，第二个是写出来的脚本可读性和维护性更好。接下里我们学习的Pipeline语法，其中一部分语法只能在Declarative模式下使用，并不支持Scripted模式。虽然，我在后面文章也会用Declarative和Script两个模式去写同一个场景的Pipeline代码，但是，作为一个初学者，我建议选择并采用Declarative的方式去组织Pipeline代码。 3. 为什么要选择使用Pipeline现在Jenkins是一个非常著名的CI服务器平台，支持很多不同第三方（插件的形式）集成自动化测试。Jenkins UI 配置已经满足不了这么复杂的自动化需求，加入Pipeline功能之后，Jenkins 表现更强大，Pipeline主要有以下特点 代码：Pipeline是用代码去实现，并且支持check in到代码仓库，这样项目团队人员就可以修改，更新Pipeline脚本代码，支持代码迭代。 耐用：Pipeline支持在Jenkins master(主节点)上计划之内或计划外的重启下也能使用。 可暂停：Pipeline支持可选的停止和恢复或者等待批准之后再跑Pipeline代码。 丰富功能：Pipeline支持复杂和实时的CD需求，包括循环，拉取代码，和并行执行的能力。 可扩展性：Pipeline支持DSL的自定义插件扩展和支持和其他插件的集成。 4. Pipeline的基本流程上面这段话提到的“主节点”，“批准” 我会后续用具体例子介绍，帮你理解这里说到的好处。这个可扩展性，可能我没法实现，暂时没有研究这么深入。在第节点里面提到的CD Pipeline和本节点提到的Pipeline可以添加到代码仓库管理，通过下面这个CD Pipeline的流程图，我们知道一个CD流程大致包含这些业务场景。 开发提交代码到项目仓库服务器 开始执行Pipeline代码文件，开始从仓库check out代码 启动Pipeline里面第一个stage，stage就是阶段的意思，后面会介绍语法 图里面第一个Stage应该是代码打包构建（Build） 然后进入测试的阶段，执行各种自动化测试验证 然后测试结束，到运维的部署阶段。 部署结束，输出报告，整个自动化流程工作完成，等待触发构建，开始重复下一轮1到7步骤。 5. Pipeline的大的一些概念基础这里开始介绍一些大的并且很基础的概念，更多、更具体的Pipeline语法，我后续文章来介绍。 1. pipeline这个单词是小写，可以看作是Pipeline语法中的一个关键字。以后一个groovy文件或者一个Jenkinsfile文件中不光只有Pipeline代码，例如还有其他的工具类方法等。通过pipeline { Pipeline代码}，这个关键字就是告诉Jenkins接下来{}中的代码就是pipeline代码，和普通的函数或者方法隔离出来。 2. node关键字node就是用来区分，Jenkins环境中不同的节点环境。例如一个Jenkins环境包括master节点，也就是主节点，还包括N多个从节点，这些从节点在添加到主节点的向导页面中有一个参数，好像是label，就是给这个从节点取一个名称。在Pipeline代码中可以通过node这个关键字告诉Jenkins去用哪一台节点机器去执行代码。 3. stage关键字stage，就是一段代码块，一般个stage包含一个业务场景的自动化，例如build是一个stage, test是第二个stage，deploy是第三个stage。通过stage隔离，让Pipeline代码读写非常直观。到后面你还会学习stages这个关键字，一个stages包含多个stage。 4. step关键字step就是一个简单步骤，一般就是几行代码或者调用外部一个模块类的具体功能。这里step是写在stage的大括号里的。 以上就是Pipeline的基础语法内容，下一篇，介绍Declarative Pipeline 和 Scripted Pipeline在Jenkins环境上的实战练习，并解释没一行代码的含义，顺便复习一下Pipeline的基础语法，也就是pipeline, node, stage, step这几个关键字的使用。]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[持续集成的意义]]></title>
    <url>%2F2019%2F05%2F07%2Fjenkins%2F1.jenkins%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F23.%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E7%9A%84%E6%84%8F%E4%B9%89%2F</url>
    <content type="text"><![CDATA[作者： 阮一峰 互联网软件的开发和发布，已经形成了一套标准流程，最重要的组成部分就是持续集成（Continuous integration，简称CI）。 本文简要介绍持续集成的概念和做法。 1. 概念持续集成指的是，频繁地（一天多次）将代码集成到主干。 它的好处主要有两个。 （1）快速发现错误。每完成一点更新，就集成到主干，可以快速发现错误，定位错误也比较容易。 （2）防止分支大幅偏离主干。如果不是经常集成，主干又在不断更新，会导致以后集成的难度变大，甚至难以集成。 持续集成的目的，就是让产品可以快速迭代，同时还能保持高质量。它的核心措施是，代码集成到主干之前，必须通过自动化测试。只要有一个测试用例失败，就不能集成。 Martin Fowler说过，”持续集成并不能消除Bug，而是让它们非常容易发现和改正。” 与持续集成相关的，还有两个概念，分别是持续交付和持续部署。 2. 持续交付持续交付（Continuous delivery）指的是，频繁地将软件的新版本，交付给质量团队或者用户，以供评审。如果评审通过，代码就进入生产阶段。 持续交付可以看作持续集成的下一步。它强调的是，不管怎么更新，软件是随时随地可以交付的。 3. 持续部署持续部署（continuous deployment）是持续交付的下一步，指的是代码通过评审以后，自动部署到生产环境。 持续部署的目标是，代码在任何时刻都是可部署的，可以进入生产阶段。 持续部署的前提是能自动化完成测试、构建、部署等步骤。它与持续交付的区别，可以参考下图。 4. 流程根据持续集成的设计，代码从提交到生产，整个过程有以下几步。 4.1 提交流程的第一步，是开发者向代码仓库提交代码。所有后面的步骤都始于本地代码的一次提交（commit）。 4.2 测试（第一轮）代码仓库对commit操作配置了钩子（hook），只要提交代码或者合并进主干，就会跑自动化测试。 测试有好几种。 单元测试：针对函数或模块的测试 集成测试：针对整体产品的某个功能的测试，又称功能测试 端对端测试：从用户界面直达数据库的全链路测试 第一轮至少要跑单元测试。 4.3 构建通过第一轮测试，代码就可以合并进主干，就算可以交付了。 交付后，就先进行构建（build），再进入第二轮测试。所谓构建，指的是将源码转换为可以运行的实际代码，比如安装依赖，配置各种资源（样式表、JS脚本、图片）等等。 常用的构建工具如下。 Jenkins Travis Codeship Strider Jenkins和Strider是开源软件，Travis和Codeship对于开源项目可以免费使用。它们都会将构建和测试，在一次运行中执行完成。 4.4 测试（第二轮）构建完成，就要进行第二轮测试。如果第一轮已经涵盖了所有测试内容，第二轮可以省略，当然，这时构建步骤也要移到第一轮测试前面。 第二轮是全面测试，单元测试和集成测试都会跑，有条件的话，也要做端对端测试。所有测试以自动化为主，少数无法自动化的测试用例，就要人工跑。 需要强调的是，新版本的每一个更新点都必须测试到。如果测试的覆盖率不高，进入后面的部署阶段后，很可能会出现严重的问题。 4.5 部署通过了第二轮测试，当前代码就是一个可以直接部署的版本（artifact）。将这个版本的所有文件打包（ tar filename.tar * ）存档，发到生产服务器。 生产服务器将打包文件，解包成本地的一个目录，再将运行路径的符号链接（symlink）指向这个目录，然后重新启动应用。这方面的部署工具有Ansible，Chef，Puppet等。 4.6 回滚一旦当前版本发生问题，就要回滚到上一个版本的构建结果。最简单的做法就是修改一下符号链接，指向上一个版本的目录。 5. 值得借鉴的流程某评论内容: 阮老师讲的“持续集成”，正好是我在公司做了将近半年多互联网项目的一个工作流程。我们项目经历了两期，每期都要进行十几次迭代，每次迭代都会在上一个迭代的基础上增加几个新功能，并且公司要求开发人员在编码前要先提交该功能的单体测试和集成测试的代码，然后才能开始写代码（所谓的测试驱动开发），然后测试人员要负责写端到端的自动化测试代码。公司用Jenkins进行持续集成，SVN管理代码库，Git进行团队开发，Sonar进行代码质量检查。项目成员每天早晨都会收到Jenkins服务器发来的前一天的集成测试报告，通知前一天的提交是否完全通过。经历了这样一个持续迭代的过程，给客户交付的代码质量有保证，很少或几乎没有出现过回滚的情况。]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[html_report插件]]></title>
    <url>%2F2019%2F05%2F05%2Fjenkins%2F1.jenkins%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F22.html_report%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[UI 界面上Publish HTML reports的使用1. 安装在插件管理的Available项，搜索 html publisher 以安装插件。安装好Publisher HTML plugin之后，会在新建或者编辑项目时，在【增加构建后操作步骤】出现【Publish HTML reports】的选项。如下： 2. 配置点击上面构建步骤后，在这里会出现配置项目： 3.测试报告测试报告最终的输出目录为%workspace%/$reportDir/$reportFiles 这个插件不会自动按照上面的路径创建文件夹和文件，所以，一定要确保上面路径上的文件夹和文件已经在工作空间创建好，不然会出现报错。也就是说，这个插件是不创建任何html内容的 2. Pipeline 上Publish HTML reports的使用1234567891011121314post&#123; always&#123; script&#123; publishHTML (target: [ allowMissing: false, alwaysLinkToLastBuild: false, keepAll: true, reportDir: 'report', reportFiles: 'index.html', reportName: "HTML Report" ]) &#125; &#125; &#125; reportDir，就是你项目中保存html文件的地方，这里写‘test-output’是一个相对路径写法，默认从你项目根路面开始，所以，这里我们写test-output就行。第二个参数reportFiles，我写了index.html，这个要和前面extentreport代码设置报告名称一致。这个地方可以同时写多个html文件，逗号隔开。第三个参数reportName，这个参数写的字符串会在Jenkins构建Job页面显示的菜单名称，后面会看到这个名称，这个名称可以随意修改，例如改成selenium report。 3. 为测试报告开启CSS和js由于Jenkins中的内容安全策略，CSS是被禁止的。 The default rule is set to: sandbox; default-src ‘none’; img-src ‘self’; style-src ‘self’;This rule set results in the following: No JavaScript allowed at allNo plugins (object/embed) allowedNo inline CSS, or CSS from other sites allowedNo images from other sites allowedNo frames allowedNo web fonts allowedNo XHR/AJAX allowed, etc. 解决方案如下： 进入系统管理-&gt;script Console-&gt;输入下面的命令： 1System.setProperty("hudson.model.DirectoryBrowserSupport.CSP", "") 然后按RUN。如果在RESULT标题下面看到输出为RESULT，则禁用保护。重新运行您的构建，您可以看到存档的新HTML文件将启用CSS。]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins密钥管理]]></title>
    <url>%2F2019%2F05%2F05%2Fjenkins%2F1.jenkins%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F20.%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4%E6%9E%84%E5%BB%BA%E5%8E%86%E5%8F%B2%2F</url>
    <content type="text"><![CDATA[Jenkins构建后会生成历史记录，默认存放在如下目录 1Jenkins_home/jobs/build_project_name/builds/ Linux下，Jenkins_home默认为/root/.jenkins 如果因磁盘空间存储等原因，想批量删除构建历史记录，可进入该目录，删除全部文件，然后重启Jenkins即可。]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins密钥管理]]></title>
    <url>%2F2019%2F05%2F04%2Fjenkins%2F1.jenkins%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F19.%E5%AF%86%E9%92%A5%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[密钥管理我们在使用git和svn拉取时，创建的密钥，通过点击主页面-&gt;左侧Credentials 进行管理。使用方法如下：]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新UI界面Blue Ocean]]></title>
    <url>%2F2019%2F05%2F03%2Fjenkins%2F1.jenkins%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F17.jenkins%E4%B8%BB%E4%BB%8E%E8%8A%82%E7%82%B9%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[在新主机添加slave节点1. 将密钥传送到子节点A为主节点master，B为从节点slave SSH登录A，如果还没有SSH密钥，则先生成SSH密钥，执行： 1$ ssh-keygen 全部按Enter使用默认值。 有了密钥之后，将密钥传送到远程主机B，执行： 1$ ssh-copy-id &lt;user-name&gt;@&lt;remote-hostB&gt; 这样，我下次登录上的账户时就不需要密码了 2. 设置launch method相关信息 HOST填写slave的ip地址 Credentials填写ssh认证信息 Host key Verification选择 Manually trusted key Verification Strategy 3. 填写SSH认证信息如果你还没有验证信息，请点击上图中的 Add 添加验证信息 在private_key填入master节点上的私钥内容，使用cat /root/.ssh/id_rsa查看 注意的是，username也要匹配，比如你ssh-copy-id root用户免登陆，则username那里需要填写 root 4. 限制此项目可以运行的位置新建JOB时，在General &gt; Restrict where this project can be run下，填写需要JOB运行的节点名称 5. 复制节点对于多个节点，我们在发送私钥后，就可以通过复制旧的节点快速创建新节点，只需要修改ip地址即可。]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新UI界面Blue Ocean]]></title>
    <url>%2F2019%2F05%2F02%2Fjenkins%2F1.jenkins%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F16.%E6%96%B0UI%E7%95%8C%E9%9D%A2Blue_Ocean%2F</url>
    <content type="text"><![CDATA[1. 在插件管理安装Blue Ocean在可选插件中输入 blue ocean 搜索，可以看到一大堆和blue ocean相关的插件，你只需要勾选Blue Ocean那个，会自动帮你安装其他依赖的插件，这个安装成功是否，取决于你的网速，如果安装失败，重启，再来一次。 安装完，建议重启下jenkins服务。 2. 如何从旧版本Jenkins切换到蓝海的皮肤版本安装完插件后，点击左侧红圈这个地方的入口。 Jenkins的Blue Ocean默认界面 这个界面看起来特别干净，清爽。好像，现在很多基于云的相关的前端页面都是采用这个风格开发的。点击顶部向右那个ICON就回到了经典的Jenkins界面。关于在蓝海皮肤下的操作就简单介绍到这里，你自己点击，看看，这个很可能未来哪个版本就采用这个皮肤来取代当前的经典皮肤。这个蓝海皮肤有一个前提条件就是你的jenkins版本必须要2.7以上，否则是不支持这个插件的]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jenkins与Pipeline]]></title>
    <url>%2F2019%2F05%2F01%2Fjenkins%2F1.jenkins%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F15.Jenkins%E4%B8%AD%E7%9A%84Pipeline%2F</url>
    <content type="text"><![CDATA[继续来学习Jenkins的基本知识。本篇来介绍Pipeline，首先什么是Pipeline，如何你百度一下这个单词，会告诉你是管道的意思。那么Jenkins中为什么引入管道的概念呢。其实Pileline在Jenkins中是一种工具，一个插件，用来监控Job的构建过程。 1.什么是Pipeline我们已经知道对应的中文是管道的意思，例如下面这个图，就是一个管道，生活中的自来水管，就是像下面这张图，由多个分支拼接而成。 在Jenkins中，把每一段管道比作是不同的Job，不同Job的链接，这个时候就是Pipeline插件闪亮登场。前面我们提到Jenkins的工作流程，build-deploy-test-release，每个流程之间我们都可以用Pipeline来连接，大致如下效果图。 2. Jenkins上安装Pipeline我们直接在插件管理，搜索 Delivery pipeline，然后点击安装，会自动安装依赖的包和插件。 3. 初步了解Pipeline的效果这里，我们百度图片搜索输入jenkins pipeline,多看几张图片，就可以看到下面这张图的效果。 从上面来看，如果是一个复杂的项目，有了pipeline，就很清楚了解每个阶段构建消耗时间和整个构建的流程走向图，很清晰地感觉。本篇，主要是对Pipeline有一个基本的了解就可以。]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jenkins邮件通知设置]]></title>
    <url>%2F2019%2F04%2F25%2Fjenkins%2F1.jenkins%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F14.Jenkins%E9%82%AE%E4%BB%B6%E9%80%9A%E7%9F%A5%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[上一篇，介绍了一个构建监控的工具，其实这个工具，在一般实际工作中用处不大，甚至还不如一个邮件提醒的实在。所以，本篇，讨论Jenkins上邮件设置，通过设置了邮件地址和邮件发送服务器，我们构建后，就可以通过邮件得到构建结果。这个功能是很有必要的，试想一个这么场景，我需要Jenkins完成一个Job的构建，这个构建过程大概持续半小时到一个小时。那么我们是不是一直监控半小时到一个小时。其实没有必要，我们可以在这个时间内去干别的事情，只需要，时间到了就能收到构建的邮件。 Jenkins邮件提醒配置界面在Jenkins邮件提示设置是在系统设置中，点击 系统管理-&gt;系统设置，下拉到页面底部，可以看到邮件通知设置。 点击勾选高级，这里用QQ邮箱举例，可以从参考这里。]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git clone]]></title>
    <url>%2F2019%2F04%2F24%2Fgit%E5%AD%A6%E4%B9%A0%2F1.git-clone%2F</url>
    <content type="text"><![CDATA[git clone 命令的作用是，将远程仓库中的代码克隆到本地 clone 某个远程分支到本地 12345678910$ git clone -b 2.1.2 ssh://git@192.168.71.104:2222/zeus/ztest.git$ cd ztest$ git branch* 2.1.2$ git branch -a* 2.1.2 remotes/origin/1.2.2 remotes/origin/2.1.2 remotes/origin/HEAD -&gt; origin/master remotes/origin/master]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins 消息提醒]]></title>
    <url>%2F2019%2F04%2F21%2Fjenkins%2F1.jenkins%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F13.Jenkins%E6%9E%84%E5%BB%BA%E7%9B%91%E8%A7%86%E5%99%A8-CatLight%2F</url>
    <content type="text"><![CDATA[这里介绍一个消息提醒器，叫CatLight,这个软件的图标就是有一个发金黄色光亮的猫。这里介绍它，是因为，CatLigh可以用来当做一个Jenkins上Job的构建的一个监控器使用，简单来了解，CatLight是如何和Jenkins进行配置，达到监控Build的。 CatLight的下载和安装 浏览器打开https://catlight.io，进行windows版本下载。 和Jenkins配置连接 点击Jenkins，出现如下配置界面。]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins 构建一个程序]]></title>
    <url>%2F2019%2F04%2F21%2Fjenkins%2F1.jenkins%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F10%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[点击Build选项卡 -&gt; Add buid step 可以看到，jenkins支持以下几种构建模式，如果主机时windows，就执行Execute Windows batch command,如果主机时linux，就点击Execute shell]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins构建触发器之远程构建]]></title>
    <url>%2F2019%2F04%2F21%2Fjenkins%2F1.jenkins%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F8.%E6%9E%84%E5%BB%BA%E8%A7%A6%E5%8F%91%E5%99%A8%E4%B9%8B%E8%BF%9C%E7%A8%8B%E6%9E%84%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1. 什么是远程构建远程构建就是，在创建job时，生成一个可供他人访问的URL地址，用以构建项目 来看看URL的组成，第一个参数JENKINS_URL,这里我们写IP地址或者机器hostname，第二个参数TOKEN_NAME就是你在身份验证令牌文本输入框输入的值。这里我们把令牌设置成123456，然后我就在我机器的另外一个浏览器来模拟远程构建，这个时候我的远程构建地址就是这样的：localhost:8080/job/Test1/build?token=123456 2. Project不同图例的含义什么是图例，就是构建状态和编译晴雨表，其中S表示上一次项目构建是否成功，W表示项目整体的成功率]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins 构建触发器之项目之间依赖关系]]></title>
    <url>%2F2019%2F04%2F21%2Fjenkins%2F1.jenkins%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F9.%E6%9E%84%E5%BB%BA%E8%A7%A6%E5%8F%91%E5%99%A8%E4%B9%8B%E9%A1%B9%E7%9B%AE%E4%B9%8B%E9%97%B4%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[1. 构建触发器项目Test1构建是需要在项目Test2构建完成之后进行，这就是依赖关系，用Jenkins上的界面来解释，就是下面这张图的红色区域选项。 这个输入会自动补全Project名称，可以选择多个Project名称，用逗号隔开,下面有几种选择，我现在第一个，只有前一个构建成功才构建本次。 2. 构建后操作类似于构建出发器，构建后操作也拥有依赖关系的用法，使用方法于Build Triggers相同]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[创建一个Project的基本过程]]></title>
    <url>%2F2019%2F04%2F19%2Fjenkins%2F1.jenkins%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F7.%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAProject%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[创建一个Project的基本过程本篇来介绍如何创建一个Job,这里叫job叫习惯了，最新版本jenkins叫item或者project。前面文章，我们简单提了一下，新建一个Job的过程。这里我们主要介绍这个Job的配置过程。我们先了解一个Job的创建的基本过程，以后才能去创建一个复杂的job。 1. 点击新建，给item命名 这里Job的名称叫Test1,类型只有一个，选择自由风格软件项目，点击Ok. 2. 出现了Test1的配置界面。 上面有5个tab来切换，方便我们进行配置。第一个要配置的就是General，这里我们主要写一下描述，其他的不勾选。然后点击源码管理，来到下一个步骤界面。 3.配置源码管理 ​ 这里我们由于没有安装相关源码管理插件，所以这里是空白，常见的源码管理有git 或者github,SVN等。这部分不是本篇讨论内容，下次再介绍。 4. 构建触发器 构建触发器，这里有四个选项供你选择。第一个是远程构建，下一篇文章举例来介绍。第二个是依赖其他project 结束后再build。也是放在后面介绍。第三个是，周期性的构建，你可以点击右侧问号，会告诉你如何进行周期性构建。第三个Poll SCM，SCM是软件配置管理的意思，这里Poll SCM就是定期检查源码变更的构建。第三和第四容易混淆，周期性构建不会关注源码是否发生变化。而Poll SCM会去代码版本控制工具，定期查找代码是否有更新，如果有，就开始构建。这里，我们选择Build periodically来先演示下。点击右边问题，看看提示，指导我们如何填写值 根据里面提供的实例，我们在输入框，填入一下值。五个星号，中间有空格隔开，表示每分钟执行一次。 5.构建 这里有三种方式构建，由于我们没有安装其他插件，这里我们就选择最基本的，而且我们安装环境是windows,所以，这里我们点击windows 批处理命令。这里我们输入命令dir,dir在windows中可以显示当前路径下所有文件清单，类似linux下的ls命令 6. 构建后操作构建后操作是非常有用的，例如测试完后发生测试报告。 这里我们做最简单的，所有，这里我们什么都不选，下篇文章，我们需要这里设置Build other projects.然后这里直接点击Apply和保存按钮，这个Test1的project就创建完成。 7. 测试构建在步骤6中的Test1 项目下，Build History是空的，这里我们点击立即构建来看看发生什么。 一般来说，构建号是从#1开始的，我这边删除了#1和#2，由于我们前面设置了周期性构建，设置没分都可以构建，所以，点击立即构建，可能会出现两个或者两个以上构建号。我们点击一个构建号，然后点击控制台输出，看到下面图片内容。 从输出的日志来看，这个build，执行了dir这个命令。一个简单的Job的创建过程和构建过程就介绍到这里，下一篇，介绍构建依赖关系，也就是构建这个需要从其他project构建完成后再进行。]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux文本编辑器vim]]></title>
    <url>%2F2019%2F04%2F19%2Flinux%2F1.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F04%20Linux%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%99%A8vim%2F</url>
    <content type="text"><![CDATA[1. 三种模式 命令模式 输入 i等进入编辑模式，使用 ESS 返回到命令模式 编辑模式 使用 ESS 返回到命令模式 命令行模式 使用 : 进入命令行模式，使用 ESS 返回到命令模式 2. 命令模式常用命令1. 基本操作 a 在当前字符之后插入内容 A 在行尾插入内容 i 在当前字符之前插入内容 I 在行首插入内容 o 在下一行插入 O 在上一行插入 x 在光标处向后删除，等同于 del X 在光标处向前删除，等同于 backspace u 撤销一步 2. 删除内容123dd 删除光标所在的行5dd 删除光标所在的后5行dG 删除光标后所有的内容 3. 复制粘贴123yy 复制光标所在的行3yy 复制光标所在后3行p 粘贴yy所复制的行 3. V模式-批量操作用于批量编辑行，在命令模式下，依次输入 ctr v,使用上下左右键选择文本，使用I等命令进行批量操作，ESC推出后生效。 4. 命令行模式:! 调用系统命令,比如:!ifconfig不退出vim直接查看ip地址:s 替换%s/a/b 每一行的第一个符合要求的词替换成b%s/a/b/g 将文本中所有的a替换为b 5. 定位与显示行号12345gg 定位到第一行行首G 定位到最后一行:# 定位到某一行,#表示行号:set nu 显示行号:set nonu 隐藏行号]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins简单介绍和安装]]></title>
    <url>%2F2019%2F04%2F18%2Fjenkins%2F1.jenkins%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F6.jenkins%E7%B3%BB%E7%BB%9F%E8%AE%BE%E7%BD%AE%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[1. 基本配置进入到系统设置 管理员账号登录Jenkins，点击系统管理-&gt;点击系统设置，界面如下。 第一项是家目录，前面文章已经介绍过，这里不再介绍 1. 系统信息接下来，我们来看系统消息，对应的是一个大的文本输入框。上面的编辑格式是plain text,就是你输入什么就显示什么，没有格式控制。其实这个地方是可以用html格式来控制系统消息的显示。 1. 通过palin text设置系统消息在文本框输入一段文字。这里因为是测试，随便输入一些文字。然后点击下面的apply或者保存按钮。 点击保存按钮之后，我们点击Jenkins来到默认的主页面，也是就dashboard页面，显示不同项目的页面，可以看到我们刚刚输入的系统消息。所以，原来系统消息，就是通过上面的设置过程来显示的，这样就理解了这个系统消息输入框的作用了吧。 2. 通过HTML语法来设置系统消息上面我们介绍了plain text方式设置系统消息，如果你觉得系统消息显示很丑，不带格式，字体没法控制，那么我们需要使用HTML语法来控制系统消息的显示，其实Jenkins是支持这个功能。我们需要点击系统管理-&gt;Configure Global Security-&gt;Markup Formatter 如果没有Safe HTML选项，请下载安装OWASP Markup Formatter Plugin 点击保存按钮之后，来到jenkins主页面，观察系统消息显示效果。 ###2. 执行者数量 这个我的环境，默认是2个，执行者数量的含义是表示最多同时跑多少个job的数量，job现在好像改成叫item。 3. 用法 Usage用法，这里有两种。我们目前只有一个节点的jenkins，如果有多个节点，就会发生节点绑定job的模式。这里不展开介绍，了解一下就可以。 4. 生成等待时间 Quiet period生成等待时间默认是5秒，表示创建一个job或者item的时间是5秒。一般不做变动，设置成0也可以，但是没有实际意义。 5. SCM签出重试次数 SCM checkout retry countSCM一般表示Jenkins从git或者其他源码管理工具拉取代码失败，然后重试的次数。这个地方可以设置5或者10，如果访问代码管理工具网络慢。 6. Restrict project naming如果勾选了Restrict project naming并勾选了pattern，那么以后创建项目，只能按照模式匹配pattern的要求创建，不符合这个要求的就不能创建。 2. Global properties这里可以设置一些环境变量和工具的位置]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[改变Jenkins home目录]]></title>
    <url>%2F2019%2F04%2F16%2Fjenkins%2F1.jenkins%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F3.%E4%BF%AE%E6%94%B9jenksins%E7%9A%84home%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[改变Jenkins home目录本篇来演示如何改变Jenkins home目录，这个设置一般很少修改，但是如何还是大企业的Jenkins服务器。作为运维人员，他需要考虑jenkins home目录如果太多，为了节约磁盘空间，可能需要把这个home目录移动到别的磁盘里。如何安装插件太多，Jenkins服务器运行时间长了，log文件都有很多。所以，修改Jenkins的home目录是有必要的 点击系统设置，查看当前home目录的位置，点击 ？查看如何修改home目录]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins简单介绍和安装]]></title>
    <url>%2F2019%2F04%2F16%2Fjenkins%2F1.jenkins%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F1.jenkins%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1. jenkins简介如果在做自动化测试的朋友，应该熟悉Jenkins，或者至少使用过。如果一个人没有使用过Jenkins或者hudson，hudson是Jenkins的前身，他还说自己做过自动化测试，只能说，他只不过是在做半自动化测试。为什么是半自动化呢？他只不过是把手工用例转换自动化脚本，真正的自动化测试是一个全公司的平台，有测试，开发，运维，三个团队一起研发和维护和使用的自动化测试平台，这个通常叫devops，也就是开发和运维两个英文单词的缩写。devops最近几年开始火热起来，据说它能解决企业传统开发，测试，运维的工作流程和模式，能够达到项目快速迭代，缩短项目上线时间。要实现这个过程，需要每个环节都全部自动化，开发代码提交到自动化打包，测试启动自动化测试脚本，运维启动自动更新文件到线上环境，三者之前，形成一个闭环，每个环节都打通，自动化实现，快速响应，快速迭代。 话题扯得有点远了，干脆再扯一点。devops高大上的一个东西，或者叫项目，我没法实现。能实现的，都是国内外技术专家，膜拜这些大神。这样的大神肯定有，但是肯定不多。我所理解的国内的devops平台，一般是这样实现的。组件一个devops研发团队，把开发，测试，运维，三种共有的东西和流程给抽象化，然后提取抽象化的东西，开始进去编程实现。每个节点都是可以扩展和提供API给别人。在这个基础之上，开发完成自己任务，测试完成自己自动化测试，运维完成自动部署和监控。每个环节都有自己团队的工具和实现方式，三个都实现好了，然后接入devops平台，从而达到完整的体系。 这个系列，我们重点介绍devops平台的关键的核心的管理工具，叫Jenkins，主要介绍Jenkins的基本认识和基本使用方法。第一个问题来了，什么是Jenkins？它是一个Java开放的开源程序，所以，需要提前安装Java JDK环境，能支持安装到windows,mac,linux平台，主要是一个管理工具。第二个问题，为什么要使用Jenkins？我们用它，主要是项目上的持续集成和持续交付。持续集成对应英文（Continuous Integration），有时候简称CI，持续交付对应英文（Continuous Delivery），简称CD，以后，听到了CI和CD，就明白了什么意思。下面这张图，是Jenkins在实际项目运用上的一个经典的流程图。 作为一个测试工程师，可能你的工作大部分是在上面图中的执行测试部分。测试工程师需要写webui,接口自动化测试脚本，或者手机功能测试脚本。自动化打包和自动化运维部署，一般是开发团队和运维团队干的活。不过，现在每个公司都是追求全栈，全能工程师，牛人需要每个环节都参与。三者之间，现在已经紧密合作关系，所以，现在经常听到有这样的描述：A是开发人员里面，做测试做得最好的；B是测试里面，写代码能力最好的；C是运维里面，开发能力最强的。好想又扯远了，带偏了主题。下面，跟着几个步骤，完成Jenkins的环境搭建。 2. 安装本导读将向您介绍使用 Jenkins、Jenkins 的主要特性和 Jenkins Pipeline 的基本知识。 本导读使用“独立”的 Jenkins 发行版，它可以在您自己本地的机器上运行。 1. 准备工作第一次使用 Jenkins，您需要： 机器要求： 256 MB 内存，建议大于 512 MB 10 GB 的硬盘空间（用于 Jenkins 和 Docker 镜像） 需要安装以下软件： Java 8 ( JRE 或者 JDK 都可以) Docker （导航到网站顶部的Get Docker链接以访问适合您平台的Docker下载） 2. 下载并运行 Jenkins 下载 Jenkins. 打开终端进入到下载目录. 运行命令 java -jar jenkins.war --httpPort=8080. 打开浏览器进入链接 http://localhost:8080. 按照说明完成安装. 安装完成后，您可以开始使用 Jenkins，当然，有时候你还需要手动开启你的防火墙端口 123# config firewallfirewall-cmd --permanent --zone=public --add-port=8080/tcpfirewall-cmd --reload 3. 设置语言 安装Locale Plugin, 重启后生效。 配置【Manage Jenkins】&gt;【Configure System】&gt; 【Locale】 语言填zh_CN，勾选强制设置语言]]></content>
      <tags>
        <tag>jenkins</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell基本运算符]]></title>
    <url>%2F2019%2F04%2F15%2Flinux%2F2.shell%E5%9F%BA%E6%9C%AC%E8%AA%9E%E6%B3%95%2F5.shell%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[5.shell基本运算符Shell 和其他编程语言一样，支持多种运算符，包括： 算数运算符 关系运算符 布尔运算符 字符串运算符 文件测试运算符 原生bash不支持简单的数学运算，但是可以通过其他命令来实现，例如 awk 和 expr，expr 最常用。 exprexpr 是一款表达式计算工具，使用它能完成表达式的求值操作。 1234#!/bin/bashval=`expr 2 + 2`echo "两数之和为 : $val" 执行脚本，输出结果如下所示： 1两数之和为 : 4 两点注意： 表达式和运算符之间要有空格，例如 2+2 是不对的，必须写成 2 + 2，这与我们熟悉的大多数编程语言不一样。完整的表达式要被 ` ` 包含，注意这个字符不是常用的单引号，在 Esc 键下边。 算术运算符下表列出了常用的算术运算符，假定变量 a 为 10，变量 b 为 20： 12345678+ 加法 `expr $a + $b` 结果为 30。- 减法 `expr $a - $b` 结果为 -10。* 乘法 `expr $a \* $b` 结果为 200。/ 除法 `expr $b / $a` 结果为 2。% 取余 `expr $b % $a` 结果为 0。= 赋值 a=$b 将把变量 b 的值赋给 a。== 相等。用于比较两个数字，相同则返回 true。 [ $a == $b ] 返回 false。!= 不相等。用于比较两个数字，不相同则返回 true。 [ $a != $b ] 返回 true。 关系运算符关系运算符只支持数字，不支持字符串，除非字符串的值是数字 123456eq equal的缩写，表示等于为真ne not equal的缩写，表示不等于为真gt greater than的缩写，表示大于为真ge greater&amp;equal的缩写，表示大于等于为真lt lower than的缩写，表示小于为真le lower&amp;equal的缩写，表示小于等于为真 示例:1234567891011#!/bin/basha=10b=20if [ $a -eq $b ]then echo &quot;$a -eq $b : a 等于 b&quot;else echo &quot;$a -eq $b: a 不等于 b&quot;fi 布尔运算符下表列出了常用的布尔运算符，假定变量 a 为 10，变量 b 为 20： 123! 非运算，表达式为 true 则返回 false，否则返回 true。 [ ! false ] 返回 true。-o 或运算，有一个表达式为 true 则返回 true。 [ $a -lt 20 -o $b -gt 100 ] 返回 true。-a 与运算，两个表达式都为 true 才返回 true。 [ $a -lt 20 -a $b -gt 100 ] 返回 false。 逻辑运算符以下介绍 Shell 的逻辑运算符，假定变量 a 为 10，变量 b 为 20: 12&amp;&amp; 逻辑的 AND [[ $a -lt 100 &amp;&amp; $b -gt 100 ]] 返回 false|| 逻辑的 OR [[ $a -lt 100 || $b -gt 100 ]] 返回 true 字符串运算符下表列出了常用的字符串运算符，假定变量 a 为 “abc”，变量 b 为 “efg”： 12345= 检测两个字符串是否相等，相等返回 true。 [ $a = $b ] 返回 false。!= 检测两个字符串是否相等，不相等返回 true。 [ $a != $b ] 返回 true。-z 检测字符串长度是否为0，为0返回 true。 [ -z $a ] 返回 false。-n 检测字符串长度是否为0，不为0返回 true。 [ -n &quot;$a&quot; ] 返回 true。$ 检测字符串是否为空，不为空返回 true。 [ $a ] 返回 true。 文件测试运算符文件测试运算符用于检测 Unix 文件的各种属性。]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker获得宿主主机公网]]></title>
    <url>%2F2019%2F04%2F15%2Fdocker%2F14.docker%E8%8E%B7%E5%BE%97%E5%AE%BF%E4%B8%BB%E7%9A%84Ip%2F</url>
    <content type="text"><![CDATA[12345# 获得宿主主机公网ipbash-4.4# /sbin/ip route|awk '/default/ &#123; print $3 &#125;'172.17.0.1# 连接到宿主主机，从而可以操纵宿主主机bash-4.4# ssh root@172.17.0.1]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[硬件资源管理与外设设备使用]]></title>
    <url>%2F2019%2F04%2F12%2Flinux%2F1.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F06%20%E7%A1%AC%E4%BB%B6%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E4%B8%8E%E5%A4%96%E8%AE%BE%E8%AE%BE%E5%A4%87%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[硬件资源的管理123dmesg | grep -i vga # 显卡lspci | grep -i eth # 网卡lspci | grep -i vga # 声卡]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell流程控制]]></title>
    <url>%2F2019%2F04%2F12%2Flinux%2F2.shell%E5%9F%BA%E6%9C%AC%E8%AA%9E%E6%B3%95%2F6.shell%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[if else 条件语句1234567891011121314a=10b=20if [ $a == $b ]then echo "a 等于 b"elif [ $a -gt $b ]then echo "a 大于 b"elif [ $a -lt $b ]then echo "a 小于 b"else echo "没有符合的条件"fi 输出结果：1a 小于 b 注意： 条件表达式要放在方括号之间，并且要有空格，例如: [$a==$b] 是错误的，必须写成 [ $a == $b ] for 循环1234for i in 1 2 3 4 5do echo "The value is: $i"done 也可以写成一行,一行的写法是，使用;代替换行符即可: 1for i in 1 2 3 4 5;do echo &quot;The value is: $i&quot;;done 输出： 12345The value is: 1The value is: 2The value is: 3The value is: 4The value is: 5 while 语句1234567#!/bin/bashint=1while(( $int&lt;=5 ))do echo $int let "int++"done case取值后面必须为单词in，每一模式必须以右括号结束。取值可以为变量或常数。匹配发现取值符合某一模式后，其间所有命令开始执行直至 ;;。 取值将检测匹配的每一个模式。一旦模式匹配，则执行完匹配模式相应命令后不再继续其他模式。如果无一匹配模式，使用星号 * 捕获该值，再执行后面的命令。 123456789101112131415echo '输入 1 到 4 之间的数字:'echo '你输入的数字为:'read aNumcase $aNum in 1) echo '你选择了 1' ;; 2) echo '你选择了 2' ;; 3) echo '你选择了 3' ;; 4) echo '你选择了 4' ;; *) echo '你没有输入 1 到 4 之间的数字' ;;esac 跳出循环在循环过程中，有时候需要在未达到循环结束条件时强制跳出循环，Shell使用两个命令来实现该功能：break和continue。 break命令: break命令允许跳出所有循环（终止执行后面的所有循环）continue命令: 它不会跳出所有循环，仅仅跳出当前循环 1234567891011121314#!/bin/bashwhile :do echo -n "输入 1 到 5 之间的数字: " read aNum case $aNum in 1|2|3|4|5) echo "你输入的数字为 $aNum!" ;; *) echo "你输入的数字不是 1 到 5 之间的!" continue echo "游戏结束" ;; esacdone]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03 Linux系统维护管理]]></title>
    <url>%2F2019%2F04%2F12%2Flinux%2F1.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F03%20Linux%E7%B3%BB%E7%BB%9F%E7%BB%B4%E6%8A%A4%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[12345678910data 查看日期，设置日期clear 清屏man 查看帮助信息who 查看用户信息w 用户信息，更全面uname 操作系统信息uptime 输出系统任务队列信息last 输出上次和过去系统用户登陆的信息,详细的日志信息，包括用户名，终端，ip，登陆时间以及断开时间dmesg 显示开机信息free 显示系统内存状态 date查看日期，设置日期（只有root用户才可以设置时间） 12# dateTue Apr 2 22:28:17 CST 2019 结合+号输出特定的格式 12# date &apos;+%Y年%m月%d日 %H时%M分%S秒&apos;2019年04月02日 22时26分55秒 设置时间 1# data -s 2020-01-01 who123# whoroot tty2 2019-04-02 22:01root pts/0 2019-04-02 22:02 (192.168.199.233) w12345# w 22:37:14 up 36 min, 2 users, load average: 0.00, 0.01, 0.03USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot tty2 22:01 35:22 0.06s 0.06s -bashroot pts/0 192.168.199.233 22:02 2.00s 0.17s 0.00s w uname12# uname -aLinux localhost.localdomain 3.10.0-957.el7.x86_64 #1 SMP Thu Nov 8 23:39:32 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux dmesgdmesg -c 显示并清空开机信息 freeMem：物理内存Swap：虚拟分区，当物理内存不足时，用于缓解问题 [root@localhost ~]# free -h total used free shared buff/cache available Mem: 3.7G 418M 2.9G 14M 412M 3.0G Swap: 2.0G 0B 2.0G]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell文件包含]]></title>
    <url>%2F2019%2F04%2F08%2Flinux%2F2.shell%E5%9F%BA%E6%9C%AC%E8%AA%9E%E6%B3%95%2F9.shell%E6%96%87%E4%BB%B6%E5%8C%85%E5%90%AB%2F</url>
    <content type="text"><![CDATA[语法和其他语言一样，Shell 也可以包含外部脚本。这样可以很方便的封装一些公用的代码作为一个独立的文件。 Shell 文件包含的语法格式如下： 1. filename # 注意点号(.)和文件名中间有一空格 或1source filename 示例创建两个 shell 脚本文件。 test1.sh 代码如下： 12#!/bin/bashurl="http://www.runoob.com" test2.sh 代码如下： 123456789#!/bin/bash#使用 . 号来引用test1.sh 文件. ./test1.sh# 或者使用source包含文件代码source ./test1.shecho "菜鸟教程官网地址：$url" 接下来，我们为 test2.sh 添加可执行权限并执行： 123$ chmod +x test2.sh $ ./test2.sh 菜鸟教程官网地址：http://www.runoob.com 注意： 被包含的文件 test1.sh 不需要可执行权限。]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell的数据类型]]></title>
    <url>%2F2019%2F03%2F29%2Flinux%2F2.shell%E5%9F%BA%E6%9C%AC%E8%AA%9E%E6%B3%95%2F3.shell%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[1. 字符串字符串是shell编程中最常用最有用的数据类型（除了数字和字符串，也没啥其它类型好用了），字符串可以用单引号，也可以用双引号，也可以不用引号。单双引号的区别跟PHP类似。 2. 单引号1str=&apos;this is a string&apos; 单引号字符串的限制： 单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的； 单引号字串中不能出现单独一个的单引号（对单引号使用转义符后也不行），但可成对出现，作为字符串拼接使用。 3. 双引号123your_name='runoob'str="Hello, I know you are \"$your_name\"! \n"echo -e $str 输出结果为： 1Hello, I know you are "runoob"! 4. 拼接字符串 拼接字符串，字符串与变量之间无需任何连接符，也不能有空格 1your_name="runoob" 12345678# 使用双引号拼接greeting="hello, "$your_name" !"greeting_1="hello, $&#123;your_name&#125; !"echo $greeting $greeting_1# 使用单引号拼接greeting_2='hello, '$your_name' !'greeting_3='hello, $&#123;your_name&#125; !'echo $greeting_2 $greeting_3 输出结果为： 12hello, runoob ! hello, runoob !hello, runoob ! hello, $&#123;your_name&#125; ! 获取字符串长度12string=&quot;abcd&quot;echo $&#123;#string&#125; #输出 4 5. 提取子字符串以下实例从字符串第 2 个字符开始截取 4 个字符： 12string=&quot;runoob is a great site&quot;echo $&#123;string:1:4&#125; # 输出 unoo 6. 查找子字符串查找字符 i 或 o 的位置(哪个字母先出现就计算哪个)： 12string=&quot;runoob is a great site&quot;echo `expr index &quot;$string&quot; io` # 输出 4 7. 数组在 Shell 中，用括号( )来表示数组，数组元素之间用空格来分隔。由此，定义数组的一般形式为： 1array_name=(ele1 ele2 ele3 ... elen) 下面是一个数组的实例： 12345names=("lee" "zhang" "wang")echo $&#123;names[1]&#125; # 可以输出指定下标的数组names[3]="zhao" # 可以使用下标为数组赋值echo $&#123;names[@]&#125; # 可以使用@输出全部的内容echo $names # 缺省下标为0 输出结果如下： 123zhanglee zhang wang zhaolee 8. 注释单行注释以 # 开头的行就是注释，会被解释器忽略。 通过每一行加一个 # 号设置多行注释，像这样： 123456#--------------------------------------------# 这是一个注释# author：菜鸟教程# site：www.runoob.com# slogan：学的不仅是技术，更是梦想！#-------------------------------------------- 多行注释如果在开发过程中，遇到大段的代码需要临时注释起来，过一会儿又取消注释，怎么办呢？ 12345:&lt;&lt;EOF注释内容...注释内容...注释内容...EOF 12345678910:&lt;&lt;'注释内容...注释内容...注释内容...':&lt;&lt;!注释内容...注释内容...注释内容...!]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-machine的使用]]></title>
    <url>%2F2019%2F03%2F26%2Fdocker%2F8.Docker-machine%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[等待完善…]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker网络设置]]></title>
    <url>%2F2019%2F03%2F25%2Fdocker%2F13.docker%E7%BD%91%E7%BB%9C%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1. 网络驱动程序Docker的网络子系统是可插拔的，使用驱动程序。默认情况下存在多个驱动程序，并提供核心网络功能： bridge：默认网络驱动程序。如果未指定驱动程序，则这是您要创建的网络类型。当您的应用程序在需要通信的独立容器中运行时，通常会使用桥接网络。查看 桥接网络。 host：对于独立容器，删除容器和Docker主机之间的网络隔离，并直接使用主机的网络。host 仅适用于Docker 17.06及更高版本的swarm服务。请参阅 使用主机网络。 overlay：覆盖网络将多个Docker守护程序连接在一起，并使群集服务能够相互通信。您还可以使用覆盖网络来促进群集服务和独立容器之间的通信，或者在不同Docker守护程序上的两个独立容器之间进行通信。此策略消除了在这些容器之间执行OS级别路由的需要。请参阅覆盖网络。 macvlan：Macvlan网络允许您为容器分配MAC地址，使其显示为网络上的物理设备。Docker守护程序通过其MAC地址将流量路由到容器。macvlan 在处理期望直接连接到物理网络的传统应用程序时，使用驱动程序有时是最佳选择，而不是通过Docker主机的网络堆栈进行路由。见 Macvlan网络。 none：对于此容器，禁用所有网络。通常与自定义网络驱动程序一起使用。none不适用于群组服务。请参阅 禁用容器网络。 network plugins：您可以使用Docker安装和使用第三方网络插件。这些插件可从 Docker Hub 或第三方供应商处获得。有关安装和使用给定网络插件的信息，请参阅供应商的文档。 网络驱动摘要 当您需要多个容器在同一个Docker主机上进行通信时，用户定义的bridge网络是最佳选择。 当网络堆栈不应与Docker主机隔离时，host网络是最好的，但您希望隔离容器的其他方面。 当您需要在不同Docker主机上运行的容器进行通信时，或者当多个应用程序使用swarm服务协同工作时，overlay是最佳选择。 当您从VM设置迁移或需要容器看起来像网络上的物理主机时，Macvlan网络是最佳的，每个主机都具有唯一的MAC地址。 第三方网络插件允许您将Docker与专用网络堆栈集成。 2. 使用桥接网络在网络方面，桥接网络是在网络段之间转发流量的链路层设备。网桥可以是硬件设备或在主机内核中运行的软件设备。 就Docker而言，桥接网络使用软件桥，该桥接器允许连接到同一桥接网络的容器进行通信，同时提供与未连接到该桥接网络的容器的隔离。Docker桥驱动程序自动在主机中安装规则，以便不同网桥上的容器无法直接相互通信。 桥接网络适用于在同一个 Docker守护程序主机上运行的容器。对于在不同Docker守护程序主机上运行的容器之间的通信，您可以在操作系统级别管理路由，也可以使用覆盖网络。 启动Docker时，会自动创建默认桥接网络（也称为bridge），并且除非另行指定，否则新启动的容器将连接到该网络。您还可以创建用户定义的自定义网桥。用户定义的网桥优于默认bridge 网络。 1. 用户定义的网桥与默认网桥之间的差异 用户定义的桥接器可在容器化应用程序之间提供更好的隔离和互操作性。 连接到同一用户定义的网桥的容器会自动将所有端口相互暴露，并且不会向外界显示任何端口。这使得容器化应用程序可以轻松地相互通信，而不会意外地打开对外界的访问。 想象一下具有Web前端和数据库后端的应用程序。外部世界需要访问Web前端（可能在端口80上），但只有后端本身需要访问数据库主机和端口。使用用户定义的网桥，只需要打开Web端口，并且数据库应用程序不需要打开任何端口，因为Web前端可以通过用户定义的网桥访问它。 如果在默认网桥上运行相同的应用程序堆栈，则需要打开Web端口和数据库端口，并使用 每个的标记-p或--publish标记。这意味着Docker主机需要通过其他方式阻止对数据库端口的访问。 用户定义的桥接器在容器之间提供自动DNS解析。 默认网桥上的容器只能通过IP地址相互访问，除非您使用被认为是遗留的--link选项。在用户定义的桥接网络上，容器可以通过名称或别名相互解析。 想象一下与前一点相同的应用程序，具有Web前端和数据库后端。如果你打电话给你的容器web和db，Web容器可以在连接到数据库容器db，无论哪个码头工人托管应用程序堆栈上运行。 如果在默认桥接网络上运行相同的应用程序堆栈，则需要在容器之间手动创建链接（使用旧--link 标志）。这些链接需要在两个方向上创建，因此您可以看到这对于需要通信的两个以上容器而言变得复杂。或者，您可以操作/etc/hosts容器中的文件，但这会产生难以调试的问题。 容器可以在运行中与用户定义的网络连接和分离。 在容器的生命周期中，您可以动态地将其与用户定义的网络连接或断开连接。要从默认桥接网络中删除容器，您需要停止容器并使用不同的网络选项重新创建容器。 每个用户定义的网络都会创建一个可配置的网桥。 如果容器使用默认网桥，则可以对其进行配置，但所有容器都使用相同的设置，例如MTU和iptables规则。此外，配置默认桥接网络发生在Docker本身之外，并且需要重新启动Docker。 使用创建和配置用户定义的网桥 docker network create。如果不同的应用程序组具有不同的网络要求，则可以在创建时单独配置每个用户定义的网桥。 默认桥接网络上的链接容器共享环境变量。 最初，在两个容器之间共享环境变量的唯一方法是使用--link标志链接它们。用户定义的网络无法实现这种类型的变量共享。但是，有更好的方法来共享环境变量。一些想法： 多个容器可以使用Docker卷装入包含共享信息的文件或目录。 可以一起启动多个容器docker-compose，并且compose文件可以定义共享变量。 您可以使用swarm服务而不是独立容器，并利用共享机密和 配置。 连接到同一用户定义的网桥的容器有效地将所有端口相互暴露。对于可以访问不同网络上的容器或非Docker主机的端口，必须使用or 标志发布该端口。-p`–publish` 2. 管理用户定义的桥使用此docker network create命令可以创建用户定义的桥接网络。 1$ docker network create my-net 您可以指定子网，IP地址范围，网关和其他选项。有关详细信息，请参阅 docker network create reference或输出docker network create --help。 使用此docker network rm命令删除用户定义的桥接网络。如果容器当前已连接到网络， 请先断开它们 。 1$ docker network rm my-net 真的发生了什么？ 当您创建或删除用户定义的网桥或从用户定义的网桥连接或断开容器时，Docker使用特定于操作系统的工具来管理底层网络基础结构（例如iptables在Linux上添加或删除网桥设备或配置规则） ）。这些细节应视为实施细节。让Docker为您管理用户定义的网络。 3. 将容器连接到用户定义的网桥创建新容器时，可以指定一个或多个--network标志。此示例将Nginx容器连接到my-net网络。它还将容器中的端口80发布到Docker主机上的端口8080，因此外部客户端可以访问该端口。连接到my-net 网络的任何其他容器都可以访问my-nginx容器上的所有端口，反之亦然。 1234$ docker create --name my-nginx \ --network my-net \ --publish 8080:80 \ nginx:latest 要将正在运行的容器连接到现有的用户定义的桥，请使用该 docker network connect命令。以下命令将已在运行的my-nginx容器连接 到已存在的my-net网络： 1$ docker network connect my-net my-nginx 4. 断开容器与用户定义的桥接器的连接要断开正在运行的容器与用户定义的桥接器的连接，请使用该docker network disconnect命令。以下命令将my-nginx 容器与my-net网络断开连接。 1$ docker network disconnect my-net my-nginx 5. 使用IPv6如果需要对Docker容器的IPv6支持，则需要 在创建任何IPv6网络或分配容器IPv6地址之前启用 Docker守护程序上的选项并重新加载其配置。 创建网络时，可以指定--ipv6标志以启用IPv6。您无法在默认bridge网络上有选择地禁用IPv6支持。 6. 启用从Docker容器转发到外部世界默认情况下，来自连接到默认网桥的容器的流量 不会转发到外部世界。要启用转发，您需要更改两个设置。这些不是Docker命令，它们会影响Docker主机的内核。 配置Linux内核以允许IP转发。 1$ sysctl net.ipv4.conf.all.forwarding=1 将策略的iptables FORWARD策略更改DROP为 ACCEPT。 1$ sudo iptables -P FORWARD ACCEPT 这些设置在重新启动时不会持续存在，因此您可能需要将它们添加到启动脚本中。 7. 使用默认桥接网络默认bridge网络被视为Docker的遗留细节，不建议用于生产。配置它是一种手动操作，它有 技术缺点。 将容器连接到默认桥接网络如果未使用该--network标志指定网络，并且指定了网络驱动程序，则默认情况下容器将连接到默认bridge网络。连接到默认bridge网络的容器只能通过IP地址进行通信，除非它们使用旧--link标志进行链接 。 配置默认网桥要配置默认bridge网络，请在中指定选项daemon.json。这是一个daemon.json指定了几个选项的示例。仅指定您需要自定义的设置。 123456789&#123; "bip": "192.168.1.5/24", "fixed-cidr": "192.168.1.5/25", "fixed-cidr-v6": "2001:db8::/64", "mtu": 1500, "default-gateway": "10.20.1.1", "default-gateway-v6": "2001:db8:abcd::89", "dns": ["10.20.1.2","10.20.1.3"]&#125; 重新启动Docker以使更改生效。 将IPv6与默认网桥一起使用如果将Docker配置为支持IPv6（请参阅使用IPv6），则还会自动为IPv6配置默认网桥。与用户定义的网桥不同，您无法在默认网桥上有选择地禁用IPv6。 3. 使用主机网络如果host对容器使用网络驱动程序，则该容器的网络堆栈不会与Docker主机隔离。例如，如果您运行绑定到端口80 host的容器并使用网络，则容器的应用程序将在主机IP地址的端口80上可用。 主机网络驱动程序仅适用于Linux主机，并且不支持Docker Desktop for Mac，Docker Desktop for Windows或Docker EE for Windows Server。 在Docker 17.06及更高版本中，您还可以host通过传递--network host给docker container create命令将网络用于群组服务。在这种情况下，控制流量（与管理群集和服务相关的流量）仍然通过覆盖网络发送，**但各个群集服务容器使用Docker守护程序的主机网络和端口发送数据。这会产生一些额外的限制。例如，如果服务容器绑定到端口80，则只有一个服务容器可以在给定的swarm节点上运行。 如果您的容器或服务未发布端口，则主机网络无效。]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker宿主主机与容器拷贝文件]]></title>
    <url>%2F2019%2F03%2F25%2Fdocker%2F12.docker%E5%AE%BF%E4%B8%BB%E4%B8%BB%E6%9C%BA%E4%B8%8E%E5%AE%B9%E5%99%A8%E6%8B%B7%E8%B4%9D%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[1. 从容器拷贝文件到宿主机语法： docker cp 容器名：容器中要拷贝的文件名及其路径 要拷贝到宿主机里面对应的路径 示例： 将容器：mycontainer 中路径：/opt/testnew/下的文件：file.txt拷贝到宿主机/opt/test/路径下，在宿主机中执行命令如下： 1$ docker cp mycontainer:/opt/testnew/file.txt /opt/test/ 2. 从宿主机拷贝文件到容器语法： docker cp 宿主机中要拷贝的文件名及其路径 容器名：要拷贝到容器里面对应的路径 示例： 将宿主机中路径：/opt/test/下的文件：file.txt拷贝到容器：mycontainer的/opt/testnew/路径下，同样还是在宿主机中执行命令如下： 1$ docker cp /opt/test/file.txt mycontainer：/opt/testnew/ 需要注意的是，不管容器有没有启动，拷贝命令都会生效。]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用dd命令快速生成大文件或者小文件的方法]]></title>
    <url>%2F2019%2F03%2F23%2Flinux%2F1.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F%E4%BD%BF%E7%94%A8dd%E5%91%BD%E4%BB%A4%E5%BF%AB%E9%80%9F%E7%94%9F%E6%88%90%E5%A4%A7%E6%96%87%E4%BB%B6%E6%88%96%E8%80%85%E5%B0%8F%E6%96%87%E4%BB%B6%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在程序的测试中有些场景需要大量的小文件或者几个比较大的文件，而在我们的文件系统里一时无法找到那么多或者那么大的文件，此时linux的dd命令就能快速的帮助你完成想要的文件。具体用法简单总结如下： dd命令可以轻易实现创建指定大小的文件，如1dd if=/dev/zero of=A bs=1M count=1000 在当前目录下会生成一个1000M的test文件，文件内容为全0（因从/dev/zero中读取，/dev/zero为0源），但是这样为实际写入硬盘，文件产生速度取决于硬盘读写速度，如果欲产生超大文件，速度很慢。在某种场景下，我们只想让文件系统认为存在一个超大文件在此，但是并不实际写入硬盘 则可以 1dd if=/dev/zero of=A bs=1M count=0 seek=1000 此时创建的文件在文件系统中的显示大小为10000MB，但是并不实际占用block，因此创建速度与内存速度相当，seek的作用是跳过输出文件中指定大小的部分，这就达到了创建大文件，但是并不实际写入的目的。当然，因为不实际写入硬盘，所以你在容量只有10G的硬盘上创建100G的此类文件都是可以的。 随机生成1百万个1K的文件1seq 1000000 | xargs -i dd if=/dev/zero of=&#123;&#125;.dat bs=1024 count=1 当你不需要关心随机文件的内容，只需一个固定大小的文件Solaris、Mac OS X等Unix系统中mkfile指令，可以产生指定大小的文件，而Linux上则没有例子：mkfile -n 160g test1Linux可以用dd指令，/dev/zero是一个特别的文件描述符可以通过它返回null值例子：dd if=/dev/zero of=test.file count=1024 bs=1024产生count * bs字节的文件，1M此方法生成随机文件的好处在于效率高（产生1G文件大概1s）,创建的文件大小精确到字节坏处也有使用null字符来填充文件内容，文件统计时没有行（wc -l test.file为0） 当你不需要关心随机文件的内容，但期望测试文件能有统计的行1dd if=/dev/urandom of=A bs=1M count=100 将/dev/zero 改为/dev/urandom，/dev/urandom是linux下的随机数生成器 关于/dev/urandom跟/dev/random两者的区别就不在此详细讨论，大概就是，前者是不受系统interrupts的限制，即使没有足够的interrupt它也能通过随机数生成器产生足够的输出值；而后者如果用在dd上，它不能被ctrl+c或者kill -9中断，如果ds的值较大时，产生的随机值不足而长期占用CPU。虽然说/dev/random产生的随机数会更随机些，但与dd混用还是建议用/dev/urandom效率更高。 缺点跟/dev/zero比当然是效率会更低些了，生成个100Mb的文件需要10秒左右，而且文件并没有可读的内容，一般的情况基本上是满足了。 漏了说句，dd是linux与unix都支持的指令。 当你关心文件的随机内容行数，而不关心内容是否有所重复这里的思路就是找一个参照文件（比如说2行），将文件重新定向到新的文件，再mv覆盖保存，外加一个for循环。（n为循环次数，产生的文件行为2^（n+1）） 例子:假设先建立一个file.txt文件，里面含有Hello 和 World两行 1for i in &#123;1..n&#125;; do cat file.txt file.txt &gt; file2.txt &amp;&amp; mv file2.txt file.txt; done 由于是阶乘，n=20左右已经是200W行，效率会下降地比较厉害 当你关心随机文件的内容，而不想出现重复内容行情况这种情况下系统的指令应该是不能满足了，或者可以通过操作系统的指令写一大串脚本也可以达到，但不建议这么做，因为可读性和维护性考虑，你应该要引入Ruby或者Python类的脚本语言帮忙了但还是要借助些系统的东西来帮忙 思路：/usr/share/dict/words里面有记录一些单词，一共235886行，每行一个单词可以从里面挑选一些作为文件的内容加循环达到我们想要的随机文件要求 举例：ruby -e ‘a=STDIN.readlines;X.times do; b=[];Y.times do; b&lt;&lt;a[rand(a.size)].chomp end; puts b.join(“ “)’ &lt; /usr/share/dict/words &gt; file.txt X为随机文件需要的行数，Y为从words中读取的单词，虽说组合成一句的命令，还是可以读懂的；从标准输入中重复读取Y个单词，写入到b列表中，然后再通过join空格连接内容写入到标准输出文件file.txt中 这样基本很少会有重复的行了，而且生成的效率与其他方法对比还是可以的，10秒生成100Mb文件。欢迎大家讨论。]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下查看PCI-E插槽信息的方法]]></title>
    <url>%2F2019%2F03%2F22%2Flinux%2F1.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2FLinux%E4%B8%8B%E6%9F%A5%E7%9C%8BPCI-E%E6%8F%92%E6%A7%BD%E4%BF%A1%E6%81%AF%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在 Linux 下要如何得知 PCI-E Bus 使用的是 Gen(Generation) 1 還是 Gen2 還是新一代的 Gen 3 雖然使用 #lspci 只要可以看到目前系統所有的裝置.但是好像看不到 PCI-E Bus 所採用的是哪一代的 PCI-E. 12345678910[root@benjr ~]# lspci00:00.0 Class 0604: 16c3:abcd00:01.0 Class 0604: 16c3:abcd00:02.0 Class 0604: 16c3:abcd00:03.0 Class 0604: 16c3:abcd01:00.0 Class 0604: 10b5:87160d:00.0 Class 0c04: 1077:29710d:00.1 Class 0c04: 1077:29710d:00.2 Class 0c04: 1077:29710d:00.3 Class 0c04: 1077:2971 如果有裝置是 unknown 的,需要更新 /usr/share/hwdata/pci.ids 請參考更新方式 http://benjr.tw/node/88 首先我們先來複習一下 PCI-E bus 的速度上限. PCI Express 1.1使用兩對低電壓的差位訊號排線（low-voltage differential signaling pairs），分別各跑2.5GBit/s速度,下面的速度是以單對的速度而言. x1 有兩對 2.5 G x 2 = 5Gbps 的頻寬. 1234x1 2.5Gbps(20% overhead - PCI-e 在每八個位元的資料串上用十位元來加以編碼) 2Gbps (250 MB/sec)x4 10Gbps 8Gbps (1 GB/sec)x8 20Gbps 16Gbps (2GB/sec)x16 40Gbps 32Gbps (4GB/sec) PCI Express 2.0PCI-SIG 的 PCI Express 2.0規格，新版每條Lane的單向頻寬從2.5Gbps倍增到5Gbps. 12345x1 5Gbps(20% overhead-PCIe並且在每八個位元的資料串上用十位元來加以編碼) 4Gbps (500 MB/sec)(5G*0.8)Mb/8=500MBx4 20Gbps 16Gbps (2 GB/sec)x8 40Gbps 32Gbps (4 GB/sec)x16 80Gbps 64Gbps (8 GB/sec) 我的系統上有一張 Qlogic Chipset 為 2432 的 4G Fiber Channel HBA,要如何得知目前系統的 PCI-E Bus 的速度呢!!首先要查出這張 HBA 的裝置名稱. 1234567891011121314151617181920212223[root@benjr ~]# lspci -n00:00.0 0600: 8086:29f0 (rev 01)00:01.0 0604: 8086:29f1 (rev 01)00:1a.0 0c03: 8086:2937 (rev 02)00:1a.1 0c03: 8086:2938 (rev 02)00:1a.2 0c03: 8086:2939 (rev 02)00:1a.7 0c03: 8086:293c (rev 02)00:1c.0 0604: 8086:2948 (rev 02)00:1c.1 0604: 8086:294a (rev 02)00:1c.2 0604: 8086:2940 (rev 02)00:1d.0 0c03: 8086:2934 (rev 02)00:1d.1 0c03: 8086:2935 (rev 02)00:1d.2 0c03: 8086:2936 (rev 02)00:1d.7 0c03: 8086:293a (rev 02)00:1e.0 0604: 8086:244e (rev 92)00:1f.0 0601: 8086:2916 (rev 02)00:1f.2 0106: 8086:2922 (rev 02)00:1f.3 0c05: 8086:2930 (rev 02)03:00.0 0200: 14e4:165a04:03.0 0300: 1002:515e (rev 02)09:00.0 0c04: 1077:2432 (rev 03)09:00.1 0c04: 1077:2432 (rev 03)0c:00.0 0100: 1000:0056 (rev 02) 可以看到目前 Qlogic 2432 的 PCI 名稱以及裝置名稱為 09:00.0 0c04: 1077:2432 (rev 03) 先來看看這些數字所代表的意義.前面的 3 個數字 “09:00.0” 是各代表什麼意思. 在 PCI 的裝置使用三個編號用來當作識別值,個別為: 匯流排(bus number) 裝置(device number) 功能(function number) 所以剛剛的 09:00.0 就是 bus number = 09 ,device number = 00 function = 0 . 這3個編號會組合成一個 16-bits 的識別碼, 匯流排(bus number) 8bits 2^8 至多可連接 256 個匯流排(0 to ff),裝置(device number) 5bits 2^5 至多可接 32 種裝置(0 to 1f) 以及功能(function number) 3bits 2^3 至多每種裝置可有 8 項功能(0 to 7) 關於更多 #lspci 的資訊請參考 http://benjr.tw/node/543 不過在 Linux 使用 Class ID + Vendor ID + Device ID 來代表裝置,如剛剛的 0c04: 1077:2432 所代表裝置名稱為 (Class ID = 0c04 ,Vendor ID = 1077,Device ID =2432) . 0c04 : class 0c04 表示是 “Fiber Channel controller” 1077 : vendor ID 1077 製造廠商 “Qlogic Corp” 2432 : device ID 2432 產品名稱 “ISP2432-based 4Gb Fiber Channel to PCI Express HBA” 你問我怎麼知道 ID 與名稱是怎麼對應的很簡單直接參考 /usr/share/hwdata/pci.ids 檔案即可. 获取PCI-Express速度接下來透過指令 #lspci -n -d 1077:2432 -vvv |grep -i width 就可以得知 PCI-Express 的速度了. 123456[root@benjr ~]# /usr/sbin/lspci -d 1077:2971 -vv grep -i widthLnkCap: Port #0, Speed 2.5GT/s, Width x4, ASPM L0s, Latency L0 &lt;4us, L1 unlimitedLnkSta: Speed 2.5GT/s, Width x1, TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt-LnkCap: Port #0, Speed 2.5GT/s, Width x4, ASPM L0s, Latency L0 &lt;4us, L1 unlimitedLnkSta: Speed 2.5GT/s, Width x1, TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt- LnkSta : 目前系統所提供的速度 PCI-Express 1.0 ( 2.5G ) ,如果是 PCI-Express 2.0 那速度是 5G LnkCap : 裝置目前所採用的速度. LnkSta 和 LnkCap 這兩個速度有可能不一樣 ,系統所提供的是 PCI Express 是 2.0 但裝置還是使用 1.0 的.]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell传递参数]]></title>
    <url>%2F2019%2F03%2F19%2Flinux%2F2.shell%E5%9F%BA%E6%9C%AC%E8%AA%9E%E6%B3%95%2F4.shell%E4%BC%A0%E9%80%92%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[我们可以在执行 Shell 脚本时，向脚本传递参数，脚本内获取参数的格式为：$n。n 代表一个数字，1 为执行脚本的第一个参数，2 为执行脚本的第二个参数，以此类推… 1234567#!/bin/bashecho "Shell 传递参数实例！";echo "第一个参数为：$1";echo "参数个数为：$#";echo "传递的参数作为一个字符串显示：$*"; 执行脚本，输出结果如下所示： 123456$ chmod +x test.sh $ ./test.sh 1 2 3Shell 传递参数实例！第一个参数为：1参数个数为：3传递的参数作为一个字符串显示：1 2 3 常见变量： 1234567$# 传递到脚本的参数个数$* 以一个单字符串显示所有向脚本传递的参数。$$ 脚本运行的当前进程ID号$! 后台运行的最后一个进程的ID号$@ 与$*相同，但是使用时加引号，并在引号中返回每个参数。$- 显示Shell使用的当前选项，与set命令功能相同。$? 显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误。 $*与 $@区别： 相同点：都是引用所有参数。 不同点：只有在双引号中体现出来。假设在脚本运行时写了三个参数 1、2、3，，则 “ * “ 等价于 “1 2 3”（传递了一个参数），而 “@” 等价于 “1” “2” “3”（传递了三个参数）。]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 技巧：让进程在后台可靠运行的几种方法]]></title>
    <url>%2F2019%2F03%2F05%2Flinux%2F1.%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2FLinux%20%E6%8A%80%E5%B7%A7%EF%BC%9A%E8%AE%A9%E8%BF%9B%E7%A8%8B%E5%9C%A8%E5%90%8E%E5%8F%B0%E5%8F%AF%E9%9D%A0%E8%BF%90%E8%A1%8C%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[如果只是临时有一个命令需要长时间运行，使用 nohub，setsid，&amp; 如果我们未加任何处理就已经提交了命令,使用 disown 但是如果有大量这种命令需要在稳定的后台里运行，使用 screen 推荐使用setsid进行后台程序启动 nohupnohup 无疑是我们首先想到的办法。顾名思义，nohup 的用途就是让提交的命令忽略 hangup 信号。让我们先来看一下 nohup 的帮助信息： 12345678910111213141516NOHUP(1) User Commands NOHUP(1) NAME nohup - run a command immune to hangups, with output to a non-tty SYNOPSIS nohup COMMAND [ARG]... nohup OPTION DESCRIPTION Run COMMAND, ignoring hangup signals. --help display this help and exit --version output version information and exit 可见，nohup 的使用是十分方便的，只需在要处理的命令前加上 nohup 即可，标准输出和标准错误缺省会被重定向到 nohup.out 文件中。一般我们可在结尾加上”&amp;”来将命令同时放入后台运行，也可用”&gt;filename 2&gt;&amp;1”来更改缺省的重定向文件名。 1234567[root@pvcent107 ~]# nohup ping www.ibm.com &amp;[1] 3059nohup: appending output to `nohup.out'[root@pvcent107 ~]# ps -ef |grep 3059root 3059 984 0 21:06 pts/3 00:00:00 ping www.ibm.comroot 3067 984 0 21:06 pts/3 00:00:00 grep 3059[root@pvcent107 ~]# setsidnohup 无疑能通过忽略 HUP 信号来使我们的进程避免中途被中断，但如果我们换个角度思考，如果我们的进程不属于接受 HUP 信号的终端的子进程，那么自然也就不会受到 HUP 信号的影响了。setsid 就能帮助我们做到这一点。让我们先来看一下 setsid 的帮助信息： 12345678910SETSID(8) Linux Programmer’s Manual SETSID(8) NAME setsid - run a program in a new session SYNOPSIS setsid program [ arg ... ] DESCRIPTION setsid runs a program in a new session. 可见 setsid 的使用也是非常方便的，也只需在要处理的命令前加上 setsid 即可。 setsid 示例 1234[root@pvcent107 ~]# setsid ping www.ibm.com[root@pvcent107 ~]# ps -ef |grep www.ibm.comroot 31094 1 0 07:28 ? 00:00:00 ping www.ibm.comroot 31102 29217 0 07:29 pts/4 00:00:00 grep www.ibm.com 值得注意的是，上例中我们的进程 ID(PID)为31094，而它的父 ID（PPID）为1（即为 init 进程 ID），并不是当前终端的进程 ID。请将此例与nohup 例中的父 ID 做比较。 &amp;这里还有一个关于 subshell 的小技巧。我们知道，将一个或多个命名包含在“()”中就能让这些命令在子 shell 中运行中，从而扩展出很多有趣的功能，我们现在要讨论的就是其中之一。 当我们将”&amp;”也放入“()”内之后，我们就会发现所提交的作业并不在作业列表中，也就是说，是无法通过jobs来查看的。让我们来看看为什么这样就能躲过 HUP 信号的影响吧。 subshell 示例 12345[root@pvcent107 ~]# (ping www.ibm.com &amp;)[root@pvcent107 ~]# ps -ef |grep www.ibm.comroot 16270 1 0 14:13 pts/4 00:00:00 ping www.ibm.comroot 16278 15362 0 14:13 pts/4 00:00:00 grep www.ibm.com[root@pvcent107 ~]# 从上例中可以看出，新提交的进程的父 ID（PPID）为1（init 进程的 PID），并不是当前终端的进程 ID。因此并不属于当前终端的子进程，从而也就不会受到当前终端的 HUP 信号的影响了。 开始使用Screen简单来说，Screen是一个可以在多个进程之间多路复用一个物理终端的窗口管理器。Screen中有会话的概念，用户可以在一个screen会话中创建多个screen窗口，在每一个screen窗口中就像操作一个真实的telnet/SSH连接窗口那样。 常用的命令如下： 1screen -S sessionname 1screen -r sessionname 参考文档Linux 技巧：让进程在后台可靠运行的几种方法]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker访问宿主主机串行设备]]></title>
    <url>%2F2019%2F03%2F05%2Fdocker%2F11.docker%E8%AE%BF%E9%97%AE%E5%AE%BF%E4%B8%BB%E4%B8%BB%E6%9C%BA%E4%B8%B2%E8%A1%8C%E8%AE%BE%E5%A4%87%2F</url>
    <content type="text"><![CDATA[1. 读取宿主主机设备的方案 使用--device 1$ docker run -t -i --device=/dev/ttyUSB0 ubuntu bash 使用--privileged 1$ docker run -t -i --privileged ubuntu bash 2. 特权模式默认情况下，Docker的容器是没有特权的，例如不能在容器中再启动一个容器。这是因为默认情况下容器是不能访问任何其它设备的。但是通过--privileged，容器就拥有了访问任何其它设备的权限。 当操作者执行时，Docker将拥有访问主机所有设备的权限，同时Docker也会在apparmor或者selinux做一些设置，使容器可以容易的访问那些运行在容器外部的设备。 同时，你也可以限制容器只能访问一些指定的设备。下面的命令将允许容器只访问一些特定设备： 1$ sudo docker run --device=/dev/snd:/dev/snd ... 使用--privileged参数，可以访问宿主主机的所有设备 1234$ docker run -it --privileged ztest ➜ serial git:(master) ls /dev tty43 tty52 tty61 ttyUSB1 ttyUSB2 ttyUSB3 ttyUSB4 ... 不使用--privileged参数，无法访问宿主主机设备123[root@host192 ~]# docker run -it ztest➜ /workspace ls /devconsole core fd full mqueue null ptmx pts random shm stderr stdin stdout tty ...]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在docker容器内使用docker]]></title>
    <url>%2F2019%2F03%2F03%2Fdocker%2F10.%E5%9C%A8docker%E5%AE%B9%E5%99%A8%E5%86%85%E4%BD%BF%E7%94%A8docker%2F</url>
    <content type="text"><![CDATA[在容器内调用docker的一种方案是，共享宿主主机的docker给docker容器 1234docker run -it \ -v /var/run/docker.sock:/var/run/docker.sock \ -v /usr/bin/docker:/usr/bin/docker \ jenkins 进入容器后，可以正常使用docker命令，并且是对宿主主机的docker环境进行操作。 1234 ➜ ~ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEjenkins latest bb82f5385818 About an hour ago 1.9GBztest latest d024d1c7d455 5 days ago 628MB]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Compose的使用]]></title>
    <url>%2F2019%2F02%2F26%2Fdocker%2F7.Docker-compose%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1. Docker Compose概述Compose是一个用于定义和运行多容器Docker应用程序的工具。使用Compose，您可以使用YAML文件来配置应用程序的服务。然后，使用单个命令，您可以从配置中创建并启动所有服务。要了解有关Compose所有功能的更多信息，请参阅功能列表。 Compose适用于所有环境：生产，登台，开发，测试以及CI工作流程。您可以在常见用例中了解有关每个案例的更多信息。 使用Compose基本上是一个三步过程： 定义您的应用程序环境，Dockerfile以便可以在任何地方进行复制。 定义构成应用程序的服务，docker-compose.yml 以便它们可以在隔离的环境中一起运行。 Run docker-compose up和Compose启动并运行整个应用程序。 一个 docker-compose.yml看起来像这样： 123456789101112131415version: '3'services: web: build: . ports: - "5000:5000" volumes: - .:/code - logvolume01:/var/log links: - redis redis: image: redisvolumes: logvolume01: &#123;&#125; 有关“撰写”文件的详细信息，请参阅“ 撰写文件参考”。 Compose具有管理应用程序整个生命周期的命令： 启动，停止和重建服务 查看正在运行的服务的状态 流式传输运行服务的日志输出 在服务上运行一次性命令 2. 安装Compose在Linux系统上系统上安装compose 运行此命令以下载Docker Compose的当前稳定版本： 1sudo curl -L "https://github.com/docker/compose/releases/download/1.24.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose 对二进制文件应用可执行权限： 1sudo chmod +x /usr/local/bin/docker-compose 在其它系统或者更详细的安装说明，请查看官方文档]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile的使用]]></title>
    <url>%2F2019%2F02%2F25%2Fdocker%2F6.Dockerfile%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1. 概览我们新建一个文件并将其命名为Dockerfile，键入以下内容：1234567FROM node:slimRUN mkdir /appWORKDIR /appCOPY ./package.json /appRUN [ "npm", "install" ]COPY . /app/CMD [ "npm", "start" ] 然后再Dockerfile的同级目录输入以下命令： 1$ docker build -t mynode . 这样，我们就构建完成了一个名为mynode的docke镜像 2. docker build 语法解析 我们构建一个镜像，通常在自己编写号的Dockerfile的同级目录，执行以下命令： 1$ docker build -t runoob/ubuntu:v1 . -t 表示要构建的镜像的Tag . 表示上下文目录 也可以使用URL github.com/creack/docker-firefox 的 Dockerfile 构建镜像。 1$ docker build github.com/creack/docker-firefox 也可以通过 -f Dockerfile 指定DOkcerfile文件的位置构建镜像： 1$ docker build -f /path/to/a/Dockerfile . 在 Docker 守护进程执行 Dockerfile 中的指令前，首先会对 Dockerfile 进行语法检查，有语法错误时会返回： 123$ docker build -t test/myapp .Sending build context to Docker daemon 2.048 kBError response from daemon: Unknown instruction: RUNCMD 3. Dockerfile语法解析1. FROM 指定基础镜像所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。就像我们之前运行了一个 nginx 镜像的容器，再进行修改一样，基础镜像是必须指定的。而 FROM 就是指定基础镜像，因此一个 Dockerfile 中 FROM 是必备的指令，并且必须是第一条指令。 在 Docker Store 上有非常多的高质量的官方镜像，有可以直接拿来使用的服务类的镜像，如 nginx、redis、mongo、mysql、httpd、php、tomcat 等；也有一些方便开发、构建、运行各种语言应用的镜像，如 node、openjdk、python、ruby、golang 等。可以在其中寻找一个最符合我们最终目标的镜像为基础镜像进行定制。 如果没有找到对应服务的镜像，官方镜像中还提供了一些更为基础的操作系统镜像，如 ubuntu、debian、centos、fedora、alpine 等，这些操作系统的软件库为我们提供了更广阔的扩展空间。 除了选择现有镜像为基础镜像外，Docker 还存在一个特殊的镜像，名为 scratch。这个镜像是虚拟的概念，并不实际存在，它表示一个空白的镜像。 1FROM scratch 如果你以 scratch 为基础镜像的话，意味着你不以任何镜像为基础，接下来所写的指令将作为镜像第一层开始存在。 不以任何系统为基础，直接将可执行文件复制进镜像的做法并不罕见，比如 swarm、coreos/etcd。对于 Linux 下静态编译的程序来说，并不需要有操作系统提供运行时支持，所需的一切库都已经在可执行文件里了，因此直接 FROM scratch 会让镜像体积更加小巧。使用 Go 语言 开发的应用很多会使用这种方式来制作镜像，这也是为什么有人认为 Go 是特别适合容器微服务架构的语言的原因之一。 2. RUN 执行命令RUN 指令是用来执行命令行命令的。由于命令行的强大能力，RUN 指令在定制镜像时是最常用的指令之一。其格式有两种： shell 格式：RUN &lt;命令&gt;，就像直接在命令行中输入的命令一样。刚才写的 Dockerfile 中的 RUN 指令就是这种格式。 exec 格式：RUN [“可执行文件”, “参数1”, “参数2”]，这更像是函数调用中的格式。 既然 RUN 就像 Shell 脚本一样可以执行命令，那么我们是否就可以像 Shell 脚本一样把每个命令对应一个 RUN 呢？比如这样： 123456789FROM debian:jessieRUN apt-get updateRUN apt-get install -y gcc libc6-dev makeRUN wget -O redis.tar.gz "http://download.redis.io/releases/redis-3.2.5.tar.gz"RUN mkdir -p /usr/src/redisRUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1RUN make -C /usr/src/redisRUN make -C /usr/src/redis install 之前说过，Dockerfile 中每一个指令都会建立一层，RUN 也不例外。每一个 RUN 的行为，就和刚才我们手工建立镜像的过程一样：新建立一层，在其上执行这些命令，执行结束后，commit 这一层的修改，构成新的镜像。 而上面的这种写法，创建了 7 层镜像。这是完全没有意义的，而且很多运行时不需要的东西，都被装进了镜像里，比如编译环境、更新的软件包等等。结果就是产生非常臃肿、非常多层的镜像，不仅仅增加了构建部署的时间，也很容易出错。 这是很多初学 Docker 的人常犯的一个错误。 Union FS 是有最大层数限制的，比如 AUFS，曾经是最大不得超过 42 层，现在是不得超过 127 层。 上面的 Dockerfile 正确的写法应该是这样： 1234567891011121314FROM debian:jessieRUN buildDeps='gcc libc6-dev make' \ &amp;&amp; apt-get update \ &amp;&amp; apt-get install -y $buildDeps \ &amp;&amp; wget -O redis.tar.gz "http://download.redis.io/releases/redis-3.2.5.tar.gz" \ &amp;&amp; mkdir -p /usr/src/redis \ &amp;&amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \ &amp;&amp; make -C /usr/src/redis \ &amp;&amp; make -C /usr/src/redis install \ &amp;&amp; rm -rf /var/lib/apt/lists/* \ &amp;&amp; rm redis.tar.gz \ &amp;&amp; rm -r /usr/src/redis \ &amp;&amp; apt-get purge -y --auto-remove $buildDeps 首先，之前所有的命令只有一个目的，就是编译、安装 redis 可执行文件。因此没有必要建立很多层，这只是一层的事情。因此，这里没有使用很多个 RUN 对一一对应不同的命令，而是仅仅使用一个 RUN 指令，并使用 &amp;&amp; 将各个所需命令串联起来。将之前的 7 层，简化为了 1 层。在撰写 Dockerfile 的时候，要经常提醒自己，这并不是在写 Shell 脚本，而是在定义每一层该如何构建。 并且，这里为了格式化还进行了换行。Dockerfile 支持 Shell 类的行尾添加 \ 的命令换行方式，以及行首 # 进行注释的格式。良好的格式，比如换行、缩进、注释等，会让维护、排障更为容易，这是一个比较好的习惯。 此外，还可以看到这一组命令的最后添加了清理工作的命令，删除了为了编译构建所需要的软件，清理了所有下载、展开的文件，并且还清理了 apt 缓存文件。这是很重要的一步，我们之前说过，镜像是多层存储，每一层的东西并不会在下一层被删除，会一直跟随着镜像。因此镜像构建时，一定要确保每一层只添加真正需要添加的东西，任何无关的东西都应该清理掉。 很多人初学 Docker 制作出了很臃肿的镜像的原因之一，就是忘记了每一层构建的最后一定要清理掉无关文件。 3. COPY 复制文件格式： 1COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;源路径&gt;... &lt;目标路径&gt; COPY 指令将从构建上下文目录中 &lt;源路径&gt; 的文件/目录复制到新的一层的镜像内的 &lt;目标路径&gt; 位置。比如： 1COPY package.json /usr/src/app/ # 上下文目录:package.json =&gt; 新镜像:/usr/src/app/ 如果&lt;目标路径&gt; 位置文件夹不存在，COPY命令会自动创建该文件夹，无需手动创建 4. CMD 容器启动命令CMD 指令的格式和 RUN 相似，也是两种格式： shell 格式：CMD &lt;命令&gt; exec 格式：CMD [“可执行文件”, “参数1”, “参数2”…] 参数列表格式：CMD [“参数1”, “参数2”…]。在指定了 ENTRYPOINT 指令后，用 CMD 指定具体的参数。 之前介绍容器的时候曾经说过，Docker 不是虚拟机，容器就是进程。既然是进程，那么在启动容器的时候，需要指定所运行的程序及参数。CMD 指令就是用于指定默认的容器主进程的启动命令的。 在运行时可以指定新的命令来替代镜像设置中的这个默认命令，比如，ubuntu 镜像默认的 CMD 是 /bin/bash，如果我们直接 docker run -it ubuntu 的话，会直接进入 bash。我们也可以在运行时指定运行别的命令，如 docker run -it ubuntu cat /etc/os-release。这就是用 cat /etc/os-release 命令替换了默认的 /bin/bash 命令了，输出了系统版本信息。 在指令格式上，一般推荐使用 exec 格式，这类格式在解析时会被解析为 JSON 数组，因此一定要使用双引号 “，而不要使用单引号。 如果使用 shell 格式的话，实际的命令会被包装为 sh -c 的参数的形式进行执行。比如： 1CMD echo $HOME 在实际执行中，会将其变更为： 1CMD [ "sh", "-c", "echo $HOME" ] 这就是为什么我们可以使用环境变量的原因，因为这些环境变量会被 shell 进行解析处理。 提到 CMD 就不得不提容器中应用在前台执行和后台执行的问题。这是初学者常出现的一个混淆。 Docker 不是虚拟机，容器中的应用都应该以前台执行，而不是像虚拟机、物理机里面那样，用 upstart/systemd 去启动后台服务，容器内没有后台服务的概念。 一些初学者将 CMD 写为： 1CMD service nginx start 然后发现容器执行后就立即退出了。甚至在容器内去使用 systemctl 命令结果却发现根本执行不了。这就是因为没有搞明白前台、后台的概念，没有区分容器和虚拟机的差异，依旧在以传统虚拟机的角度去理解容器。 对于容器而言，其启动程序就是容器应用进程，容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义，从而退出，其它辅助进程不是它需要关心的东西。 而使用 service nginx start 命令，则是希望 upstart 来以后台守护进程形式启动 nginx 服务。而刚才说了 CMD service nginx start 会被理解为 CMD [ “sh”, “-c”, “service nginx start”]，因此主进程实际上是 sh。那么当 service nginx start 命令结束后，sh 也就结束了，sh 作为主进程退出了，自然就会令容器退出。 正确的做法是直接执行 nginx 可执行文件，并且要求以前台形式运行。比如： 1CMD ["nginx", "-g", "daemon off;"] 5. ENTRYPOINT 入口点ENTRYPOINT 的格式和 RUN 指令格式一样，分为 exec 格式和 shell 格式。 ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数。ENTRYPOINT 在运行时也可以替代，不过比 CMD 要略显繁琐，需要通过 docker run 的参数 –entrypoint 来指定。 当指定了 ENTRYPOINT 后，CMD 的含义就发生了改变，不再是直接的运行其命令，而是将 CMD 的内容作为参数传给 ENTRYPOINT 指令，换句话说实际执行时，将变为： ““ 那么有了 CMD 后，为什么还要有 ENTRYPOINT 呢？这种 ““ 有什么好处么？让我们来看几个场景。 场景一：让镜像变成像命令一样使用假设我们需要一个得知自己当前公网 IP 的镜像，那么可以先用 CMD 来实现： 12345FROM ubuntu:18.04RUN apt-get update \ &amp;&amp; apt-get install -y curl \ &amp;&amp; rm -rf /var/lib/apt/lists/*CMD [ "curl", "-s", "https://ip.cn" ] 假如我们使用 docker build -t myip . 来构建镜像的话，如果我们需要查询当前公网 IP，只需要执行： 1$ docker run myip 但是之前我们说过，跟在镜像名后面的是 command，运行时会替换 CMD 的默认值。无法实现增加参数的功能： 1$ docker run myip -i # 此时相当于 CMD [ "-i" ] 那么如果我们希望加入 -i 这参数，我们就必须重新完整的输入这个命令： 1$ docker run myip curl -s https://ip.cn -i 这显然不是很好的解决方案，而使用 ENTRYPOINT 就可以解决这个问题。现在我们重新用 ENTRYPOINT 来实现这个镜像： 12345FROM ubuntu:18.04RUN apt-get update \ &amp;&amp; apt-get install -y curl \ &amp;&amp; rm -rf /var/lib/apt/lists/*ENTRYPOINT [ "curl", "-s", "https://ip.cn" ] 这次我们再来尝试直接使用 docker run myip -i： 1$ docker run myip 当前 IP：61.148.226.66 来自：北京市 联通 1234567891011121314$ docker run myip -i# outputHTTP/1.1 200 OKServer: nginx/1.8.0Date: Tue, 22 Nov 2016 05:12:40 GMTContent-Type: text/html; charset=UTF-8Vary: Accept-EncodingX-Powered-By: PHP/5.6.24-1~dotdeb+7.1X-Cache: MISS from cache-2X-Cache-Lookup: MISS from cache-2:80X-Cache: MISS from proxy-2_6Transfer-Encoding: chunkedVia: 1.1 cache-2:80, 1.1 proxy-2_6:8006Connection: keep-alive 当前 IP：61.148.226.66 来自：北京市 联通可以看到，这次成功了。这是因为当存在 ENTRYPOINT 后，CMD 的内容将会作为参数传给 ENTRYPOINT，而这里 -i 就是新的 CMD，因此会作为参数传给 curl，从而达到了我们预期的效果。 场景二：应用运行前的准备工作启动容器就是启动主进程，但有些时候，启动主进程前，需要一些准备工作。 比如 mysql 类的数据库，可能需要一些数据库配置、初始化的工作，这些工作要在最终的 mysql 服务器运行之前解决。 此外，可能希望避免使用 root 用户去启动服务，从而提高安全性，而在启动服务前还需要以 root 身份执行一些必要的准备工作，最后切换到服务用户身份启动服务。或者除了服务外，其它命令依旧可以使用 root 身份执行，方便调试等。 这些准备工作是和容器 CMD 无关的，无论 CMD 为什么，都需要事先进行一个预处理的工作。这种情况下，可以写一个脚本，然后放入 ENTRYPOINT 中去执行，而这个脚本会将接到的参数（也就是 ）作为命令，在脚本最后执行。比如官方镜像 redis 中就是这么做的： 12345678FROM alpine:3.4...RUN addgroup -S redis &amp;&amp; adduser -S -G redis redis...ENTRYPOINT ["docker-entrypoint.sh"] # =&gt; 这个脚本用于使用root权限命令EXPOSE 6379CMD [ "redis-server" ] 该脚本的内容就是根据 CMD 的内容来判断，如果是不然传入参数，默认命令就是 CMD [ &quot;redis-server&quot; ] ，则切换到 redis 用户身份启动服务器，如果添加参数，则将参数传入入口文件，使用 root 身份执行。比如： 1$ docker run -it redis # 启动redis 12$ docker run -it redis id # 调试redisuid=0(root) gid=0(root) groups=0(root) 6. EXPOSE 声明端口格式为 EXPOSE &lt;端口1&gt; [&lt;端口2&gt;…]。 EXPOSE 指令是声明运行时容器提供服务端口，这只是一个声明，在运行时并不会因为这个声明应用就会开启这个端口的服务。在 Dockerfile 中写入这样的声明有两个好处，一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射；另一个用处则是在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。 要将 EXPOSE 和在运行时使用 -p &lt;宿主端口&gt;:&lt;容器端口&gt; 区分开来。-p，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。 7. ENV 设置环境变量格式有两种： 12ENV &lt;key&gt; &lt;value&gt;ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;... 这个指令很简单，就是设置环境变量而已，无论是后面的其它指令，如 RUN，还是运行时的应用，都可以直接使用这里定义的环境变量。 12ENV VERSION=1.0 DEBUG=on \ NAME=&quot;Happy Feet&quot; 这个例子中演示了如何换行，以及对含有空格的值用双引号括起来的办法，这和 Shell 下的行为是一致的。 定义了环境变量，那么在后续的指令中，就可以使用这个环境变量。比如在官方 node 镜像 Dockerfile 中，就有类似这样的代码： 123456789ENV NODE_VERSION 7.2.0RUN curl -SLO &quot;https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.xz&quot; \ &amp;&amp; curl -SLO &quot;https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc&quot; \ &amp;&amp; gpg --batch --decrypt --output SHASUMS256.txt SHASUMS256.txt.asc \ &amp;&amp; grep &quot; node-v$NODE_VERSION-linux-x64.tar.xz\$&quot; SHASUMS256.txt | sha256sum -c - \ &amp;&amp; tar -xJf &quot;node-v$NODE_VERSION-linux-x64.tar.xz&quot; -C /usr/local --strip-components=1 \ &amp;&amp; rm &quot;node-v$NODE_VERSION-linux-x64.tar.xz&quot; SHASUMS256.txt.asc SHASUMS256.txt \ &amp;&amp; ln -s /usr/local/bin/node /usr/local/bin/nodejs 在这里先定义了环境变量 NODE_VERSION，其后的 RUN 这层里，多次使用 $NODE_VERSION 来进行操作定制。可以看到，将来升级镜像构建版本的时候，只需要更新 7.2.0 即可，Dockerfile 构建维护变得更轻松了。 下列指令可以支持环境变量展开： ADD、COPY、ENV、EXPOSE、LABEL、USER、WORKDIR、VOLUME、STOPSIGNAL、ONBUILD。 可以从这个指令列表里感觉到，环境变量可以使用的地方很多，很强大。通过环境变量，我们可以让一份 Dockerfile 制作更多的镜像，只需使用不同的环境变量即可。 8. VOLUME 定义匿名卷格式为： 12VOLUME ["&lt;路径1&gt;", "&lt;路径2&gt;"...]VOLUME &lt;路径&gt; 之前我们说过，容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中，后面的章节我们会进一步介绍 Docker 卷的概念。为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在 Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。 1VOLUME /data 这里的 /data 目录就会在运行时自动挂载为匿名卷，任何向 /data 中写入的信息都不会记录进容器存储层，从而保证了容器存储层的无状态化。当然，运行时可以覆盖这个挂载设置。比如： 1docker run -d -v mydata:/data xxxx 在这行命令中，就使用了 mydata 这个命名卷挂载到了 /data 这个位置，替代了 Dockerfile 中定义的匿名卷的挂载配置。 9. WORKDIR 指定工作目录格式: WORKDIR &lt;工作目录路径&gt; 使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录,这个目录是新镜像构建时的工作目录，与上下文目录无关），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录。 示例： 项目文件结构： 12345--|-lib|-requirements.txt|-main.py|-Dockerfile 12345FROM python:3.7WORKDIR /app # 指定新镜像的工作目录为/appCOPY . /app/ # 将上下文目录（也就是你的项目所在目录）的所有文件复制到镜像的/app/文件夹下RUN ["pip","install","-r","requirements.txt"] # 此处，如果之前没指定WORKDIR，默认工作目录为新镜像的 /，显然无法找到requirements.txtCMD ["python","main.py"] # 此处同理 之前提到一些初学者常犯的错误是把 Dockerfile 等同于 Shell 脚本来书写，这种错误的理解还可能会导致出现下面这样的错误： 错误示例 12RUN cd /appRUN echo "hello" &gt; world.txt 如果将这个 Dockerfile 进行构建镜像运行后，会发现找不到 /app/world.txt 文件，或者其内容不是 hello。原因其实很简单，在 Shell 中，连续两行是同一个进程执行环境，因此前一个命令修改的内存状态，会直接影响后一个命令；而在 Dockerfile 中，这两行 RUN 命令的执行环境根本不同，是两个完全不同的容器。这就是对 Dockerfile 构建分层存储的概念不了解所导致的错误。 之前说过每一个 RUN 都是启动一个容器、执行命令、然后提交存储层文件变更。第一层 RUN cd /app 的执行仅仅是当前进程的工作目录变更，一个内存上的变化而已，其结果不会造成任何文件变更。而到第二层的时候，启动的是一个全新的容器，跟第一层的容器更完全没关系，自然不可能继承前一层构建过程中的内存变化。 因此如果需要改变以后各层的工作目录的位置，那么应该使用 WORKDIR 指令。 正确示例 12WORKDIR /appRUN echo "hello" &gt; world.txt 10. USER 指定当前用户格式：USER &lt;用户名&gt;[:&lt;用户组&gt;] USER 指令和 WORKDIR 相似，都是改变环境状态并影响以后的层。WORKDIR 是改变工作目录，USER 则是改变之后层的执行 RUN, CMD 以及 ENTRYPOINT 这类命令的身份。 当然，和 WORKDIR 一样，USER 只是帮助你切换到指定用户而已，这个用户必须是事先建立好的，否则无法切换。 123RUN groupadd -r redis &amp;&amp; useradd -r -g redis redisUSER redisRUN [ &quot;redis-server&quot; ] 如果以 root 执行的脚本，在执行期间希望改变身份，比如希望以某个已经建立好的用户来运行某个服务进程，不要使用 su 或者 sudo，这些都需要比较麻烦的配置，而且在 TTY 缺失的环境下经常出错。建议使用 gosu。 12345678# 建立 redis 用户，并使用 gosu 换另一个用户执行命令RUN groupadd -r redis &amp;&amp; useradd -r -g redis redis# 下载 gosuRUN wget -O /usr/local/bin/gosu "https://github.com/tianon/gosu/releases/download/1.7/gosu-amd64" \ &amp;&amp; chmod +x /usr/local/bin/gosu \ &amp;&amp; gosu nobody true# 设置 CMD，并以另外的用户执行CMD [ "exec", "gosu", "redis", "redis-server" ] 所以，如果你不是要运行shell脚本，就一定不要以shell的形式写入CMD，比如 CMD npm start =&gt; sh -c “npm start” 11. ONBUILD 为他人做嫁衣裳格式：ONBUILD &lt;其它指令&gt; ONBUILD 是一个特殊的指令，它后面跟的是其它指令，比如 RUN, COPY 等，而这些指令，在当前镜像构建时并不会被执行。只有当以当前镜像为基础镜像，去构建下一级镜像的时候才会被执行。 Dockerfile 中的其它指令都是为了定制当前镜像而准备的，唯有 ONBUILD 是为了帮助别人定制自己而准备的。 假设我们要制作 Node.js 所写的应用的镜像。我们都知道 Node.js 使用 npm 进行包管理，所有依赖、配置、启动信息等会放到 package.json 文件里。在拿到程序代码后，需要先进行 npm install 才可以获得所有需要的依赖。然后就可以通过 npm start 来启动应用。因此，一般来说会这样写 Dockerfile: 1234567FROM node:slimRUN mkdir /appWORKDIR /appCOPY ./package.json /appRUN [ "npm", "install" ]COPY . /app/CMD [ "npm", "start" ] 把这个 Dockerfile 放到 Node.js 项目的根目录，构建好镜像后，就可以直接拿来启动容器运行。但是如果我们还有第二个 Node.js 项目也差不多呢？好吧，那就再把这个 Dockerfile 复制到第二个项目里。那如果有第三个项目呢？再复制么？文件的副本越多，版本控制就越困难，让我们继续看这样的场景维护的问题。 如果第一个 Node.js 项目在开发过程中，发现这个 Dockerfile 里存在问题，比如敲错字了、或者需要安装额外的包，然后开发人员修复了这个 Dockerfile，再次构建，问题解决。第一个项目没问题了，但是第二个项目呢？虽然最初 Dockerfile 是复制、粘贴自第一个项目的，但是并不会因为第一个项目修复了他们的 Dockerfile，而第二个项目的 Dockerfile 就会被自动修复。 那么我们可不可以做一个基础镜像，然后各个项目使用这个基础镜像呢？这样基础镜像更新，各个项目不用同步 Dockerfile 的变化，重新构建后就继承了基础镜像的更新？好吧，可以，让我们看看这样的结果。那么上面的这个 Dockerfile 就会变为： 1234FROM node:slimRUN mkdir /appWORKDIR /appCMD [ &quot;npm&quot;, &quot;start&quot; ] 这里我们把项目相关的构建指令拿出来，放到子项目里去。假设这个基础镜像的名字为 my-node 的话，各个项目内的自己的 Dockerfile 就变为： 1234FROM my-nodeCOPY ./package.json /appRUN [ &quot;npm&quot;, &quot;install&quot; ]COPY . /app/ 基础镜像变化后，各个项目都用这个 Dockerfile 重新构建镜像，会继承基础镜像的更新。 那么，问题解决了么？没有。准确说，只解决了一半。如果这个 Dockerfile 里面有些东西需要调整呢？比如 npm install 都需要加一些参数，那怎么办？这一行 RUN 是不可能放入基础镜像的，因为涉及到了当前项目的 ./package.json，难道又要一个个修改么？所以说，这样制作基础镜像，只解决了原来的 Dockerfile 的前4条指令的变化问题，而后面三条指令的变化则完全没办法处理。 ONBUILD 可以解决这个问题。让我们用 ONBUILD 重新写一下基础镜像的 Dockerfile: 标准示例 1234567FROM node:slimRUN mkdir /appWORKDIR /app # 改变工作目录为/appONBUILD COPY ./package.json /app # 将源镜像 /app/package.json 复制到新镜像/app 路径下ONBUILD RUN [ "npm", "install" ]ONBUILD COPY . /app/CMD [ "npm", "start" ] 这次我们回到原始的 Dockerfile，但是这次将项目相关的指令加上 ONBUILD，这样在构建基础镜像的时候，这三行并不会被执行。然后各个项目的 Dockerfile 就变成了简单地： 1FROM my-node 是的，只有这么一行。当在各个项目目录中，用这个只有一行的 Dockerfile 构建镜像时，之前基础镜像的那三行 ONBUILD 就会开始执行，成功的将当前项目的代码复制进镜像、并且针对本项目执行 npm install，生成应用镜像。]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何去掉Linux终端输出的颜色]]></title>
    <url>%2F2019%2F02%2F17%2Flinux%2F%E5%A6%82%E4%BD%95%E5%8E%BB%E6%8E%89Linux%E7%BB%88%E7%AB%AF%E8%BE%93%E5%87%BA%E7%9A%84%E9%A2%9C%E8%89%B2%2F</url>
    <content type="text"><![CDATA[1. 问题一般我们都会输出一些带颜色的日志或者标准输出,但现在我们想获取的这部分正好是有颜色的,就出现问题了. 例如: 1234#grep 2.6.9_5-9-0-0 kernel.list |awk &apos;&#123;print $2&#125;&apos; | xargs -i ssh &#123;&#125;Pseudo-terminal will not be allocated because stdin is not a terminal.ssh: \033[34mbj-xxx.db: Name or service not knownxargs: ssh: exited with status 255; aborting 这里面的bj-xxx.db是需要处理的host,但是因为 kernel.list里面是有颜色的,所以ssh的时候报错,提示”\033[34m”+”真实的host”出错. 该如何去掉这些颜色字符呢? 2.回答1grep --color=never 如果是源数据里包含颜色转义符，用sed可以去掉： 1sed -r "s/\x1B\[([0-9]&#123;1,2&#125;(;[0-9]&#123;1,2&#125;)?)?[m|K]//g"]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker持久化存储]]></title>
    <url>%2F2019%2F02%2F12%2Fdocker%2F5.docker%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[1. 管理Docker中的数据默认情况下，在容器内创建的所有文件都存储在可写容器层中。这意味着： 当该容器不再存在时，数据不会持久存在，并且如果另一个进程需要数据，则可能很难从容器中获取数据。 容器的可写层紧密耦合到运行容器的主机。您无法轻松地将数据移动到其他位置。 写入容器的可写层需要 存储驱动程序来管理文件系统。存储驱动程序使用Linux内核提供联合文件系统。与使用直接写入主机文件系统的数据卷相比，这种额外的抽象降低了性能 。 Docker有两个容器选项可以在主机中存储文件，因此即使在容器停止之后文件仍然存在：卷和 绑定挂载。如果你在Linux上运行Docker，你也可以使用tmpfs mount。 继续阅读有关这两种持久数据方式的更多信息。 2. 选择正确的Mount类型无论您选择使用哪种类型的安装，数据在容器内看起来都是相同的。它作为目录或容器文件系统中的单个文件公开。 可视化卷，绑定装载和tmpfs 装载之间差异的简单方法是考虑数据在Docker宿主主机上的位置。 1. Volume存储在由Docker（/var/lib/docker/volumes/）管理的主机文件系统的一部分中。非Docker进程不应修改文件系统的这一部分。卷是在Docker中保留数据的最佳方式。 对于容器内非空目录的处理：第一次创建卷时 ，会将容器内的非空目录内容复制到docker-volume中 2. Mount Points可以存储在主机系统的任何位置。它们甚至可能是重要的系统文件或目录。Docker主机或Docker容器上的非Docker进程可以随时修改它们。 对于容器内非空目录的处理：如果容器非空目录挂载到宿主主机，容器内该目录就会隐藏，如果时/usr/bin等目录，则该容器就无法启动，所以，通常不应该使用Mount Points挂载容器的非空目录到宿主主机 3. tmpfs挂载仅存储在主机系统的内存中，永远不会写入主机系统的文件系统。 3. Docker VolumeDocker Volume 是保存Docker容器生成和使用的数据的首选机制。虽然绑定挂载依赖于主机的目录结构，但卷完全由Docker管理。卷绑定安装有几个优点： 与绑定装载相比，卷更易于备份或迁移。 您可以使用Docker CLI命令或Docker API管理卷。 卷适用于Linux和Windows容器。 可以在多个容器之间更安全地共享卷。 卷驱动程序允许您在远程主机或云提供程序上存储卷，加密卷的内容或添加其他功能。 新卷的内容可以由容器预先填充。 此外，卷通常是比容器的可写层中的持久数据更好的选择，因为使用卷不会增加使用它的容器的大小，并且卷的内容存在于给定容器的生命周期之外。 1. 创建和管理volume1. 创建存储卷 1docker volume create $volume_name 2. 列出已有存储卷 12345$ docker volume lsDRIVER VOLUME NAMElocal 1aa54b00b05aaf521f93586afcae5c9f6a5e44eb6a84c0ab99222435f75b28f5local 9cf0504c3ac9b408837aa456e13c450b7b9a212f450910ccbee3cefde165027a 2. 删除存储卷1. 删除卷 1docker volume rm volume_name 2. 删除所有未使用的卷并释放空间： 1$ docker volume prune 3. 检查卷：1234567891011$ docker volume inspect my-vol[ &#123; &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: &#123;&#125;, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/my-vol/_data&quot;, &quot;Name&quot;: &quot;my-vol&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Scope&quot;: &quot;local&quot; &#125;] 4. 使用docker卷我们还可以使用-v或–volume标志将Docker卷附加到容器。但是，我们不是像使用绑定挂载那样将路径放在主机上的目录中，而是简单地放置卷名。 用法：-v 后面跟随 卷名 : docker容器路径 1-v volume_name:/docker_path 示例： 123docker run -d --name adonis \ -v volume_name:/docker_path \ ztest 2. 使用绑定挂载自Docker早期以来，Bind Mounts一直存在。与卷相比，绑定装载具有有限的功能。使用绑定装入时，主机上的文件或目录将装入容器中。文件或目录由其在主机上的完整路径或相对路径引用。相反，当您使用卷时，会在主机上的Docker存储目录中创建一个新目录，Docker会管理该目录的内容。 该文件或目录不需要已存在于Docker主机上。如果它尚不存在，则按需创建。绑定安装非常高效，但它们依赖于具有特定目录结构的主机文件系统。如果您正在开发新的Docker应用程序，请考虑使用命名卷。您无法使用Docker CLI命令直接管理Bind Mount。 1. 选择-v或–mount标志最初，-v或--volume标志用于独立容器，--mount标志用于群组服务。但是，从Docker 17.06开始，您还可以使用--mount独立容器。一般来说， --mount更明确和冗长。最大的区别在于-v 语法将所有选项组合在一个字段中，而--mount语法将它们分开。以下是每个标志的语法比较。 提示：新用户应使用–mount语法。有经验的用户可能更熟悉-v或–volume语法，但鼓励使用–mount，因为研究表明它更容易使用。 1. –volume-v或--volume：由三个字段组成，用冒号字符（:）分隔。字段必须按正确的顺序排列，并且每个字段的含义不是很明显。 对于Bind Mounts，第一个字段是主机上文件或目录的路径。第二个字段是文件或目录在容器中安装的路径。第三个字段是可选的，并且是用逗号分隔的选项，诸如列表ro，consistent，delegated，cached，z，和Z。这些选项将在下面讨论。 2. –mount--mount：由多个键值对组成，以逗号分隔，每个键&lt;key&gt;=&lt;value&gt;组由一个元组组成。该--mount语法比更详细的-v或--volume，但按键的顺序并不显著，并且标志的价值更容易理解。 mount的type，其可以是bind，volume，或tmpfs。本主题讨论bind mounts，因此类型始终是bind。mount的source。对于bind mounts,这是Docker宿主主机上的文件或目录的路径。可以指定为source或 src。destination文件或目录安装在容器中的路径作为其值。可以指定为destination，dst或target。readonly选项（如果存在）导致bind mounts以只读方式装入容器中。bind-propagation 选项（如果存在）会更改 绑定传播。可以是一个 rprivate ，private，rshared，shared，rslave，slave。consistency选项，如果存在，可以是一种consistent，delegated或cached。此设置仅适用于Docker Desktop for Mac，在所有其他平台上均被忽略。 该–mount标志不支持z或Z修改selinux标签的选项。下面的示例显示了可能的语法–mount和-v语法，并 –mount首先给出。 3. -v和–mount行为之间的差异因为-v和--volume flags一直是Docker的一部分，所以它们的行为无法改变。这意味着在-v和--mount之间存在一种不同的行为。 如果使用-v或--volume绑定装载Docker主机上尚不存在的文件或目录，请-v为您创建端点。它始终作为目录创建。 如果您使用--mount绑定Docker宿主主机上尚不存在的文件或目录，Docker也不会自动为您创建它，但会产生一个错误。 2. 使用绑定装载启动容器考虑一个你有一个目录的情况source，当你构建源代码时，工件被保存到另一个目录中source/target/。您希望工件可用于容器/app/，并且您希望每次在开发主机上构建源时容器都可以访问新构建。使用以下命令将target/ 目录绑定到容器中/app/。从source目录中运行该命令 。该$(pwd)子命令将扩展到Linux或者MacOS主机的当前工作目录。 所述--mount和-v以下实施例产生相同的结果。除非devtest在运行第一个容器后删除容器，否则不能同时运行它们。 --mount -v 12345$ docker run -d \ -it \ --name devtest \ --mount type=bind,source=&quot;$(pwd)&quot;/target,target=/app \ nginx:latest 使用docker inspect devtest验证绑定安装正确创建。寻找Mounts部分： 12345678910&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/tmp/source/target&quot;, &quot;Destination&quot;: &quot;/app&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;], 这表明mount是一个bindmount，它显示了正确的源和目标，它表明mount是读写的，并且传播设置为rprivate。 停止容器： 123$ docker container stop devtest$ docker container rm devtest 3. 配置绑定传播对于bind mounts和volumes，绑定传播默认为rprivate。它仅可用于绑定装入，并且仅适用于Linux主机。绑定传播是一个高级主题，许多用户永远不需要配置它。绑定传播是指在给定的绑定装载或命名卷（比如宿主主机）中创建的装载是否可以传播到该装载的副本（比如Docker）。 传播设置 描述 shared 原始挂载的子挂载将暴露给副本挂载，副挂载的子挂载也会传播到原始挂载。 slave 类似于共享挂载，但只在一个方向上。如果原始挂载程序公开子挂载，则副本挂载程序可以看到它。而副本挂载的子挂载不会传播到原始挂载 private 挂载是私人的。其中的子挂载不会暴露给副本挂载，副挂载的子挂载也不会暴露给原始挂载。 rshared 与共享相同，但传播也扩展到嵌套在任何原始或副本挂载点中的挂载点。 rslave 与从属相同，但传播也延伸到嵌套在任何原始或副本挂载点内的挂载点。 rprivate 默认。与private相同，意味着原始或副本装入点中任何位置的挂载点都不会沿任一方向传播。 示例： 1$ docker run -v /foo:/bar:slave 4. 挂载到容器上的非空目录中如果将bind-mount绑定到容器上的非空目录中，则绑定装置会隐藏目录的现有内容。这可能是有益的，例如当您想要在不构建新图像的情况下测试新版本的应用程序时。但是，它也可能令人惊讶，并且此行为与docker卷的行为不同。 此示例设计为极端，将容器/usr/目录的内容替换/tmp/为主机上的目录, 在大多数情况下，这会导致容器无法运行。 12345678$ docker run -d \ -it \ --name broken-container \ --mount type=bind,source=/tmp,target=/usr \ nginx:latestdocker: Error response from daemon: oci runtime error: container_linux.go:262:starting container process caused "exec: \"nginx\": executable file not found in $PATH". 容器已创建但无法启动。去掉它： 1$ docker container rm broken-container 5. NFSdocker volume create \ --driver local \ --opt type=nfs \ --opt o=addr=192.168.71.104,rw \ --opt device=:/tytest/workspace \ tytest docker run -it -v tytest:/workspace --privileged 192.168.71.104:5000/tytest]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker仓库的使用]]></title>
    <url>%2F2019%2F02%2F08%2Fdocker%2F4.docker%E4%BB%93%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[仓库（ Repository ）是集中存放镜像的地方。一个容易混淆的概念是注册服务器（ Registry ）。实际上注册服务器是管理仓库的具体服务器，每个服务器上可以有多个仓库，而每个仓库下面有多个镜像。从这方面来说，仓库可以被认为是一个具体的项目或目录。例如对于仓库地址 dl.dockerpool.com/ubuntu 来说，dl.dockerpool.com 是注册服务器地址， ubuntu 是仓库名。 1. Docker RegistryDocker Registry用来提供集中的存储、分发镜像的服务。一个 Docker Registry 中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。 通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本。我们可以通过 &lt;仓库名&gt;:&lt;标签&gt; 的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签。 以 Ubuntu 镜像 为例，ubuntu 是仓库的名字，其内包含有不同的版本标签，如，14.04, 16.04。我们可以通过 ubuntu:14.04，或者 ubuntu:16.04 来具体指定所需哪个版本的镜像。如果忽略了标签，比如 ubuntu，那将视为 ubuntu:latest。 仓库名经常以 两段式路径 形式出现，比如 jwilder/nginx-proxy，前者往往意味着 Docker Registry 多用户环境下的用户名，后者则往往是对应的软件名。但这并非绝对，取决于所使用的具体 Docker Registry 的软件或服务。 2. 公共库 Docker Hub最常使用的 Registry 公开服务是官方的 Docker Hub，这也是默认的 Registry，并拥有大量的高质量的官方镜像。 可以在 https://cloud.docker.com 免费注册一个 Docker 账号。然后将你构建好的镜像教育docker hub管理。具体使用流程可以参考官方说明，这里不再赘述。 3. 私有 Docker Registry在部署Registry之前，需要在主机上安装Docker。Registry是Registry 镜像的实例，并在Docker中运行。本主题提供有关部署和配置Registry的基本信息。有关配置选项的详尽列表，请参阅配置参考 1. 将映像从Docker Hub复制到您的Registry 安装运行 docker-registry 1$ docker run -d -p 5000:5000 -e --restart=always --name registry registry 从Docker Hub 拉取 ubuntu:16.04 镜像 1$ docker pull ubuntu:16.04 将镜像标记为localhost:5000/my-ubuntu。这会为现有图像创建一个附加标记。当标记的第一部分是主机名和端口时，Docker在推送时将其解释为Registry的位置。 1$ docker tag ubuntu:16.04 localhost:5000/my-ubuntu 将镜像推送到本地Registry localhost:5000: 1$ docker push localhost:5000/my-ubuntu 删除本地缓存的ubuntu:16.04和localhost:5000/my-ubuntu映像，以便您可以测试从注册表中提取映像。这不会从您的 Registry 中删除localhost:5000/my-ubuntu映像。 12$ docker image remove ubuntu:16.04$ docker image remove localhost:5000/my-ubuntu 从本地 registry 拉取localhost:5000/my-ubuntu镜像 1$ docker pull localhost:5000/my-ubuntu 2. 局域网内其它主机访问registry假设同一局域网内有两台主机71.199和71.104，其中在71.104部署了docker registry服务。如果希望使用71.199主机拉取71.104的仓库镜像，需要首先执行以下操作： 创建或修改 /etc/docker/daemon.json文件 12# 如果daemon没有其它配置，否则请手动添加echo '&#123; "insecure-registries":["192.168.71.104:5000"] &#125;' &gt; /etc/docker/daemon.json 重启docker服务 1sudo service docker restart 对于windows设置如下： 之后就可以正常使用71.104的仓库服务了： 1$ docker pull 192.168.71.104:5000/my-ubuntu 3. 停止本地 registry停止本地仓库，与停止其它容器相同，使用 docker container stop 命令 1$ docker container stop registry 删除仓库容器，使用 docker container rm命令 1$ docker container stop registry &amp;&amp; docker container rm -v registry]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker容器的操作]]></title>
    <url>%2F2019%2F02%2F03%2Fdocker%2F3.docker%E5%AE%B9%E5%99%A8%E7%9A%84%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1. 创建一个新容器当利用 docker run 来创建容器时，Docker 在后台运行的标准操作包括： 检查本地是否存在指定的镜像，不存在就从公有仓库下载 利用镜像创建并启动一个容器 分配一个文件系统，并在只读的镜像层外面挂载一层可读写层 从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去 从地址池配置一个 ip 地址给容器 执行用户指定的应用程序 执行完毕后容器被终止 1. 新建并启动所需要的命令主要为 docker run 例如，下面的命令输出一个 “Hello World”，之后终止容器。 12$ docker run ubuntu:14.04 /bin/echo 'Hello world'Hello world 这跟在本地直接执行 /bin/echo &#39;hello world&#39;几乎感觉不出任何区别。 2. 进入交互模式下面的命令则启动一个 bash 终端，允许用户进行交互。 12$ docker run -t -i ubuntu:14.04 /bin/bashroot@af8bae53bdd3:/# 其中，-t 选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上， -i则让容器的标准输入保持打开。 在交互模式下，用户可以通过所创建的终端来输入命令，例如 12root@af8bae53bdd3:/# lsbin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var 3. docker run 的一些参数1docker run --name some-nginx -p 80:80 -v /mnt/workspace:/workspace -d nginx --name :给容器指定一个名字 -d : 使容器在后台执行 -v : 绑定挂载点，将宿主机目录挂载到container中的目录上，这样在容器内的环境就可以访问宿主主机内的内容.前面是主机目录，后面是容器目录 -p: 端口映射，将宿主主机端口映射到container中的端口 -P: 将容器暴露所有端口进行映射，默认为false 2. 启动已终止容器当一个容器终止后，我们可以利用 docker container start 命令，直接将一个已经终止的容器启动运行。 首先使用docker ps -a查看所有的容器(不含参数-a只包含正在运行的容器) 使用docker container start $conname重新启动容器 使用docker container start -i $conname 启动容器并进入交互模式 【注】不应该使用docker run，因为这样docker会创建一个新的容器而不是重启此容器 3. 后台运行更多的时候，需要让 Docker 在后台运行而不是直接把执行命令的结果输出在当前宿主机下。此时，可以通过添加-d参数来实现。 下面举两个例子来说明一下。 如果不使用 -d参数运行容器 12345$ docker run ubuntu:18.04 /bin/sh -c "while true; do echo hello world; sleep 1; done"hello worldhello worldhello worldhello world 容器会把输出的结果 (STDOUT) 打印到宿主机上面 如果使用了-d参数运行容器 12$ docker run -d ubuntu:18.04 /bin/sh -c "while true; do echo hello world; sleep 1; done"77b2dc01fe0f3f1265df143181e7b9af5e05279a884f4776ee75350ea9d8017a 此时容器会在后台运行并不会把输出的结果 (STDOUT) 打印到宿主机上面(输出结果可以用 docker logs 查看)。 注： 容器是否会长久运行，是和 docker run 指定的命令有关，和 -d 参数无关。 使用 -d 参数启动后会返回一个唯一的 id，也可以通过 docker container ls 命令来查看容器信息。 123$ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES77b2dc01fe0f ubuntu:18.04 /bin/sh -c 'while tr‘ 2 minutes ago Up 1 minute agitated_wright 要获取容器的输出信息，可以通过 docker container logs 命令。 1234$ docker container logs [container ID or NAMES]hello worldhello worldhello world 4. 终止容器可以使用 docker container stop 来终止一个运行中的容器。 此外，当 Docker 容器中指定的应用终结时，容器也自动终止。 例如对于上一章节中只启动了一个终端的容器，用户通过 exit 命令或 Ctrl+d 来退出终端时，所创建的容器立刻终止。 终止状态的容器可以用 docker container ls -a命令看到。例如 1234$ docker container ls -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba267838cc1b ubuntu:14.04 "/bin/bash" 30 minutes ago Exited (0) About a minute ago trusting_newton98e5efa7d997 training/webapp:latest "python app.py" About an hour ago Exited (0) 34 minutes ago backstabbing_pike 处于终止状态的容器，可以通过 docker container start 命令来重新启动。 此外，docker container restart 命令会将一个运行态的容器终止，然后再重新启动它。 终止所有的容器： 1$ docker stop $(docker ps -aq) 5. 进入后台运行中的容器在使用 -d 参数时，容器启动后会进入后台。 某些时候需要进入容器进行操作，包括使用 docker attach 命令或 docker exec 命令，推荐大家使用 docker exec 命令，原因会在下面说明。 1. exec 命令[推荐]docker exec 后边可以跟多个参数，这里主要说明 -i -t 参数。 只用-i参数时，由于没有分配伪终端，界面没有我们熟悉的 Linux 命令提示符，但命令执行结果仍然可以返回。 当-i -t参数一起使用时，则可以看到我们熟悉的 Linux 命令提示符。 123456789101112131415$ docker run -dit ubuntu69d137adef7a8a689cbcb059e94da5489d3cddd240ff675c640c8d96e84fe1f6$ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES69d137adef7a ubuntu:latest "/bin/bash" 18 seconds ago Up 17 seconds zealous_swirles$ docker exec -i 69d1 bashlsbinbootdev$ docker exec -it 69d1 bashroot@69d137adef7a:/# 如果从这个 stdin 中 exit，不会导致容器的停止。这就是为什么推荐大家使用 docker exec 的原因。 更多参数说明请使用 docker exec –help 查看。 2. attach 命令docker attach 是 Docker 自带的命令。下面示例如何使用该命令。 123456789$ docker run -dit ubuntu243c32535da7d142fb0e6df616a3c3ada0b8ab417937c853a9e1c251f499f550$ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES243c32535da7 ubuntu:latest "/bin/bash" 18 seconds ago Up 17 seconds nostalgic_hypatia$ docker attach 243croot@243c32535da7:/# 注意： 如果从这个 stdin 中 exit，会导致容器的停止。 6. 导出和导入容器1. 导出容器如果要导出本地某个容器，可以使用 docker export 命令 1234$ docker container ls -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES7691a814370e ubuntu:14.04 &quot;/bin/bash&quot; 36 hours ago Exited (0) 21 hours ago test$ docker export 7691a814370e &gt; ubuntu.tar 这样将导出容器快照到本地文件。 2. 导入容器可以使用 docker import 从容器快照文件中再导入为镜像 1234$ cat ubuntu.tar | docker import - test/ubuntu:v1.0$ docker image lsREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEtest/ubuntu v1.0 9d37a6082e97 About a minute ago 171.3 MB 此外，也可以通过指定 URL 或者某个目录来导入，例如 1$ docker import http://example.com/exampleimage.tgz example/imagerepo 注：用户既可以使用 docker load 来导入镜像存储文件到本地镜像库，也可以使用 docker import 来导入一个容器快照到本地镜像库。这两者的区别在于容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文件导入时可以重新指定标签等元数据信息。 7. 删除容器1. 删除一个处于终止状态的容器可以使用 docker container rm 来删除一个处于终止状态的容器。 12$ docker container rm trusting_newtontrusting_newton 如果要删除一个运行中的容器，可以添加-f参数。Docker 会发送 SIGKILL 信号给容器。 2. 清理所有处于终止状态的容器用 docker container ls -a 命令可以查看所有已经创建的包括终止状态的容器，如果数量太多要一个个删除可能会很麻烦，用下面的命令可以清理掉所有处于终止状态的容器。 1$ docker container prune]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker镜像的操作]]></title>
    <url>%2F2019%2F02%2F01%2Fdocker%2F2.docker%E9%95%9C%E5%83%8F%E7%9A%84%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Docker 把应用程序及其依赖，打包在 image 文件里面。只有通过这个文件，才能生成 Docker 容器。image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。 1. 查看已安装的images查看一下我们拥有了哪些images，可用下面的命令 1$ docker images image 文件是通用的，一台机器的 image 文件拷贝到另一台机器，照样可以使用。 Docker 允许你在容器内运行应用程序， 使用 docker run 命令来在容器内运行一个应用程序。输出Hello world 2. 拉取镜像到本地首先，运行下面的命令，将 image 文件从仓库抓取到本地。 1$ docker image pull library/hello-world 上面代码中，docker image pull是抓取 image 文件的命令。library/hello-world是 image 文件在仓库里面的位置，其中library是 image 文件所在的组，hello-world是 image 文件的名字。 由于 Docker 官方提供的 image 文件，都放在library组里面，所以它的是默认组，可以省略。因此，上面的命令可以写成下面这样。 1$ docker image pull hello-world 抓取成功以后，就可以在本机看到这个 image 文件了。 1$ docker image ls 现在，运行这个 image 文件。 1$ docker container run hello-world 注意，docker container run命令具有自动抓取 image 文件的功能。如果发现本地没有指定的 image 文件，就会从仓库自动抓取。因此，前面的docker image pull命令并不是必需的步骤。 3. 停止、删除所有的docker容器和镜像1. 清理所有停止，无用的containers,images和network 1$ docker system prune -f 2. 删除所有不使用的镜像 1$ docker image prune -f -a 3. 删除所有停止的容器 1$ docker container prune -f 删除所有镜像 1$ docker rmi $(docker images -q) 4. 镜像的导出与导入1. 导出镜像为tar文件 1$ docker save -o node-chrome.tar selenium/node-chrome:latest 2. 导入tar文件为镜像 1$ docker load &lt; save.tar]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker的介绍与安装]]></title>
    <url>%2F2019%2F01%2F28%2Fdocker%2F1.docker%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1. Docker简介Docker 源代码托管在 GitHub 上，Docker 公司开源的一个基于 LXC 技术之上构建的 Container 容器引擎，基于 Go 语言并遵从 Apache2.0 协议开源。 Docker 通过 namespaces 、cgroups 等实现内核虚拟化技术，并提供容器的资源隔离与安全保障等。由于Docker通过操作系统层的虚拟化实现隔离，所以Docker的容器在运行的时候，并不需要类似虚拟机（VM）额外的操作系统开销，提高资源利用率。 三大理念：Build（构建）、Ship（传输）、Run（运行） Docker组成：Docker Client、Docker Server 2. Docker与虚拟机的区别 Docker 与虚拟机的区别可以用如下的图来解释，从图中我们不难看出 Docker 与虚拟机的区别主要体现在以下几个方面。 Docker 通过 Docker Engine 运行在操作系统 OS 上 虚拟机运行在硬件资源上 Docker 无法做到完全隔离，比如用户空间 3. Docker版本概述Docker有两个版本： 社区版（CE） 企业版（EE） Docker Community Edition（CE）非常适合希望开始使用Docker并尝试使用基于容器的应用程序的个人开发人员和小型团队。 Docker企业版（EE）专为企业开发和IT团队而设计，他们在生产中大规模构建，发布和运行业务关键型应用程序。 2. docker容器的相关技术简介docker依赖的linux内核特性 Namespace命名空间 Controlgroups控制组 Namespace命名空间 编程语言:封装 =&gt; 代码隔离 操作系统:系统资源的隔离 =&gt; 进程，网络，文件系统 docker使用的Namespace命名空间 PID 进程隔离 NET 管理网络接口 IPC 管理跨进程通信访问 MNT 管理挂载点.文件管理 UTS 隔离内核和版本标识 Controlgroups控制组 Controlgroups控制组用来分配进程资源，来源与google,后被整合入linux内核，是容器技术的基础。 docker容器的能力 文件系统隔离:每个容器都有自己的root系统 进程隔离:每个容器都运行在自己的进程环境中 网络隔离:容器间的虚拟网络接口和IP地址都是分开的 资源隔离和分组:使用cgroup将cup和内存之类的资源独立分配给每个Docker容器 3. docker的安装与启动任意linux系统都可以通过此脚本快速的安装docker【推荐】 1curl -s https://get.docker.com | sh 更详细的安装方式可以查看官方文档 安装完成后，在terminal中就有了docker的命令了，这个命令就是Docker Client。所有的操作都是通过docker命令完成的。运行下面的命令，验证是否安装成功。 123$ docker version# 或者$ docker info Docker 需要用户具有 sudo 权限，为了避免每次命令都输入sudo，可以把用户加入Docker 用户组，可用下面的命令 1$ sudo usermod -aG docker $USER 启动docker服务 Docker 是服务器—-客户端架构。命令行运行docker命令的时候，需要本机有 Docker 服务。如果这项服务没有启动，可以用下面的命令启动 12345# service 命令的用法$ sudo service docker start# systemctl 命令的用法$ sudo systemctl start docker 也可以选择离线安装： 1234567export docker_path="docker-18.09.6.tgz"scp root@192.168.71.104:/root/soft/$&#123;docker_path&#125; $docker_pathtar xzvf $&#123;docker_path&#125;cp docker/* /usr/bin/dockerd &gt; /dev/null 2&gt;&amp;1 &amp;echo "dockerd &gt; /dev/null 2&gt;&amp;1 &amp;" &gt; /etc/rc.d/rc.local1chmod +x /etc/rc.d/rc.local1 3. 为docker设置代理由于国内网络环境，在没有代理的情况下，很多时候，拉取镜像是会失败的。而且命令行界面设置的代理对于docker来说是无效的，如果主机没有联网。就会出现以下错误： 12docker: Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on [::1]:53: read udp [::1]:33792-&gt;[::1]:53: read: connection refused.See 'docker run --help'. 因此，我们需要从以下几个方面去考虑代理设置： 1. 为从官方docker仓库拉取镜像设置代理 默认情况下这个配置文件夹并不存在，我们要创建它。 1$ mkdir -p /etc/systemd/system/docker.service.d 创建一个文件 /etc/systemd/system/docker.service.d/http-proxy.conf包含 HTTP_PROXY 环境变量: 12[Service]Environment="HTTP_PROXY=http://192.168.71.60:1081/" 如果有局域网或者国内的registry，我们还需要使用 NO_PROXY 变量声明一下，比如你可以能国内的daocloud.io放有镜像: 12[Service]Environment="HTTP_PROXY=http://proxy.example.com:80/" "NO_PROXY=localhost,127.0.0.1,daocloud.io" 刷新systemd配置: 1$ sudo systemctl daemon-reload 用系统命令验证环境变量加上去没: 12$ systemctl show --property=Environment dockerEnvironment=HTTP_PROXY=http://proxy.example.com:80/ 万事俱备，重启docker: 1$ sudo systemctl restart docker 2. 为使用Dockerfile构建镜像时设置代理通过上一步的设置，docker 已经可以正常联网，然而使用 Dokcerfile 构建镜像时，代理是无效的，需要在 Dokcerfile 中重新设置 如果出于特定的，临时的原因 - 比如你在公司代理服务器后面构建容器,并且随后不再需要它，我建议如下设置： 12345RUN export \ http_proxy="http://some.custom.proxy:8080/” \ https_proxy="https://some.custom.proxy:8080/" \ &amp;&amp; &lt; E.G. pip install requirements.txt&gt; \ &amp;&amp; unset http_proxy https_proxy 您也可以通过调用ENV在Dockerfile中使用更永久的解决方案，但请注意，如果您将图像推送/部署到其他位置，这些解决方案会持续存在并可能导致问题进一步发展]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell函数]]></title>
    <url>%2F2019%2F01%2F19%2Flinux%2F2.shell%E5%9F%BA%E6%9C%AC%E8%AA%9E%E6%B3%95%2F7.%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[下面的例子定义了一个函数并进行调用： 12345678#!/bin/bashdemoFun()&#123; echo "这是我的第一个 shell 函数!"&#125;echo "-----函数开始执行-----"demoFunecho "-----函数执行完毕-----" 注意： 1.可以带function fun() 定义，也可以直接 fun() 定义,不带任何参数。2.参数返回，可以显示加：return 返回，如果不加，将以最后一条命令运行结果，作为返回值。return 后跟数值 n(0-255 输出结果：123-----函数开始执行-----这是我的第一个 shell 函数!-----函数执行完毕----- 函数参数在Shell中，调用函数时可以向其传递参数。在函数体内部，通过 $n 的形式来获取参数的值，例如，$1表示第一个参数，$2表示第二个参数… 带参数的函数示例： 123456789101112#!/bin/bashfunWithParam()&#123; echo "第一个参数为 $1 !" echo "第二个参数为 $2 !" echo "第十个参数为 $10 !" echo "第十个参数为 $&#123;10&#125; !" echo "第十一个参数为 $&#123;11&#125; !" echo "参数总数有 $# 个!" echo "作为一个字符串输出所有参数 $* !"&#125;funWithParam 1 2 3 4 5 6 7 8 9 34 73]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用minicom管理串口]]></title>
    <url>%2F2019%2F01%2F17%2Flinux%2F%E4%BD%BF%E7%94%A8minicom%E8%AF%BB%E5%86%99%E4%B8%B2%E5%8F%A3%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[1. minicom简介1. 安装minicom是linux下一款常用的串口调试工具。ubuntu环境下，使用如下命令安装 1sudo apt-get install minicom 2. 配置使用前需要进行配置，执行 1sudo minicom -s 可打开minicom并进入配置模式，使用方向键，选择需要配置的项目，如 Serial port setup ，回车进入配置，可以看到多个配置项，此时光标在最下方。 需要修改某个配置，则输入对应的字母，光标即会跳转到对应的项，编辑后，回车确认，光标再次回到最下方。 一般而言，需要修改 123A - Serial DeviceE - Bps/Par/BitsF - Hardware Flow Control A配置项，指定USB设备。一般USB转串口会生成设备/dev/ttyUSBx，x是数字序号。可以执行以下命令确认下 1ls -l /dev/ttyUSB* E配置项，根据实际情况，指定波特率等参数 F配置项，硬件流控，要看你的设备是否有。如果没有，或者你不确定的话，可以先关掉，将默认的Yes切换为No. 修改好之后，回车退到上一个界面，此时记得往下，选择 Save setup as dfl 将刚刚的修改保存为默认配置，避免下次使用还需要再次配置。 最后，选择 Exit 会退出配置界面，并打开minicom。选择 Exit from Minicom 则会直接退出minicom。 3. 退出minicom使用前缀按键 Ctrl-A，即执行特殊操作时，都需要先按 Ctrl+A，再按某个按键使用对应的功能。 Ctrl+A，再按 Z， 可查看帮助，从帮助可以看到，退出时，要先按Ctrl+A，再按X 2. 配置权限minicom 本身不需要sudo权限，但因为要打开串口设备/dev/xxx ，所以一般会需要使用sudo来启动minicom。 这里我们可以修改下串口设备的权限，这样以后就不用使用sudo了。 1. 使用命令更改简单粗暴地使用chmod命令修改 1sudo chmod 666 /dev/ttyUSB0 2. 配置udev规则(推荐)修改配置文件 1sudo vim /etc/udev/rules.d/70-ttyusb.rules 增加一行 1KERNEL==&quot;ttyUSB[0-9]*&quot;, MODE=&quot;0666&quot; 修改后，需要重新插拔设备，以重新生成设备节点。 3. 自动设置设备名如果日常只用一个设备，设备名固定是/dev/ttyUSB0，那每次直接打开minicom即可。 但当你可能需要使用多个串口时，问题就来了，每次需要先查看下设备名 1ls /dev/ttyUSB* 再配置下minicom，手工改成这个设备，才能使用。一点都不方便。因此推荐使用命令行的方式实现设置 1. 使用参数指定设备研究下mincom的参数后，发现有更简单的实现方式，使用minicom的-D参数。 同样编写脚本~/.myminicom.sh 1234567891011121314com() &#123; ports_USB=$(ls /dev/ttyUSB*) ports_ACM=$(ls /dev/ttyACM*) #arduino ports=&quot;$ports_USB $ports_ACM&quot; select port in $ports;do if [ &quot;$port&quot; ]; then echo &quot;You select the choice &apos;$port&apos;&quot; minicom -D &quot;$port&quot; $@&quot; break else echo &quot;Invaild selection&quot; fi done&#125; 在~/.bashrc中引入此函数 12echo &apos;source ~/.myminicom.sh&apos; &gt;&gt; ~/.bashrcsource ~/.bashrc 添加完毕后，可使用 com 命令调用。 2. 使用效果1234zhuangqiubin@zhuangqiubin-PC:~$ com1) /dev/ttyUSB02) /dev/ttyUSB1#? 此时输入数字，选择要的打开的串口设备，回车即可。 4. 自动保存log让 minicom 自动保存log，可以方便调试。 查看参数，minicom可以使用 -C 参数指定保存log文件。于是完善脚本，自动把log以日期命名，保存到/tmp目录下。 注意，tmp目录关机即清空，如果想持久保存log，需要修改到其他目录。 修改后脚本如下 12345678910111213141516com() &#123; ports_USB=$(ls /dev/ttyUSB*) ports_ACM=$(ls /dev/ttyACM*) #arduino ports=&quot;$ports_USB $ports_ACM&quot; datename=$(date +%Y%m%d-%H%M%S) select port in $ports;do if [ &quot;$port&quot; ]; then echo &quot;You select the choice &apos;$port&apos;&quot; minicom -D &quot;$port&quot; -C /tmp/&quot;$datename&quot;.log &quot;$@&quot; break else echo &quot;Invaild selection&quot; fi done&#125;com 5. 暂停输出Ctrl+A 是mimicom的特殊功能前缀按键，但还有另一个很实用的作用，就是暂停屏幕输出。 在设备开始大量输出log时，基本看不清屏幕内容。此时可以按 Ctrl+A，暂停输出，方便查看所需log。 6. 打开minicom时间戳在minicom中，按下 Ctrl+A，再按 N，即可激活时间戳，在每行log前添加当前系统的时间戳。 用于观察启动时间之类的，还是比较方便。 7. 发送接收文件设备端支持的话，按下 Ctrl+A，再按 S，即可向设备端发送文件。 按 Ctrl+A，再按 R，可接收文件。 8. 自动换行当你的log中可能存在，单行长度超过屏幕宽度的log时（比如启动时打印的kernel cmdline），可以使用mimicom的自动换行功能。 在启动minicom时加上 -w 选项，或者在minicom中，按 Ctrl+A 再按 W。 9. 更多功能可以使用 minicom -h 查看，也可在mincon中，按 Ctrl+A 再按 Z 查看。 有什么其他使用功能或技巧，也欢迎留言告诉我。 源码文中的代码非最新版本，请访问 https://github.com/zqb-all/EasierMinicom 获取带有更多功能的最新版本。 如果觉得本文对你有帮助的的话，顺手点下推荐哦～～]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell 并行执行与并发数控制]]></title>
    <url>%2F2019%2F01%2F16%2Flinux%2F2.shell%E5%9F%BA%E6%9C%AC%E8%AA%9E%E6%B3%95%2F10.shell%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C%E4%B8%8E%E5%B9%B6%E5%8F%91%E6%95%B0%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[1. 串行执行正常情况下，Shell脚本是串行执行的，一条命令执行完才会执行接下来的命令。如下代码： 12345678# !/bin/bashfor i in `seq 1 10` do echo $idoneecho "----end----" 脚本执行的结果如下： 123456789101112345678910----end---- 2. 并发执行echo $1 命令串行执行，如果命令耗时较长导致总时间较长。如果命令之间没有互相依赖关系时，可以让命令并行执行，并行执行的方法就是在命令后加上 &amp; 符号。 12345678# !/bin/bashfor i in `seq 1 10` do echo $i &amp;doneecho "----end----" 脚本执行的结果如下： 123456789101112354----end----107896 可以看到，这样不能保证命令的执行顺序，有的时候需要保证for循环所有命令执行完后再向后执行接下来的命令。可以使用 wait 实现 123456789# !/bin/bashfor i in `seq 1 10` do echo $i &amp;donewaitecho "----end----" 脚本执行的结果如下： 123456789101112369104578----end---- 3. 并发数量控制问题还没有结束，当需要并行执行的命令数量特别多的时候，特别是执行命令的资源占用较多时，直接用 &amp; 实现并行容易将服务器资源占用打满，影响其他程序运行。 使用管道和令牌原理实现并发控制。 12345678910111213141516171819202122232425262728293031#!/bin/bash# Step1 创建有名管道[ -e ./fd1 ] || mkfifo ./fd1# 创建文件描述符，以可读（&lt;）可写（&gt;）的方式关联管道文件，这时候文件描述符3就有了有名管道文件的所有特性exec 3&lt;&gt; ./fd1 # 关联后的文件描述符拥有管道文件的所有特性,所以这时候管道文件可以删除，我们留下文件描述符来用就可以了rm -rf ./fd1 # Step2 创建令牌 for i in `seq 1 2`;do # echo 每次输出一个换行符,也就是一个令牌 echo &gt;&amp;3 done# Step3 拿出令牌，进行并发操作for line in `seq 1 10`;do read -u3 # read 命令每次读取一行，也就是拿到一个令牌 &#123; echo $line echo &gt;&amp;3 # 执行完一条命令会将令牌放回管道 &#125;&amp;donewaitexec 3&lt;&amp;- # 关闭文件描述符的读exec 3&gt;&amp;- # 关闭文件描述符的写]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell输入输出重定向]]></title>
    <url>%2F2019%2F01%2F16%2Flinux%2F2.shell%E5%9F%BA%E6%9C%AC%E8%AA%9E%E6%B3%95%2F8.shell%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E9%87%8D%E5%AE%9A%E5%90%91%2F</url>
    <content type="text"><![CDATA[输出重定向1command1 &gt; file1 上面这个命令执行command1然后将输出的内容存入file1。 注意: 任何file1内的已经存在的内容将被新内容替代。如果要将新内容添加在文件末尾，请使用 &gt;&gt; 操作符。 1who &gt; users 执行后，并没有在终端输出信息，这是因为输出已被从默认的标准输出设备（终端）重定向到指定的文件。 你可以使用 cat 命令查看文件内容： 1234$ cat users_mbsetupuser console Oct 31 17:35 tianqixin console Oct 31 17:35 tianqixin ttys000 Dec 1 11:33 输入重定向和输出重定向一样，Unix 命令也可以从文件获取输入，语法为： 1command1 &lt; file1 这样，本来需要从键盘获取输入的命令会转移到文件读取内容。 123$ wc -l &lt; users # 从users读取内容作为标准输入传给wc2 注意：上面两个例子的结果不同：第一个例子，会输出文件名；第二个不会，因为它仅仅知道从标准输入读取内容。 1command1 &lt; infile &gt; outfile 同时替换输入和输出，执行command1，从文件infile读取内容，然后将输出写入到outfile中。 重定向深入讲解一般情况下，每个 Unix/Linux 命令运行时都会打开三个文件： 标准输入文件(stdin)：stdin的文件描述符为0，Unix程序默认从stdin读取数据。 标准输出文件(stdout)：stdout 的文件描述符为1，Unix程序默认向stdout输出数据。 标准错误文件(stderr)：stderr的文件描述符为2，Unix程序会向stderr流中写入错误信息。 默认情况下，command &gt; file 将 stdout 重定向到 file，command &lt; file 将stdin 重定向到 file。 如果希望 stderr 重定向到 file，可以这样写： 12$ command 2 &gt; file # 重写file内容$ command 2 &gt;&gt; file # 向file追加写内容 如果希望将 stdout 和 stderr 合并后重定向到 file，可以这样写： 12$ command &gt; file 2&gt;&amp;1$ command &gt;&gt; file 2&gt;&amp;1 Here DocumentHere Document 是 Shell 中的一种特殊的重定向方式，用来将输入重定向到一个交互式 Shell 脚本或程序。 示例： 123456# wc -l &lt;&lt; EOF&gt; 1&gt; 2&gt; 3&gt; EOF # 当再次接收到EOF时，退出交互模式3 /dev/null 文件/dev/null 是一个特殊的文件，写入到它的内容都会被丢弃；如果尝试从该文件读取内容，那么什么也读不到。但是 /dev/null 文件非常有用，将命令的输出重定向到它，会起到”禁止输出”的效果。 如果希望执行某个命令，但又不希望在屏幕上显示输出结果，那么可以将输出重定向到 /dev/null： 1$ command &gt; /dev/null 如果希望屏蔽 stdout 和 stderr，可以这样写：1$ command &gt; /dev/null 2&gt;&amp;1]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为linux增加虚拟内存swap]]></title>
    <url>%2F2019%2F01%2F14%2Flinux%2F%E4%B8%BAlinux%E5%A2%9E%E5%8A%A0%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98swap%2F</url>
    <content type="text"><![CDATA[使用hexo g时，遇到了因vps内存不足而被killed，通过查阅资料，通过以下方法解决了问题: 12345free -m dd if=/dev/zero of=/swap bs=4096 count=1572864mkswap /swapswapon /swapecho "LABEL=SWAP-sda /swap swap swap defaults 0 0" &gt;&gt; /etc/fstab 如果遇到swapon命令报错，可以尝试swapoff -a, 意思是关闭所有swap,然后重新执行swapon /swap即可]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell变量]]></title>
    <url>%2F2019%2F01%2F12%2Flinux%2F2.shell%E5%9F%BA%E6%9C%AC%E8%AA%9E%E6%B3%95%2F2.shell%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[定义变量定义变量时，变量名不加美元符号（$，PHP语言中变量需要），如： 1your_name="runoob.com" 注意，变量名和等号之间不能有空格，这可能和你熟悉的所有编程语言都不一样。同时，变量名的命名须遵循如下规则： 命名规范： 命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。 中间不能有空格，可以使用下划线（_）。 不能使用标点符号。 不能使用bash里的关键字（可用help命令查看保留关键字） 使用变量使用一个定义过的变量，只要在变量名前面加美元符号即可，如： 123your_name="qinjx"echo $your_nameecho $&#123;your_name&#125; 变量名外面的花括号是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界，比如下面这种情况： 123for skill in Ada Coffe Action Java; do echo "I am good at $&#123;skill&#125;Script"done 如果不给skill变量加花括号，写成echo “I am good at $skillScript”，解释器就会把$skillScript当成一个变量（其值为空），代码执行结果就不是我们期望的样子了。 推荐给所有变量加上花括号，这是个好的编程习惯。 已定义的变量，可以被重新定义，如： 1234your_name="tom"echo $your_nameyour_name="alibaba"echo $your_name 这样写是合法的，但注意，第二次赋值的时候不能写$your_name=&quot;alibaba&quot;，使用变量的时候才加美元符（$）。 只读变量使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。 下面的例子尝试更改只读变量，结果报错： 1234#!/bin/bashmyUrl="http://www.google.com"readonly myUrlmyUrl="http://www.runoob.com" 运行脚本，结果如下： 1/bin/sh: NAME: This variable is read only. 删除变量使用 unset 命令可以删除变量。语法： 1unset variable_name 变量被删除后不能再次使用。unset 命令不能删除只读变量。 实例 1234#!/bin/shmyUrl="http://www.runoob.com"unset myUrlecho $myUrl 以上实例执行将没有任何输出。 变量类型运行shell时，会同时存在三种变量： 局部变量 局部变量在脚本或命令中定义，仅在当前shell实例中有效，其他shell启动的程序不能访问局部变量。 环境变量 所有的程序，包括shell启动的程序，都能访问环境变量，有些程序需要环境变量来保证其正常运行。必要的时候shell脚本也可以定义环境变量。 shell变量 shell变量是由shell程序设置的特殊变量。shell变量中有一部分是环境变量，有一部分是局部变量，这些变量保证了shell的正常运行]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在centos7上设置新的网段]]></title>
    <url>%2F2019%2F01%2F10%2Flinux%2F3.%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%BB%B4%2F%E5%9C%A8centos7%E4%B8%8A%E8%AE%BE%E7%BD%AE%E6%96%B0%E7%9A%84%E7%BD%91%E6%AE%B5%2F</url>
    <content type="text"><![CDATA[1. eth0 eth0:1 和eth0.1三者的关系eth0 eth0:1 和eth0.1三者的关系分别对应于物理网卡、子网卡、虚拟VLAN网卡的关系： 物理网卡：物理网卡这里指的是服务器上实际的网络接口设备，这里我服务器上双网卡，在系统中看到的2个物理网卡分别对应是eth0和eth1这两个网络接口。 子网卡：子网卡在这里并不是实际上的网络接口设备，但是可以作为网络接口在系统中出现，如eth0:1、eth1:2这种网络接口。它们必须要依赖于物理网卡，虽然可以与物理网卡的网络接口同时在系统中存在并使用不同的IP地址，而且也拥有它们自己的网络接口配置文件。但是当所依赖的物理网卡不启用时（Down状态）这些子网卡也将一同不能工作。 虚拟VLAN网卡：这些虚拟VLAN网卡也不是实际上的网络接口设备，也可以作为网络接口在系统中出现，但是与子网卡不同的是，他们没有自己的配置文件。他们只是通过将物理网加入不同的VLAN而生成的VLAN虚拟网卡。如果将一个物理网卡通过vconfig命令添加到多个VLAN当中去的话，就会有多个VLAN虚拟网卡出现，他们的信息以及相关的VLAN信息都是保存在/proc/net/vlan/config这个临时文件中的，而没有独自的配置文件。它们的网络接口名是eth0.1、eth1.2这种名字。 注意：当需要启用VLAN虚拟网卡工作的时候，关联的物理网卡网络接口上必须没有IP地址的配置信息，并且，这些主物理网卡的子网卡也必须不能被启用和必须不能有IP地址配置信息。这个在网上看到的结论根据我的实际测试结果来看是不准确的，物理网卡本身可以绑定IP，并且给本征vlan提供通信网关的功能，但必须是在802.1q下。 2. /sbin/ifconfig 查看、配置、启用或禁用网络接口（网卡）的工具ifconfig 是一个用来查看、配置、启用或禁用网络接口的工具，这个工具极为常用的。比如我们可以用这个工具来配置网卡的IP地址、MAC地址、掩码、广播地址等。 值得一说的是用ifconfig 为网卡指定IP地址，这只是用来调试网络用的，并不会更改系统关于网卡的配置文件。服务器每次重启后，配置都会消失。 1. ifconfig配置网络接口语法：1ifconfig 网络端口 IP地址 hw MAC地址 netmask 掩码地址 broadcast 广播地址 [up/down] 2. ifconfig常用用法： ifconfig ： 查看主机激活状态的网络接口情况； 输出结果中：lo 是表示主机的回坏地址，eth0 表示第一块网卡， 其中 HWaddr 表示网卡的物理地址（MAC地址）； inet addr 用来表示网卡的IP地址，Bcast表示广播地址，Mask表示掩码地址 ifconfig -a ： 查看主机所有（包括没有被激活的）网络接口的情况 ifconfig eth0 ： 查看特定网络接口的状态 ifconfig eth0 down = ifup eth0 ： 如果eth0是激活的，就把它终止掉。此命令等同于 ifdown eth0； ifconfig eth0 up = ifdown eth0 ： 激活eth0 ； 此命令等同于 ifup eth0 ifconfig eth0 192.168.1.99 broadcast 192.168.1.255 netmask 255.255.255.0 ： 配置 eth0的IP地址、广播地址和网络掩码； ifconfig eth0 192.168.1.99 broadcast 192.168.1.255 netmask 255.255.255.0 up ： 配置IP地址、网络掩码、广播地址的同时，激活网卡eth0 ifconfig eth1 hw ether 00:11:00:00:11:22 ： 设置网卡的物理地址（MAC地址）。其中 hw 后面所接的是网络接口类型， ether表示以太网， 同时也支持 ax25 、ARCnet、netrom等，详情请查看 man ifconfig ； 虚拟IP技术在高可用领域像数据库SQLSERVER、web服务器等场景下使用很多，很疑惑它是怎么实现的，偶然，发现了一种方式可以实现虚拟ip。它的原理在于同一个物理网卡，是可以拥有多个ip地址的，至于虚拟网卡，也可用通过该方式拥有多个ip。 即对外提供数据库服务器的主机除了有一个真实IP外还有一个虚IP，使用这两个IP中的 任意一个都可以连接到这台主机，所有项目中数据库链接一项配置的都是这个虚IP，当服务器发生故障无法对外提供服务时，动态将这个虚IP切换到备用主机。 其实现原理主要是靠TCP/IP的ARP协议。因为ip地址只是一个逻辑地址，在以太网中MAC地址才是真正用来进行数据传输的物理地址，每台主机中都有一个ARP高速缓存，存储同一个网络内的IP地址与MAC地址的对应关系，以太网中的主机发送数据时会先从这个缓存中查询目标IP对应的MAC地址，会向这个MAC地址发送数据。操作系统会自动维护这个缓存。这就是整个实现 的关键。 在eth0处引用别名，设置完子网掩码即可 1$ ifconfig eth0:0 166.111.69.100 netmask 255.255.255.0 up 此时查看网卡信息 12345678910111213141516171819202122eth0 Link encap:Ethernet HWaddr 08:00:27:64:59:11 inet addr:166.111.69.17 Bcast:166.111.69.255 Mask:255.255.255.0 inet6 addr: 2402:f000:1:4412:a00:27ff:fe64:5911/64 Scope:Global inet6 addr: fe80::a00:27ff:fe64:5911/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:597673 errors:0 dropped:0 overruns:0 frame:0 TX packets:215472 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:67285933 (67.2 MB) TX bytes:22782158 (22.7 MB)eth0:0 Link encap:Ethernet HWaddr 08:00:27:64:59:11 inet addr:166.111.69.100 Bcast:166.111.69.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:843 errors:0 dropped:0 overruns:0 frame:0 TX packets:843 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:67174 (67.1 KB) TX bytes:67174 (67.1 KB) 然后找另一台机器ping这个vip(eth0:0)就可以看到显示结果了。写在/etc/rc.local里也可以，写在这里就不怕断电后机器无法正常使用了。 3. 用ifconfig 来配置虚拟网络接口：有时我们为了满足不同的需要还需要配置虚拟网络接口，比如我们用不同的IP地址来架运行多个HTTPD服务器，就要用到虚拟地址；这样就省却了同一个IP地址，如果开设两个的HTTPD服务器时，要指定端口号。 虚拟网络接口指的是为一个网络接口指定多个IP地址，虚拟接口是这样的 eth0:0 、 eth0:1、eth0:2 … .. eth1N。当然您为eth1 指定多个IP地址，也就是 eth1:0、eth1:1、eth1:2 … …以此类推； 123ifconfig eth1:0 192.168.1.250 hw ether 00:11:00:00:11:44 netmask 255.255.255.0 broadcast 192.168.1.255 upifconfig eth1:1 192.168.1.249 hw ether 00:11:00:00:11:55 netmask 255.255.255.0 broadcast 192.168.1.255 upifconfig eth1:3 192.168.1.251 注意：指定时，要为每个虚拟网卡指定不同的物理地址； 3.持久化的设置网段本机的ip地址是192.168.71.192，无法ping通192.168.72网段的服务器，因为，需要给服务器设置新的网段。 1. 列出所有的网卡和虚拟网卡 12345678910111213141516$ ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: em1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000 link/ether 20:04:0f:eb:cd:98 brd ff:ff:ff:ff:ff:ff inet 192.168.71.192/24 brd 192.168.71.255 scope global em1 valid_lft forever preferred_lft forever inet 172.10.3.192/24 brd 172.10.3.255 scope global em1:0 valid_lft forever preferred_lft forever inet6 fe80::ba64:d3bb:4896:90b1/64 scope link valid_lft forever preferred_lft forever..... 由于本机ip地址是192.168.71.192，由此可知，网卡为em1 2. 复制虚拟网卡 123456$ cd /etc/sysconfig/network-scripts/$ lsifcfg-em1 ifcfg-em1:0 ifcfg-lo ifdown-bnep ifdown-ipv6 ifdown-routes ...$ cp ifcfg-em1:0 ifcfg-em1:1$ lsifcfg-em1 ifcfg-em1:0 ifcfg-em1:1 ifcfg-lo ifdown-bnep ifdown-ipv6 ... 3. 编辑复制好的虚拟网卡 修改DEVICE和新的ip地址，注意修改DEVICE和IPADDR 12345678[root@localhost network-scripts]# vim ifcfg-em1:1DEVICE=em1:1TYPE=EthernetONBOOT=yesBOOTPROTO=staticIPADDR=192.168.72.192NETMASK=255.255.255.0#GATEWAY= 4. 重启网络服务1[root@localhost network-scripts]# systemctl restart network]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell的基本説明]]></title>
    <url>%2F2019%2F01%2F10%2Flinux%2F2.shell%E5%9F%BA%E6%9C%AC%E8%AA%9E%E6%B3%95%2F1.shell%E5%9F%BA%E6%9C%AC%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[第一个shell脚本打开文本编辑器(可以使用 vi/vim 命令来创建文件)，新建一个文件 test.sh，扩展名为 sh（sh代表shell），扩展名并不影响脚本执行，见名知意就好，如果你用 php 写 shell 脚本，扩展名就用 php 好了。 输入一些代码，第一行一般是这样： 12#!/bin/bashecho "Hello World !" #! 是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种 Shell。 运行 Shell 脚本有两种方法：1、作为可执行程序 12chmod +x ./test.sh #使脚本具有执行权限./test.sh #执行脚本 注意，一定要写成 ./test.sh，而不是 test.sh，运行其它二进制的程序也一样，直接写 test.sh，linux 系统会去 PATH 里寻找有没有叫 test.sh 的，而只有 /bin, /sbin, /usr/bin，/usr/sbin 等在 PATH 里，你的当前目录通常不在 PATH 里，所以写成 test.sh 是会找不到命令的，要用 ./test.sh 告诉系统说，就在当前目录找。 2、作为解释器参数 这种运行方式是，直接运行解释器，其参数就是 shell 脚本的文件名，如： 12/bin/sh test.sh/bin/php test.php 这种方式运行的脚本，不需要在第一行指定解释器信息，写了也没用。]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[javascript之网页可见区域宽和高]]></title>
    <url>%2F2019%2F01%2F09%2F%E5%89%8D%E7%AB%AF%2Fjavascript%E4%B9%8B%E7%BD%91%E9%A1%B5%E5%8F%AF%E8%A7%81%E5%8C%BA%E5%9F%9F%E5%AE%BD%E5%92%8C%E9%AB%98%2F</url>
    <content type="text"><![CDATA[网页可见区域宽： document.body.clientWidth; 网页可见区域高： document.body.clientHeight; 网页可见区域宽： document.body.offsetWidth (包括边线的宽); 网页可见区域高： document.body.offsetHeight (包括边线的宽); 网页正文全文宽： document.body.scrollWidth; 网页正文全文高： document.body.scrollHeight; 网页被卷去的高： document.body.scrollTop; 网页被卷去的左： document.body.scrollLeft; 网页正文部分上： window.screenTop; 网页正文部分左： window.screenLeft; 屏幕分辨率的高： window.screen.height; 屏幕分辨率的宽： window.screen.width; 屏幕可用工作区高度： window.screen.availHeight; 屏幕可用工作区宽度：window.screen.availWidth;]]></content>
  </entry>
  <entry>
    <title><![CDATA[Windows中iscsicli命令的使用]]></title>
    <url>%2F2018%2F11%2F01%2Fwindows%2FWindows%E4%B8%ADiscsicli%E5%91%BD%E4%BB%A4%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[设置msiscsi服务的启动状态123sc config msiscsi start= autonet start msiscsi 使用iscsicli命令连接Target1234iscsicli QAddTargetPortal &lt;Portal IP Address&gt;iscsicli ListTargetsiscsicli QloginTarget &lt;target_iqn&gt; 示例123456789101112131415161718C:\Users\user&gt;iscsicli QAddTargetPortal 192.168.71.200Microsoft iSCSI 发起程序版本 10.0 內部版本 17134操作成功完成。C:\Users\user&gt;iscsicli ListTargetsMicrosoft iSCSI 发起程序版本 10.0 內部版本 17134目标列表: iqn.2001-06.cn.com.lee:disk-array-000d62ac0:dev5.ctr1操作成功完成。C:\Users\user&gt;iscsicli QloginTarget iqn.2001-06.cn.com.lee:disk-array-000d62ac0:dev5.ctr1Microsoft iSCSI 发起程序版本 10.0 內部版本 17134会话 ID 是 0xffffd001de85b010-0x400001370000001e连接 ID 是 0xffffd001de85b010-0xb0操作成功完成。]]></content>
  </entry>
  <entry>
    <title><![CDATA[python子程序subprocess]]></title>
    <url>%2F2018%2F11%2F01%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E5%AD%90%E7%A8%8B%E5%BA%8Fsubprocess%2F</url>
    <content type="text"><![CDATA[python subprocess用于程序执行时调用子程序，通过stdout,stdin和stderr进行交互。123Stdout子程序执行结果返回，如文件、屏幕等Stdin 子程序执行时的输入，如文件，文件对象Stderr 错误输出 常用的两种方式（以shell程序为例）： 12subprocess.Popen(&apos;脚本/shell&apos;, shell=True) #无阻塞并行subprocess.call(&apos;脚本/shell&apos;, shell=True) #等子程序结束再继续 两者的区别是前者无阻塞,会和主程序并行运行,后者必须等待命令执行完毕,如果想要前者编程阻塞加wait()： 12p = subprocess.Popen(&apos;脚本/shell&apos;, shell=True)a=p.wait() # 返回子进程结果 代码实例对于diskpart这种命令行交互程序，应该使用Popen 1234import subprocessdiskpart = subprocess.Popen('diskpart',stdout = subprocess.PIPE, stdin = subprocess.PIPE,stderr = subprocess.PIPE,shell=False)stdout, stderr = diskpart.communicate(b'RESCAN\r\nLIST DISK\r\n')print (stdout.decode('gbk')) 输出:123456789Microsoft DiskPart 版本 10.0.17134.1Copyright (C) Microsoft Corporation.在计算机上: DESKTOP-UI2H32ODISKPART&gt; 磁盘 ### 状态 大小 可用 Dyn Gpt -------- ------------- ------- ------- --- --- 磁盘 0 联机 465 GB 1024 KB * 这是一次性交互，读入是stdin，直接执行完毕后，返回给stdout，communicate通信一次之后即关闭了管道。但如果需要多次交互，频繁地和子线程通信不能使用communicate()， 可以分步进行通信，如下： 1234567p= subprocess.Popen(["ls","-l"], stdin=subprocess.PIPE,stdout=subprocess.PIPE,shell=False) //输入p.stdin.write('your command') p.stdin.flush() //查看输出p.stdout.readline() p.stdout.read() 参数shell的意义call()和Popen()都有shell参数，默认为False，可以赋值为True。参数shell（默认为False）指定是否使用shell来执行程序。如果shell为True，前面会自动加上/bin/sh命令，则建议传递一个字符串（而不是序列）给args，如果为False就必须传列表，分开存储命令内容。比如1subprocess.Popen(&quot;cat test.txt&quot;, shell=True) 相当于1subprocess.Popen([&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;cat test.txt&quot;]) 原因具体是: 在Linux下，shell=False时, Popen调用os.execvp()执行args指定的程序；在Windows下，Popen调用CreateProcess()执行args指定的外部程序，args传入字符和序列都行，序列会自动list2cmdline()转化为字符串，但需要注意的是，并不是MS Windows下所有的程序都可以用list2cmdline来转化为命令行字符串。 所以，windows下 1subprocess.Popen(&quot;notepad.exe test.txt&quot; shell=True) 等同于 1subprocess.Popen(&quot;cmd.exe /C &quot;+&quot;notepad.exe test.txt&quot; shell=True） shell=True可能引起问题传递shell=True在与不可信任的输入绑定在一起时可能出现安全问题警告 执行的shell命令如果来自不可信任的输入源将使得程序容易受到shell注入攻击，一个严重的安全缺陷可能导致执行任意的命令。因为这个原因，在命令字符串是从外部输入的情况下使用shell=True 是强烈不建议的：12345&gt;&gt;&gt; from subprocess import call&gt;&gt;&gt; filename = input(&quot;What file would you like to display?\n&quot;)What file would you like to display?non_existent; rm -rf / #&gt;&gt;&gt; call(&quot;cat &quot; + filename, shell=True) # Uh-oh. This will end badly... shell=False禁用所有基于shell的功能，所以不会受此漏洞影响；参见Popen构造函数文档中的注意事项以得到如何使shell=False工作的有用提示。当使用shell=True时，pipes.quote()可以用来正确地转义字符串中将用来构造shell命令的空白和shell元字符。]]></content>
  </entry>
  <entry>
    <title><![CDATA[resilio btsync自动同步网盘]]></title>
    <url>%2F2018%2F10%2F16%2F%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%2Fresilio-btsync%E8%87%AA%E5%8A%A8%E5%90%8C%E6%AD%A5%E7%BD%91%E7%9B%98%2F</url>
    <content type="text"><![CDATA[linux服务器我在linux部署了sync客户端，管理地址为 0.0.0.0:8080/gui/]]></content>
  </entry>
  <entry>
    <title><![CDATA[在python中使用代理]]></title>
    <url>%2F2018%2F10%2F12%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2F%E5%9C%A8python%E4%B8%AD%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[Urllib首先我们以最基础的 Urllib 为例，来看一下代理的设置方法，代码如下： 1234567891011121314from urllib.error import URLErrorfrom urllib.request import ProxyHandler, build_openerproxy = '127.0.0.1:9743'proxy_handler = ProxyHandler(&#123; 'http': 'http://' + proxy, 'https': 'https://' + proxy&#125;)opener = build_opener(proxy_handler)try: response = opener.open('http://httpbin.org/get') print(response.read().decode('utf-8'))except URLError as e: print(e.reason) 运行结果如下： 1234567891011&#123; "args": &#123;&#125;, "headers": &#123; "Accept-Encoding": "identity", "Connection": "close", "Host": "httpbin.org", "User-Agent": "Python-urllib/3.6" &#125;, "origin": "106.185.45.153", "url": "http://httpbin.org/get"&#125; 在这里我们需要借助于 ProxyHandler 设置代理，参数是字典类型，键名为协议类型，键值是代理，注意此处代理前面需要加上协议，即 http 或者 https，此处设置了 http 和 https 两种代理，当我们请求的链接是 http 协议的时候，它会调用 http 代理，当请求的链接是 https 协议的时候，它会调用https代理，所以此处生效的代理是：http://127.0.0.1:9743 创建完 ProxyHandler 对象之后，我们需要利用 build_opener() 方法传入该对象来创建一个 Opener，这样就相当于此 Opener 已经设置好代理了，接下来直接调用它的 open() 方法即可使用此代理访问我们所想要的链接。 运行输出结果是一个 Json，它有一个字段 origin，标明了客户端的 IP，此处的 IP 验证一下，确实为代理的 IP，而并不是我们真实的 IP，所以这样我们就成功设置好代理，并可以隐藏真实 IP 了。 如果遇到需要认证的代理，我们可以用如下的方法设置： 1234567891011121314from urllib.error import URLErrorfrom urllib.request import ProxyHandler, build_openerproxy = 'username:password@127.0.0.1:9743'proxy_handler = ProxyHandler(&#123; 'http': 'http://' + proxy, 'https': 'https://' + proxy&#125;)opener = build_opener(proxy_handler)try: response = opener.open('http://httpbin.org/get') print(response.read().decode('utf-8'))except URLError as e: print(e.reason) 这里改变的只是 proxy 变量，只需要在代理前面加入代理认证的用户名密码即可，其中 username 就是用户名，password 为密码，例如 username 为foo，密码为 bar，那么代理就是 foo:bar@127.0.0.1:9743。 如果代理是 SOCKS5 类型，那么可以用如下方式设置代理： 123456789101112import socksimport socketfrom urllib import requestfrom urllib.error import URLErrorsocks.set_default_proxy(socks.SOCKS5, '127.0.0.1', 9742)socket.socket = socks.socksockettry: response = request.urlopen('http://httpbin.org/get') print(response.read().decode('utf-8'))except URLError as e: print(e.reason) 此处需要一个 Socks 模块，可以通过如下命令安装： 1pip3 install PySocks 本地我有一个 SOCKS5 代理，运行在 9742 端口，运行成功之后和上文 HTTP 代理输出结果是一样的： 1234567891011&#123; "args": &#123;&#125;, "headers": &#123; "Accept-Encoding": "identity", "Connection": "close", "Host": "httpbin.org", "User-Agent": "Python-urllib/3.6" &#125;, "origin": "106.185.45.153", "url": "http://httpbin.org/get"&#125; 结果的 origin 字段同样为代理的 IP，设置代理成功。 Requests对于 Requests 来说，代理设置更加简单，我们只需要传入 proxies 参数即可。 还是以上例中的代理为例，我们来看下 Requests 的代理的设置： 123456789101112import requestsproxy = '127.0.0.1:9743'proxies = &#123; 'http': 'http://' + proxy, 'https': 'https://' + proxy,&#125;try: response = requests.get('http://httpbin.org/get', proxies=proxies) print(response.text)except requests.exceptions.ConnectionError as e: print('Error', e.args) 运行结果： 123456789101112&#123; "args": &#123;&#125;, "headers": &#123; "Accept": "*/*", "Accept-Encoding": "gzip, deflate", "Connection": "close", "Host": "httpbin.org", "User-Agent": "python-requests/2.18.1" &#125;, "origin": "106.185.45.153", "url": "http://httpbin.org/get"&#125; 可以发现 Requests 的代理设置比 Urllib 简单很多，只需要构造代理字典即可，然后通过 proxies 参数即可设置代理，不需要重新构建 Opener。 可以发现其运行结果的 origin 也是代理的 IP，证明代理已经设置成功。 如果代理需要认证，同样在代理的前面加上用户名密码即可，代理的写法就变成： 1proxy = 'username:password@127.0.0.1:9743' 和 Urllib 一样，只需要将 username 和 password 替换即可。 如果需要使用 SOCKS5 代理，则可以使用如下方式： 123456789101112import requestsproxy = '127.0.0.1:9742'proxies = &#123; 'http': 'socks5://' + proxy, 'https': 'socks5://' + proxy&#125;try: response = requests.get('http://httpbin.org/get', proxies=proxies) print(response.text)except requests.exceptions.ConnectionError as e: print('Error', e.args) 在这里需要额外安装一个 Socks 模块，命令如下： 1pip3 install "requests[socks]" 运行结果是完全相同的： 123456789101112&#123; &quot;args&quot;: &#123;&#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;*/*&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;python-requests/2.18.1&quot; &#125;, &quot;origin&quot;: &quot;106.185.45.153&quot;, &quot;url&quot;: &quot;http://httpbin.org/get&quot;&#125; 另外还有一种设置方式，和 Urllib 中的方法相同，使用 socks 模块，也需要像上文一样安装该库，设置方法如下： 1234567891011import requestsimport socksimport socketsocks.set_default_proxy(socks.SOCKS5, '127.0.0.1', 9742)socket.socket = socks.socksockettry: response = requests.get('http://httpbin.org/get') print(response.text)except requests.exceptions.ConnectionError as e: print('Error', e.args) 这样也可以设置 SOCKS5 代理，运行结果完全相同，相比第一种方法，此方法是全局设置，不同情况可以选用不同的方法。 SeleniumSelenium 同样也可以设置代理，在这里分两种介绍，一个是有界面浏览器，以 Chrome 为例介绍，另一种是无界面浏览器，以 PhantomJS 为例介绍。 Chrome对于 Chrome 来说，用 Selenium 设置代理的方法也非常简单，设置方法如下： 1234567from selenium import webdriverproxy = '127.0.0.1:9743'chrome_options = webdriver.ChromeOptions()chrome_options.add_argument('--proxy-server=http://' + proxy)browser = webdriver.Chrome(chrome_options=chrome_options)browser.get('http://httpbin.org/get') 在这里我们通过 ChromeOptions 来设置代理，在创建 Chrome 对象的时候通过 chrome_options 参数传递即可。 这样在运行之后便会弹出一个 Chrome 浏览器，访问目标链接之后输出结果如下： 1234567891011121314&#123; "args": &#123;&#125;, "headers": &#123; "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8", "Accept-Encoding": "gzip, deflate", "Accept-Language": "zh-CN,zh;q=0.8", "Connection": "close", "Host": "httpbin.org", "Upgrade-Insecure-Requests": "1", "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36" &#125;, "origin": "106.185.45.153", "url": "http://httpbin.org/get"&#125; 可以看到 origin 同样为代理 IP 的地址，代理设置成功。 如果代理是认证代理，则设置方法相对比较麻烦，方法如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768from selenium import webdriverfrom selenium.webdriver.chrome.options import Optionsimport zipfileip = '127.0.0.1'port = 9743username = 'foo'password = 'bar'manifest_json = """&#123; "version": "1.0.0", "manifest_version": 2, "name": "Chrome Proxy", "permissions": [ "proxy", "tabs", "unlimitedStorage", "storage", "&lt;all_urls&gt;", "webRequest", "webRequestBlocking" ], "background": &#123; "scripts": ["background.js"] &#125;&#125;"""background_js = """var config = &#123; mode: "fixed_servers", rules: &#123; singleProxy: &#123; scheme: "http", host: "%(ip)s", port: %(port)s &#125; &#125; &#125;chrome.proxy.settings.set(&#123;value: config, scope: "regular"&#125;, function() &#123;&#125;);function callbackFn(details) &#123; return &#123; authCredentials: &#123; username: "%(username)s", password: "%(password)s" &#125; &#125;&#125;chrome.webRequest.onAuthRequired.addListener( callbackFn, &#123;urls: ["&lt;all_urls&gt;"]&#125;, ['blocking'])""" % &#123;'ip': ip, 'port': port, 'username': username, 'password': password&#125;plugin_file = 'proxy_auth_plugin.zip'with zipfile.ZipFile(plugin_file, 'w') as zp: zp.writestr("manifest.json", manifest_json) zp.writestr("background.js", background_js)chrome_options = Options()chrome_options.add_argument("--start-maximized")chrome_options.add_extension(plugin_file)browser = webdriver.Chrome(chrome_options=chrome_options)browser.get('http://httpbin.org/get') 在这里需要在本地创建一个 manifest.json 配置文件和 background.js 脚本来设置认证代理，运行之后本地会生成一个 proxy_auth_plugin.zip 文件保存配置。 运行结果和上例一致，origin 同样为代理 IP。 PhantomJS对于 PhantomJS，代理设置方法可以借助于 service_args 参数，也就是命令行参数，代理设置方法如下： 123456789from selenium import webdriverservice_args = [ '--proxy=127.0.0.1:9743', '--proxy-type=http']browser = webdriver.PhantomJS(service_args=service_args)browser.get('http://httpbin.org/get')print(browser.page_source) 在这里我们只需要使用 service_args 参数，将命令行的一些参数定义为列表，在初始化的时候传递即可。 运行结果： 12345678910111213&#123; &quot;args&quot;: &#123;&#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, &quot;Accept-Language&quot;: &quot;zh-CN,en,*&quot;, &quot;Connection&quot;: &quot;close&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X) AppleWebKit/538.1 (KHTML, like Gecko) PhantomJS/2.1.0 Safari/538.1&quot; &#125;, &quot;origin&quot;: &quot;106.185.45.153&quot;, &quot;url&quot;: &quot;http://httpbin.org/get&quot;&#125; 运行结果的 origin 同样为代理的 IP，设置代理成功。 如果需要认证，那么只需要再加入 –proxy-auth 选项即可，这样参数就改为： 12345service_args = [ '--proxy=127.0.0.1:9743', '--proxy-type=http', '--proxy-auth=username:password'] 将 username 和 password 替换为认证所需的用户名和密码即可。 参考链接Python3 中代理使用方法总结 ​]]></content>
  </entry>
  <entry>
    <title><![CDATA[python多线程1 启动与停止线程]]></title>
    <url>%2F2018%2F10%2F09%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[在windows下实现异步编程，个人认为还是使用多线程为佳，在windows下使用多进程会遇到很多的坑,比如创建进程必须要在if name == __main__下面，因此需要重新调整软件架构，又比如有的对象不可以被pickle则不能当作参数传递，解决起来十分棘手。 启动线程threading 库可以在单独的线程中执行任何的在 Python 中可以调用的对象。你可以创建一个 Thread 对象并将你要执行的对象以 target 参数的形式提供给该对象。 下面是一个简单的例子： 123456789101112131415import timeimport sysdef countdown(n): while n &gt; 0: print('T-minus', n) n -= 1 time.sleep(1)# Create and launch a threadfrom threading import Threadt = Thread(target=countdown, args=(10,))t.start()time.sleep(5)print('main process over')sys.exit(0) 输出: 1234567891011T-minus 10T-minus 9T-minus 8T-minus 7T-minus 6main process overT-minus 5T-minus 4T-minus 3T-minus 2T-minus 1 当你创建好一个线程对象后，该对象并不会立即执行，除非你调用它的 start() 方法（当你调用 start() 方法时，它会调用你传递进来的函数，并把你传递进来的参数传递给该函数）。 Python中的线程会在一个单独的系统级线程中执行（比如说一个 POSIX 线程或者一个 Windows 线程），这些线程将由操作系统来全权管理。线程一旦启动，将独立执行直到目标函数返回。 主线程报错或者主动终止（sys.exit(0)）都不会对子线程产生影响，子线程仍会等待程序执行完毕并退出 join与查看线程状态你可以查询一个线程对象的状态，看它是否还在执行： 1234if t.is_alive(): print('Still running')else: print('Completed') 你也可以将一个线程加入到当前线程，并等待它终止(join的含义是将子线程合并入主线程，由异步变为同步)： 1234567891011121314151617import timeimport sysdef countdown(n): while n &gt; 0: print('T-minus', n) n -= 1 time.sleep(1)# Create and launch a threadfrom threading import Threadt = Thread(target=countdown, args=(10,))t.start()# joint.join()time.sleep(5)print('main process over')sys.exit(0) 输出:1234567891011T-minus 10T-minus 9T-minus 8T-minus 7T-minus 6T-minus 5T-minus 4T-minus 3T-minus 2T-minus 1main process over 守护进程Python解释器直到所有线程都终止前仍保持运行。如果你希望主线程终止后，子线程自动销毁，可以使用daemon参数：12345678910111213141516import timeimport sysdef countdown(n): while n &gt; 0: print('T-minus', n) n -= 1 time.sleep(1)# Create and launch a threadfrom threading import Thread# daemon=Truet = Thread(target=countdown, args=(10,),daemon=True)t.start()time.sleep(5)print('main process over')sys.exit(0) 输出:123456T-minus 10T-minus 9T-minus 8T-minus 7T-minus 6main process over 后台线程无法等待，不过，这些线程会在主线程终止时自动销毁。 除了如上所示的两个操作，并没有太多可以对线程做的事情。你无法结束一个线程，无法给它发送信号，无法调整它的调度，也无法执行其他高级操作。]]></content>
  </entry>
  <entry>
    <title><![CDATA[jinjia2模板引擎]]></title>
    <url>%2F2018%2F09%2F30%2Fpython%2Fjinjia2%E6%A8%A1%E6%9D%BF%E5%BC%95%E6%93%8E%2F</url>
    <content type="text"><![CDATA[for循环for循环序号loop.index 从1开始序号循环loop.index0 从零开始序号循环 123456789&#123;% for item in items %&#125; &lt;tr&gt; &lt;th&gt;&#123;&#123;loop.index&#125;&#125;&lt;/th&gt; &lt;td&gt;&#123;&#123;item[1]&#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123;item[2]&#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123;item[3]&#125;&#125;&lt;/td&gt; &lt;td&gt;&lt;input item_id = &#123;&#123;item[0]&#125;&#125; type="checkbox"&gt;&lt;/td&gt; &lt;/tr&gt;&#123;% endfor %&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[flask 使用蓝图实现模块化路由应用]]></title>
    <url>%2F2018%2F09%2F21%2Fpython%2F3.python%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93%2Fflask-%E4%BD%BF%E7%94%A8%E8%93%9D%E5%9B%BE%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9D%97%E5%8C%96%E8%B7%AF%E7%94%B1%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[flask蓝图将所有路由设定在入口文件中，在大型应用中会使项目混乱并难以维护。flask提供了蓝图Blueprint对象来解决这个问题。我们可以实例化一个蓝图对象，并在此文件中设定好路由，然后在入口文件中引入并注册此蓝图实例即可。 蓝图文件 admin.py，这个文件设定了admin路由并绑定了相应的方法 12345678910from flask import Blueprint,render_template, request,jsonify# 实例化一个蓝图admin = Blueprint('admin',__name__)# 页面路由，这里路由可以不指定前缀路径，最后在入口文件指定@admin.route("/login")def login(): return render_template('login.html')@admin.route("/regist")def register(): return render_template('register.html') 入口文件： 1234567891011121314from flask import Flask, jsonify, render_template, request# 1.引入蓝图文件from admin import adminapp = Flask(__name__)@app.route("/")def index(name=None): return render_template('index.html')# 2.注册蓝图app.register_blueprint(admin,url_prefix='/admin')if __name__ == '__main__': app.run(host='0.0.0.0',port = 80,debug = True) 程序启动后，访问127.0.0.1/admin/login和127.0.0.1/admin/register便可以访问到在蓝图中的相应页面]]></content>
  </entry>
  <entry>
    <title><![CDATA[关于前端开发谈谈单元测试]]></title>
    <url>%2F2018%2F09%2F20%2F%E5%89%8D%E7%AB%AF%2F%E5%85%B3%E4%BA%8E%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91%E8%B0%88%E8%B0%88%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[单元测试Unit Test很早就知道单元测试这样一个概念，但直到几个月前，我真正开始接触和使用它。究竟什么是单元测试？我想也许很多使用了很久的人也不一定能描述的十分清楚，所以写了这篇文章来尝试描述它的特征和原则，以帮助更多人。 什么是单元测试？先来看看单元测试的定义，在维基百科英文版中可以找到Kolawa Adam在 Automated Defect Prevention: Best Practices in Software Management 一书中对单元测试的定义： In computer programming, unit testing is a method by which individual units of source code, sets of one or more computer program modules together with associated control data, usage procedures, and operating procedures are tested to determine if they are fit for use. 重点在于最后，单元测试的目的显而易见，用来确定是否适合使用。而测试的方法则包括控制数据，使用和操作过程。那么以我的理解，每个单元测试就是一段用于测试一个模块或接口是否能达到预期结果的代码。开发人员需要使用代码来定义一个可用的衡量标准，并且可以快速检验。 很快我发现有一个误区，许多人认为单元测试必须是一个runner集中运行所有单元的测试，并一目了然。不，这仅仅是一种自动化单元测试的最佳实践，在一些小型项目中单元测试可能仅仅是一组去除其他特性的接口调用。甚至在一些图形处理或布局的项目中单元测试可以结合自身特性变的十分有趣，比如Masonry，一个网格布局库，在它的单元测试中不是一个红或绿的条目，而是一行一行的小格布局用以说明布局被完成的事实，这样比代码检查布局是否正确再以颜色显示结果来得更直观高效，也避免了测试程序本身的bug导致的失误。 打个比方，单元测试就像一把尺子，当测量的对象是一个曲面时，也许可以花费大力气去将它抽象成平面，但我更提倡量身定做一把弯曲的尺子去适应这个曲面。无论怎样，单元测试是为了生产代码而写，它应当足够的自由奔放，去适应各种各样的生产代码。 为什么要单元测试？也许定义中已经很清楚的指明了其意义，确认某段代码或模块或接口是否适合使用，但我想会有更多的人认为，直接在测试环境中使用软件可以更加确保软件是否可用。不，在实际使用过程中会伴随着一大批的附带操作大量增加测试时间，并且无法保证其测试覆盖率。所以我认为单元测试的目的并不仅仅是确认是否可用，而是更高效更稳定的确认其是否可用。 随着项目规模的增加，函数、方法、变量都在递增，尤其是进度的不足，来自产品经理的压力，还有QA所带来的各种Bug报告会让原本整洁的代码变得一片混乱。我甚至见过同一个接口以不同的名称出现在8个不同的控制器中。这时也许我们首先想到的是重构，可是等等，在重构结束时我们如何确定项目仅仅是被重构了，而不是被改写了？此时单元测试将是一根救命稻草，它是一个衡量标准，告诉开发人员这么做是否将改变结果。 不仅仅是这样。许多人认为单元测试，甚至整个测试都是在编码结束后的一道工序，而修复bug也不过是在做垃圾掩埋一类的工作。但测试应该伴随整个编码或软件周期进行，还有将在后面提到的TDD这样有趣的东西，单元测试将超前于编码。我的意思是，单元测试应该是一个框架、标准，经常被形容被脚手架，像建筑一样，脚手架的高度至少应该和大楼高度不相上下，甚至一开始就搭好脚手架。 如何做单元测试？弄清了单元测试的目的和意义，但如何开始？很简单，首先它是一个检验，所以应该只有pass或fail两种情况。而检验的对象应该是某个接口或模块，所以应该调用它获得一个结果。检验这个结果就是单元测试的基本动作，就拿一个除法函数来做例子： 123456789function division (a, b) &#123; return a / b;&#125;var result = division(4, 2);if (result === 2) &#123; alert('pass');&#125; else &#123; alert('fail');&#125; 显然，将会提示pass通过。但是问题来了，这个测试的用例太单一和普通了，如果使用0做除数呢？是NaN？还是Infinity？或者在实际使用时，产品需要一个0来代替这样一个不符合数学概念的结果去适应必须为数字类型的某种计算，于是division出现了一个bug。另外当覆盖率增加，也意味着用例的增加，我们需要把if条件语句提出来做成一个函数多次调用。还有alert方法，如果用例太多，我相信你会点确认点到手软，也许可以直接显示在页面上。 所以我添加了一个关于除数为0的用例，并重构了代码： 123456789101112131415161718192021function division (a, b) &#123; if (b === 0) &#123; return 0; &#125; else &#123; return a / b; &#125;&#125;function matcher (name, result, expect) &#123; if (result === expect) &#123; _print(name + '- pass'); &#125; else &#123; _print(name + '- fail'); &#125; function _print (str) &#123; var _bar = document.createElement('p'); _bar.innerText = str; document.body.appendChild(_bar); &#125;&#125;matcher('normal', division(4, 2), 2);matcher('zero', division(5, 0), 0); 现在可以使用matcher方法添加许多测试用例，并且还能为该用例命名，在页面中直接显示每个用例是否通过。这样一个基本的单元测试就完成了，当然它的覆盖率还远远不够，这里仅作为一个例子。另外为了提高效率还应该使用颜色来标记是否通过，可以一目了然。 测试驱动开发TDD是Test Driven Development 的缩写，也就是测试驱动开发。 通常传统软件工程将测试描述为软件生命周期的一个环节，并且是在编码之后。但敏捷开发大师Kent Beck在2003年出版了 Test Driven Development By Example 一书，从而确立了测试驱动开发这个领域。 TDD需要遵循如下规则： 写一个单元测试去描述程序的一个方面。 运行它应该会失败，因为程序还缺少这个特性。 为这个程序添加一些尽可能简单的代码保证测试通过。 重构这部分代码，直到代码没有重复、代码责任清晰并且结构简单。 持续重复这样做，积累代码。 另外，衡量是否使用了TDD的一个重要标准是测试对代码的覆盖率，覆盖率在80%以下说明一个团队没有充分掌握TDD，当然高覆盖率也不能说一定使用了TDD，这仅仅是一个参考指标。 在我看来，TDD是一种开发技术，而非测试技术，所以它对于代码构建的意义远大于代码测试。也许最终的代码和先开发再测试写的测试代码基本一致，但它们仍然是有很大不同的。TDD具有很强的目的性，在直接结果的指导下开发生产代码，然后不断围绕这个目标去改进代码，其优势是高效和去冗余的。所以其特点应该是由需求得出测试，由测试代码得出生产代码。打个比方就像是自行车的两个轮子，虽然都是在向同一个方向转动，但是后轮是施力的，带动车子向前，而前轮是受力的，被向前的车子带动而转。 行为驱动开发所谓的BDD行为驱动开发，即Behaviour Driven Development，是一种新的敏捷开发方法。它更趋向于需求，需要共同利益者的参与，强调用户故事（User Story）和行为。2009年，在伦敦发表的“敏捷规格，BDD和极限测试交流”[3]中，Dan North对BDD给出了如下定义： BDD是第二代的、由外及内的、基于拉(pull)的、多方利益相关者的(stakeholder)、多种可扩展的、高自动化的敏捷方法。它描述了一个交互循环，可以具有带有良好定义的输出（即工作中交付的结果）：已测试过的软件。 另外最主观的区别就是用词，‘example’取代了‘test’，‘describe’取代了‘class’，‘behaviour’取代了‘method’等等。这正是其特征之一，自然语言的加入，使得非程序人员也能参与到测试用例的编写中来，也大大降低了客户、用户、项目管理者与开发者之间来回翻译的成本。 简单来说，我认为BDD更加注重业务需求而不是技术，虽然看起来BDD确实是比ATDD做的更好，但这是一种误导，这仅仅是就某种环境下而言的。而且以国内的现状来看TDD要比BDD更适合，因为它不需要所有人员的理解和加入。 单元测试框架无论如何，单元测试永远是少不了的。其实在单元测试中测试代码和生产代码应该是等量的，正如Robert C. Martin在其 Clean Code: A Handbook of Agile Software Craftsmanship 一书中所写： 测试必须随生产代码的演进而修改，测试越脏就越难修改 于是新的测试很难被加入其中，测试代码的维护变得异常困难，最终在各种压力之中只有扔掉测试代码组。但是没有了测试代码，就失去了确保对代码的改动能如愿以偿的能力，各种问题随之而来。因此，单元测试也需要一种行之有效的实践来确保其质量和可维护性。 所以正如生产代码一样，测试代码也有框架，下面介绍几种主流的Javascript的单元测试框架。 Jasmine有一类框架叫做xUnit，来源于著名的JAVA测试框架JUnit，xUnit则代表了一种模式，并且使用这样的命名。在Javascript中也有这样的一个老牌框架JsUnit，他的作者是Edward Hieatt来自Pivotal Labs，但在几年前JsUnit就已经停止维护了，他们带来了新的BDD框架Jasmine。 Jasmine不依赖于任何框架，所以适用于所有的Javascript代码。使用一个全局函数 describe 来描述每个测试，并且可以嵌套。describe函数有2个参数，一个是字符串用于描述，一个是函数用于测试。在该函数中可以使用全局函数 it 来定义Specs，也就是单元测试的主要内容， 使用 expect 函数来测试： 123456describe('A suite', function () &#123; it('is a spec', function () &#123; var a = true; expect(a).toBe(true); &#125;);&#125;); 另外如果想去掉某个describe，无须注释掉整段代码，只需要在describe前面加上x即可忽略该describe。 MatchertoBe方法是一个基本的 matcher 用来定义判断规则，可以看得出来Jasmine的方法是非常语义化的，“expect ‘a’ to be true”，如果想判断否定条件，则只需要在toBe前调用 not 方法： 1expect(a).not().toBe(false); 除了toBe这样基本的还有许多其他的Matcher，比如 toEqual 。很多初学Jasmine会弄不清和toBe的区别，一个简单的例子就能明白它们的区别： 12expect(&#123;&#125;).not().toBe(&#123;&#125;);expect(&#123;&#125;).toEqual(&#123;&#125;); 一个新建的Object不是（not to be）另一个新建的Object，但是它们是相等（to equal）的。还有 toMatch 可以使用字符串或者正则表达式来验证，以及其他一些特殊验证，比如undefined或者boolean的判断， toThrow 可以检查函数所抛出的异常。另外Jasmine还支持自定义Matcher，以NaN的检查为例，像这样使用beforeEach方法在每个测试执行前添加一个matcher： 1234567beforeEach(function () &#123; this.addMatchers(&#123; toBeNaN: function (expected) &#123; return isNaN(expected); &#125; &#125;);&#125;); 可以想到，其参数expected是传入的一个期望的字面量，而在expect方法中传入的参数，可以通过 this.acturl 获取，是否调用了 not 方法则可以通过 this.isNot 获取，这是一个boolean值。最后测试输出的失败信息应该使用 this.message 来定义，不过它是一个function，然后在其中返回一个信息。所以继续增进toBeNaN： 123456789101112beforeEach(function () &#123; this.addMatchers(&#123; toBeNaN: function (expected) &#123; var actual = this.actual; var not = this.isNot ? ' not' : ''; this.message = function () &#123; return 'Expected ' + actual + not + ' to be NaN ' + expected; &#125;; return isNaN(expected); &#125; &#125;);&#125;); 这样一个完整的matcher就创建成了。 另外需要说明的是对应beforeEach是在每个spec之前执行， afterEach 方法则是在每个spec之后执行。这是一种AOP，即面向方面的编程（Aspect Oriented Programming）。比如有时候为了测试一个对象，可能需要多次创建和销毁它，所以为了避免冗余代码，使用它们是最佳选择。 还可以使用 jasmine.any 方法来代表一类数据传入matcher中，比如 12expect(123).toEqual(jasmine.any(Number));expect(function () &#123;&#125;).toEqual(jasmine.any(Function)); Spy方法一个Spy能监测任何function的调用和获取其参数。这里有2个特殊的Matcher， toHaveBeenCalled 可以检查function是否被调用过，还有 toHaveBeenCalledWith 可以传入参数检查是否和这些参数一起被调用过，像这样使用 spyOn 来注册一个对象中的方法： 1234567891011121314151617var foo, a = null;beforeEach(function () &#123; var foo = &#123; set: function (str) &#123; a = str; &#125; &#125; spyOn(foo, 'set'); foo.set(123);&#125;);it('tracks calls', function () &#123; expect(foo.set).toHaveBeenCalled(); expect(foo.set).toHaveBeenCalled(123); expect(foo.set.calls[0].args[0]).toEqual(123); expect(foo.set.mostRecentCall.args[0]).toEqual(123); expect(a).toBeNull();&#125;); 在测试时该function将带有一个被调用的数组 calls ，而 args 数组就是调用时传入的参数，另外特殊属性 mostRencentCall 则代表最后一次调用，和calls[calls.length]一致。需要特别注意的是，这些调用将不会对变量产生作用，所以 a 仍为null。 如果需要调用产生实际的作用，可以在spyOn方法后调用 andCallThrough 方法。还可以通过调用 andReturn 方法设定一个返回值给function。 andCallFake 则可以传入一个function作为参数去代替原本的function。 1spyOn(foo, 'set').andCallThrough(); 甚至在没有function的时候可以使用Jasmine的 createSpy 和 createSpyObj 创建一个spy： 123456foo = jasmine.createSpy('foo');obj = jasmine.createSpyObj('obj', [set, do]);foo(123);obj.set(123);obj.do(); 其效果相当于spyOn使用在了已存在的function上。 时间控制上面的方法都在程序顺序执行的前提下执行，但 setTimeout 以及 setInterval 两个方法会使代码分离在时间轴上。所以Jasmine提供了 Clock 方法来模拟时间，以获取setTimeout的不同状态。 1234567891011121314beforeEach(function () &#123; jasmine.Clock.useMock();&#125;);it(&apos;set time&apos;, function () &#123; var str = 0; setTimeout(function () &#123; str++; &#125;, 100); expect(str).toEqual(0); jasmine.Click.tick(101); expect(str).toEqual(1); jasmine.Click.tick(200); expect(str).toEqual(3);&#125;); 使用Clock的方法 useMock 来开始时间控制，然后在it中使用 tick 方法来推进时间。 异步Javascript最大的特色之一就是异步，之前介绍的方法如果存在异步调用，大部分测试时可能会不通过。因此，需要等异步回调之后再进行测试。 Jasmine提供了 runs 和 waitsFor 两个方法来完成这个异步的等待。需要将waitsFor方法夹在多个runs方法中，runs方法中的语句会按顺序直接执行，然后进入waitsFor方法，如果waitsFor返回false，则继续执行waitsFor，直到返回true才执行后面的runs方法。 0; }); runs(function () { expect(cb).toBeTruthy(); });});&quot; title=&quot;&quot; data-original-title=&quot;复制&quot;&gt; 123456789101112131415161718var cb = false;var ajax = &#123; success: function () &#123; cb = true; &#125;&#125;;spyOn(ajax, &apos;success&apos;);it(&apos;async callback&apos;, function () &#123; runs(function () &#123; _toAjax(ajax); &#125;); waitsFor(function () &#123; return ajax.success.callCount &gt; 0; &#125;); runs(function () &#123; expect(cb).toBeTruthy(); &#125;);&#125;); 如此，只要在waitsFor中判断回调函数是否被调用了即可完成异步测试。上面代码中我使用一个方法名直接代替了ajax请求方法来缩减不必要的代码。在第一个runs方法中发出了一个ajax请求，然后在waitsFor中等待其被调用，当第二个runs执行时说明回调函数已经被调用了，进行测试。 Qunit它是由jQuery团队开发的一款测试套件，最初依赖于jQuery库，在2009年时脱离jQuery的依赖，变成了一个真正的测试框架，适用于所有Javascript代码。 Qunit采用断言（Assert）来进行测试，相比于Jasmine的matcher更加多的类型，Qunit更集中在测试的度上。 deepEqual 用于比较一些纵向数据，比如Object或者Function等。而最常用的 ok 则直接判断是否为true。异步方面Qunit也很有趣，通过 stop 来停止测试等待异步返回，然后使用 start 继续测试，这要比Jasmine的过程化的等待更自由一些，不过有时也许会更难写一些。Qunit还拥有3组AOP的方法( done 和 ‘begin’ )来对应于整个测试，测试和模块。 对于Function的跟踪测试，Qunit似乎完全没有考虑。不过可以使用另外一个测试框架为Qunit带来的插件 sinon-qunit。这样就可以在test中使用 spy 方法了。 SinonSinon并不是一个典型的单元测试框架，更像一个库，最主要的是对Function的测试，包括 Spy 和 Stub 两个部分，Spy用于侦测Function，而Stub更像是一个Spy的插件或者助手，在Function调用前后做一些特殊的处理，比如修改配置或者回调。它正好极大的弥补了Qunit的不足，所以通常会使用Qunit+Sinon来进行单元测试。 值得一提的是，Sinon的作者Christian Johansen就是 Test-Driven JavaScript Development 一书的作者，这本书针对Javascript很详细的描述了单元测试的每个环节。 Mocha它的作者就是在Github上粉丝6K的超级Jser TJ Holowaychuk，可以在他的页面上看到过去一年的提交量是5700多，拥有300多个项目，无论是谁都难以想象他是如何进行coding的。 理所当然的，Mocha充满了Geek感，不但可以在bash中进行测试，而且还拥有一整套命令对测试进行操作。甚至使用 diff 可以查看当前测试与上一次成功测试的代码不一致。 不仅仅是这样，Mocha非常得自由。Mocha将更多的方法集中在了describe和it中，比如异步的测试就非常棒，在it的回调函数中会获取一个参数 done ，类型是function，用于异步回调，当执行这个函数时就会继续测试。还可以使用 only 和 skip 去选择测试时需要的部分。Mocha的接口也一样自由，除了 BDD 风格和Jasmine类似的接口，还有 TDD 风格的 （suite test setup teardown suiteSetup suiteTeardown），还有AMD风格的 exports，Qunit风格等。同时测试报告也可以任意组织，无论是列表、进度条、还是飞机跑道这样奇特的样式都可以在bash中显示。 前端测试工具Client/Server 测试相比于服务端开发，前端开发在测试方面始终面临着一个严峻的问题，那就是浏览器兼容性。Paul Irish曾发表文章Browser Market Pollution: IE[x] Is the New IE6阐述了一个奇怪的设想，未来你可能需要在76个浏览器上开发，因为每次IE的新版本都是一个特别的浏览器，而且还有它对之前所有版本的兼容模式也是一样。虽然没人认为微软会继续如此愚蠢，不过这也说明了一个问题，前端开发中浏览器兼容性是一个永远的问题，而且我认为即使解决了浏览器的兼容性问题，未来在移动开发方面，设备兼容性也是一个问题。 所以在自动化测试方面也是如此，即使所有的单元测试集中在了一个runner中，前端测试仍然要面对至少4个浏览器内核以及3个电脑操作系统加2个或更多移动操作系统，何况还有令移动开发人员头疼的Android的碎片化问题。不过可以安心的是，早已存在这样的工具可以捕获不同设备上的不同浏览器，并使之随时更新测试结果，甚至可以在一个终端上看到所有结果。 工具介绍JSTD（Javascript Test Driver）是一个最早的C/S测试工具，来自Google，基于JAVA编写，跨平台，使用命令行控制，还有很好的编辑器支持，最常用于eclipse。不过它无法显示测试对象的设备及浏览器版本，只有浏览器名是不够的。另外JSTD已经慢慢不再活跃，它的早正如它的老。 Google的新贵Karma出现了，它使用Nodejs构建，因此跨平台，还支持PhantomJS浏览器，还支持多种框架，包括以上介绍的Jasmine、Qunit和Mocha。一次可以在多个浏览器及设备中进行测试，并控制浏览器行为和测试报告。虽然它不支持Nodejs的测试，不过没什么影响，因为Nodejs并不依赖于浏览器。 还有TestSwarm，出自jQuery之父John Resig之手，看来jQuery的强大果然不是偶然的，在测试方面非常到位，各种工具齐全。它最特别的地方在于所有测试环境由服务器提供，包括各种版本的主流浏览器以及iOS5的iphone设备，不过目前加入已经受限。 最受瞩目的当属Buster，其作者之一就是Christian Johansen。和Karma很像，也使用Nodejs编写跨平台并且支持PhantomJS，一次测试所有客户端。更重要的是支持Nodejs的测试，同样支持各种主流测试框架。不过目前还在Beta测试中，很多bug而且不能很好的兼容Windows系统。它的目标还包括整合Browser Stack。 基于网页的测试到目前为止我们的测试看起来十分的完美了，但是别忘了，在前端开发中存在交互问题，不能期待QA玩了命的点击某个按钮或者刷新一个页面并输入一句乱码之类的东西来测试代码。即使是开发者本身也会受不了，如果产品本身拥有一堆复杂的表单和逻辑的话。 Selenium是一个测试工具集，由Thoughtworks开发，分为两部分。Selenium IDE是一个Firefox浏览器的插件，可以录制用户行为，并快速测试。 而Selenium WebDriver是一个多语言的驱动浏览器的工具，支持Python、Java、Ruby、Perl、PHP或.Net。并且可以操作IE、Firefox、Safari和Chrome等主流浏览器。通过 open , type , click , waitForxxx 等指令来模拟用户行为，比如用Java测试： 1234567public void testNew() throws Exception &#123; selenium.open(&quot;/&quot;); selenium.type(&quot;q&quot;, &quot;selenium rc&quot;); selenium.click(&quot;btnG&quot;); selenium.waitForPageToLoad(&quot;30000&quot;); assertTrue(selenium.isTextPresent(&quot;Results * for selenium rc&quot;));&#125; 首先跳转到跟目录，然后选择类型，点击按钮G，并等待页面载入30秒，然后使用断言测试。这样就完成了一次用户基本行为的模拟，不过复杂的模拟以及在一些非链接的操作还需要格外注意，比如Ajax请求或者Pjax的无刷新等等。 另外还有一款可以模拟用户行为的网页测试工具WATIR，是Web Application Testing in Ruby的缩写，显然它只支持Ruby语言来操作浏览器模拟用户行为。官方声称它是一个简单而灵活的工具，无论怎样至少就官方网站的设计来看要比Selenium简约多了。同样支持模拟链接点击，按钮点击，还有表单的填写等行为。不过WATIR不支持Ajax的测试。和其他Ruby库一样需要gem来安装它： 1gem install watir --no-rdoc --no-ri 然后使用它 &apos;entry.0.single&apos;).set &apos;Watir&apos;browser.radio(:value =&gt; &apos;Watir&apos;).setbrowser.radio(:value =&gt; &apos;Watir&apos;).clearbrowser.checkbox(:value =&gt; &apos;Ruby&apos;).setbrowser.checkbox(:value =&gt; &apos;Javascript&apos;).clearbrowser.button(:name =&gt; &apos;submit&apos;).click&quot; title=&quot;&quot; data-original-title=&quot;复制&quot;&gt; 1234567891011require &apos;rubygems&apos;require &apos;watir&apos;require &apos;watir-webdriver&apos;browser = Watir::Browser.newbrowser.goto &apos;http://www.example.com/form&apos;browser.test_field(:name =&gt; &apos;entry.0.single&apos;).set &apos;Watir&apos;browser.radio(:value =&gt; &apos;Watir&apos;).setbrowser.radio(:value =&gt; &apos;Watir&apos;).clearbrowser.checkbox(:value =&gt; &apos;Ruby&apos;).setbrowser.checkbox(:value =&gt; &apos;Javascript&apos;).clearbrowser.button(:name =&gt; &apos;submit&apos;).click 这样就使用watir完成了一次表单填写。 持续集成测试持续集成就是通常所谓的CI(Continuous integration)，持续不断的自动化测试新加入代码后的项目。它并不属于单元测试，而是另外的范畴，不过通过使用CI服务可以很容易的在Github上测试项目，而这也就是持续集成的意义。 下面以我的jQ小插件Dialog为例介绍一下Travis-CI的使用方法，注册Travis，然后链接自己的Github，选择要进行持续集成的项目。此时会显示build failing，那是因为还没有在项目中进行相关配置。 首先需要使用Grunt等工具配置好测试框架的自动化测试，细节可以参考我之前的文章改进我的Workflow。然后在 package.json 中添加一下代码来指定执行的脚本： 123&quot;scripts&quot;: &#123; &quot;test&quot;: &quot;grunt jasmine:test&quot;&#125; 接着添加一个文件 .travis.yml 来配置travis： 12345language: node_jsnode_js: - &quot;0.8&quot;before_script: - npm install -g grunt-cli language 是集成测试所使用的语言，这里前端开发当然是使用Nodejs，在 node_js 中指定版本即可。当然Travis还支持其他多种语言，以及后端数据库等。 before_script 则是在测试前执行的脚本程序，这里在全局安装Grunt-cli即可，因为默认的Travis会执行 npm install 将package.json中指定的Node包安装到项目。 最后在Github中还需要在项目的Setting中的Service Hooks中配置Travis，输入Token并保存。或者直接在Travis中点击该项目条目中的扳手图标进入Github，会自动配置好。 另外，如果在Github上为README文件添加一行 1[![Build Status](https://travis-ci.org/tychio/dialog.png?branch=master)](https://travis-ci.org/tychio/dialog) 就可以持续直观的显示其测试结果。 博客原文: http://www.tychio.net/tech/2013/07/10/unit-test.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[读《富爸爸，穷爸爸》的一些收获]]></title>
    <url>%2F2018%2F09%2F19%2F%E9%9A%8F%E7%AC%94%2F%E5%AF%8C%E7%88%B8%E7%88%B8%EF%BC%8C%E7%A9%B7%E7%88%B8%E7%88%B8%E9%9A%8F%E7%AC%94%2F</url>
    <content type="text"><![CDATA[资产与负债： 理解现金流的的流通。这里最重要的是要分清什么是资产（为自己能够带来现金的东西，比如：股票、债券、专利的版税、出版的书等），什么是负债（不断地将现金流投入的东西，比如：消费贷款、信用卡、抵押贷款等） 富人关心自己的资产项，而不是收入项。从长远来看，重要的不是你挣了多少钱，而是你能留下多少钱，以及能够留多久。 事业和职业： 麦当劳的创始人的职业是买汉堡，事业是买能够产生收入的房地产。你可能在银行工作，这是你的职业，但你不拥有这家银行，所以它不是你事业。你的事业的重心是你的资产项，不是收入项。我们可以做个区分，把能够增加资产项的工作叫做事业，把单纯增加收入项的工作叫做职业。职业和事业要分隔开，在起初要注重自己的职业，同时要发展和培养自己的事业。 关于安稳： 如果你是这种人，你的一生会过得稳稳当当，不做错事、假想着有事情发生时自救，然后慢慢变老，在无聊中死去。你会有许多朋友，他们很喜欢你，因为你真的是一个努力工作的好人。你的一生过得很安稳，处世无误。但事实是，你向生活屈服了，不敢承担风险。你的确想赢，但失败的恐惧超过了成功的兴奋。只有你知道，在你内心深处，你始终认为你不可能赢，所以选择了安稳。 关于恐惧： 正是出于恐惧的心理，人们才想找一份安稳的工作。这些恐惧有：害怕付不起账单，害怕被解雇，害怕没有足够的钱，害怕重新开始。为了寻求保障，他们会学习某种专业，或者做生意，拼命为钱而工作。大多数人成了钱的奴隶，然后就把怒气发泄在他们的老板身上。 所以我们应该做的是，寻找并投资资产，不断提高资产的营收能力 关于处理财务 懂得财务报表是投资理财的首要基础，从财务报表上的数字，看到的信息而不是简单地数字 ，。以诚实的态度对待自己的财务问题，认清自己的现金流，区分自己的资产和负债；避免自己在年轻是背有负债，如果有负债，将会束缚自己的在理财方面探索，是自己容易进入“老鼠赛跑的局面” 关于财商学习 财商的教育需要4个方面专门知识来建立：会计（财务报表的信息）、投资（钱赚钱、做真正的投资者，把多的鸡蛋放在一个篮子里，而不是普遍的认为那样——把鸡蛋房子多个篮子里）、了解市场（投资到自己熟悉的领域）、法律（法律的变动会间接的影响社会大的变动，段时间可能不会出现明显的结果，长期的一定有影响）。 关于沟通能力 沟通能力对一个人的发展尤其重要，包括书面表达能力，口头表达能力，可以利用销售来培养自己。 关于立即执行 过于忙于工作而不关心自己的财富的人，过分忙于工作而不照顾自己身体，总是把忙碌作为逃避问题的借口，其实，他们心理都很明白，只是自己不愿接受而已。]]></content>
      <tags>
        <tag>随笔</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你需要了解的前端测试“金字塔”]]></title>
    <url>%2F2018%2F09%2F19%2F%E5%89%8D%E7%AB%AF%2F%E5%89%8D%E7%AB%AF%E6%B5%8B%E8%AF%95%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%2F</url>
    <content type="text"><![CDATA[如果您正在测试前端应用程序，则应该了解前端测试金字塔。 在本文中，我们将看到前端测试金字塔是什么，以及如何使用它来创建全面的测试套件。 前端测试金字塔 前端测试金字塔是一个前端测试套件应该如何构建的结构化表示。 理想的测试套件由单元测试，一些快照测试和一些端到端（e2e）测试组成。 这是测试金字塔的改进版本，特定于测试前端应用程序。 在这篇文章中，我们将看到每个测试类型的样子。 为此，我们将为示例应用程序创建一个测试套件。 应用 要详细了解前端测试金字塔，我们来看看如何测试一个 Web 应用。 该应用是一个简单的 modal 应用。 点击一个按钮打开一个 modal ，点击 modal 上的 OK 按钮关闭 modal。 我们将从基于组件的框架构建应用。 别担心细节，我们会保持这个（详细）的级别。 该应用由三个组件组成 – 一个 Button 组件，一个 Modal 组件和一个 App 组件。 我们要写的第一个测试是单元测试。 在前端测试金字塔中，大部分测试都是单元测试。 单元测试单元测试测试的是代码库的单元。 它们直接调用函数或单元，并确保返回正确的结果。 在我们的应用中，我们的组件是单元。所以我们将为 Button&nbsp;和 Modal 编写单元测试。没有必要为我们的应用组件编写测试，因为它没有任何逻辑。 单元测试会浅渲染组件，并断言当我们与它们交互时，它们的行为是正确的。 浅渲染意味着我们渲染组件一层深度。这样我们可以确保只测试组件，单元，而不是几个级别的子组件。 在我们的测试中，我们将触发组件上的操作，并检查组件的行为是否与预期一致。 我们不用盯着代码。但是我们的组件规格会如下所示： 当 displayModal 为 true 时，Modal 有类是活跃的 当 displayModal 为 false 时，Modal 没有类是活跃的 当成功按钮被点击时，Modal 调用 toggleModal 单击删除按钮时，Modal 会调用 toggleModal 当 button 被点击时，button 调用 toggleModal 我们的测试将浅渲染组件，然后检查每一项规格的工作。 单元测试应该占据我们的测试套件的绝大部分有以下几个原因： 单元测试很快几百个单元测试套件能在几秒钟内运行。 这使得单元测试对开发很有用。 当重构代码时，我们可以更改代码，并在没有中断组件的情况下运行单元测试来检查更改。 我们会在几秒钟之内知道我们是否破坏了代码，因为其中一个测试会失败。 单元测试是细颗粒的换句话说，他们是非常具体的。 如果一个单元测试失败了，那么这个测试会告诉我们它是如何以及为什么失败的。 单元测试能很好地检查我们的应用程序工作的细节。 它们是开发时最好的工具，特别是如果你遵循测试驱动的开发。 但是它们无法测试一切。 为了确保我们呈现正确的样式，我们还需要使用快照测试。 快照测试快照测试是测试你的渲染组件的图片，并将其与组件的以前的图片进行比较。 用 JavaScript 编写快照测试的最好方法是使用&nbsp;Jest&nbsp;。 Jest 不是拍摄渲染组件的图片，而是渲染组件标记的快照。 这使得 Jest 快照测试比传统快照测试快得多。 要在 Jest 中注册快照测试，需要添加如下代码： const&nbsp;renderedMarkup&nbsp;=&nbsp;renderToString(ModalComponent)expect(renderedMarkup).toMatchSnapshot()|12|const&nbsp;renderedMarkup&nbsp;=&nbsp;renderToString(ModalComponent)expect(renderedMarkup).toMatchSnapshot()| 一旦你注册一个快照，Jest 将顾及其它的一切。 每次运行单元测试时，都会重新生成一个快照，并将其与之前的快照进行比较。 如果代码改变，Jest 会抛出一个错误，并警告标记已经改变。 然后开发者可以手动检查没有类被误删的情况。 在下面的测试中，有人从中删除了 modal-card-foot 类。 快照测试是一种检查组件样式或标记的方法。 如果快照测试通过，我们知道代码更改不会影响组件的显示。 如果测试失败，那么我们知道确实影响了组件的渲染，并可以手动检查样式是否正确。 每个组件至少应有一次快照测试。 一个典型的快照测试呈现组件的状态，以检查它正确呈现。 现在我们已经有了单元测试和快照测试，是时候看看端到端（e2e）测试。 端到端测试端到端（e2e）测试是高层测试。 它们执行与我们手动测试应用程序时相同的操作。 在我们的应用程序中，我们有一个用户（操作）旅程。当用户点击按钮时，模式将打开，当他们点击模式中的按钮时，模式将关闭。 我们可以编写一个贯穿这一旅程的端到端测试。测试将打开浏览器，导航到网页，并通过每个操作来确保应用程序正常运行。 这些测试将告诉我们，我们的单元正确地协同工作。它使我们高度自信，该应用程序的主要功能是可以正常工作的。 对 JavaScript 应用程序来说有几种方法可以编写端到端测试。像 test cafe 这样的程序会记录您在浏览器中执行操作并将其作为测试源重播。 还有类似 nightwatch 的项目，可让你用 JavaScript 编写测试项目。我会推荐使用类似 nightwatch 的库。拿起来直接用很容易，该测试运行速度比记录的测试更快。 也就是说，night1qtch 的测试还是比较慢的。一套200个单元测试需要花费几分钟的时间，一套200个端到端测试仅需要几分钟时间来运行。 端到端测试的另一个问题是难以调试。当测试失败时，很难找出失败的原因，因为测试涵盖了太多功能。 结语要有效地测试基于前端组件的 Web 应用程序，你需要三种类型的测试：单元测试，快照测试和 e2e 测试。 你应该对每个组件进行多个单元测试，对每个组件进行一次或两次快照测试，以及测试链接在一起的多个组件的一次或两次端到端测试。 整体单元测试将涵盖大部分测试，你将有一些快照测试和一些 e2e 测试。 如果你遵循前端测试金字塔，你就可以使用杀手级测试套件创建可维护的 Web 应用程序。 你可以在GitHub&nbsp;上看到应用程序的快照测试、单元测试和端到端测试的示例源码库。 参考链接你需要了解的前端测试“金字塔”]]></content>
  </entry>
  <entry>
    <title><![CDATA[PyQt5学习教程13 进度提示条]]></title>
    <url>%2F2018%2F09%2F18%2Fpyqt5%2FPyQt5%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B13-%E8%BF%9B%E5%BA%A6%E6%8F%90%E7%A4%BA%E6%9D%A1%2F</url>
    <content type="text"><![CDATA[在日常处理事务的过程，由于过程漫长，需要等待一会，这时一般软件都会给予一定的提示，我们可以使用进度条来给出友好的提示 基本示例进度条的基本流程是： 实例化一个进度条提示窗口progress = QProgressDialog(self) 使用progress.setRange(0,max_value)得到进度条的取值范围 使用progress.setValue(value)设置当前的进度值 使用value/max_value得到百分比，就是进度条的百分百 当value == max_value时，进度百分之百，表示进程操作完成 具体做法是:工作线程通过sig.emit(int)发送值，ui线程接收到信号后，执行相应的槽函数设置进度 示例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from PyQt5.QtWidgets import (QApplication, QWidget, QPushButton, QLabel, QLineEdit, QMessageBox, QProgressDialog)from PyQt5.QtCore import Qtimport sysclass Example(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): self.resize(300,150) self.setWindowTitle("微信公众号：学点编程吧--进度对话框") self.lb = QLabel("文件数量",self) self.lb.move(20,40) self.bt1 = QPushButton('开始',self) self.bt1.move(20,80) self.edit = QLineEdit('1000000',self) self.edit.move(100,40) self.show() self.bt1.clicked.connect(self.showDialog) def showDialog(self): num = int(self.edit.text()) # 实例化一个QProgressDialog progress = QProgressDialog(self) progress.setWindowTitle("请稍等") progress.setLabelText("正在操作...") progress.setCancelButtonText("取消") # 如果任务的预期持续时间小于minimumDuration，则对话框根本不会出现。这样可以防止弹出对话框，快速完成任务。 progress.setMinimumDuration(5) # 当progress窗口显示时，阻止用户对所有父级窗口的输入（默认情况为不阻止） progress.setWindowModality(Qt.WindowModal) # setRange(0,num)就是设置其进度的最小和最大值，这里最小值0，最大值num，分别对应0%和100% progress.setRange(0,num) for i in range(num): # 通过调用setValue得到的 值/最大值 得到的百分比显示进度 progress.setValue(i) # 用户点击取消会提示操作失败 if progress.wasCanceled(): QMessageBox.warning(self,"提示","操作失败") break # 完全完成后，提示操作成功 else: progress.setValue(num) QMessageBox.information(self,"提示","操作成功")if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_()) 实际应用首先定义一个process_value信号: 12345678from PyQt5.QtCore import QObject,pyqtSignalclass Sig(QObject): # 进度条提示 process_value = pyqtSignal(int)sig = Sig() 工作线程:发送process_value信号，参数为int： 123for i in range(100): time.sleep(0.1) self.sig.process_value.emit(i) ui线程:接收到process_value信号时，调用self.process.setValue方法设置当前进度数值 1234def showDialog(self): self.progress = QProgressDialog(self) ... self.sig.process_value.connect(self.process.setValue) 参考文档PyQt5系列教程（13）：进度对话框]]></content>
  </entry>
  <entry>
    <title><![CDATA[PyQt5学习教程12 单选按钮]]></title>
    <url>%2F2018%2F09%2F18%2Fpyqt5%2FPyQt5%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B12-%E5%8D%95%E9%80%89%E6%8C%89%E9%92%AE%2F</url>
    <content type="text"><![CDATA[QRadioButton简介QRadioButton小部件提供了一个带有文本标签的单选按钮（没错，本质上它还是一个按钮）。 QRadioButton是一个选项按钮，可以打开（选中）或关闭（取消选中）。单选按钮通常为用户提供“多选一”操作。在一组单选按钮中，一次只能检查一个单选按钮;如果用户选择另一个按钮，则先前选择的按钮被关闭。 单选按钮默认为autoExclusive（自动互斥）。如果启用了自动互斥功能，则属于同一个父窗口小部件的单选按钮的行为就属于同一个互斥按钮组的一部分。当然加入QButtonGroup中能够实现多组单选按钮互斥。 无论何时打开或关闭按钮，都会发出toggled()信号。如果要在每次按钮更改状态时触发某个操作，请连接到此信号。使用isChecked()来查看是否选择了一个特定的按钮。 就像QPushButton一样，单选按钮显示文本，还可以选择一个小图标。该图标是用setIcon()设置的。文本可以在构造函数中设置，也可以在setText()中设置。快捷键可以通过在文本前面加一个＆符号来指定。 基本示例12345678910111213141516171819202122232425262728293031323334353637383940414243from PyQt5 import QtWidgetsfrom PyQt5.QtCore import *from PyQt5.QtWidgets import *import sysclass ExampleWindow(QMainWindow): def __init__(self): QMainWindow.__init__(self) self.setMinimumSize(QSize(230, 40)) self.setWindowTitle("Checkbox") # 1. 实例化raidobutton self.raido_yes = QRadioButton("是",self) self.raido_no = QRadioButton("否",self) # 2. 实例化QButtonGroup self.buttongroup = QButtonGroup(self) self.buttongroup.addButton(self.raido_yes,1) self.buttongroup.addButton(self.raido_no,2) # 3. 连接槽函数 self.buttongroup.buttonClicked.connect(self.bgclicked) # 4. 调整按钮位置 self.raido_yes.move(30,30) self.raido_no.move(80,30) def bgclicked(self): # 通过checkedId的方式判断某个单选按钮是否被勾选 sender = self.sender() if sender == self.buttongroup: if self.buttongroup.checkedId() == 1: print('checkedId:是') elif self.buttongroup.checkedId() == 2: print('checkedId:否') # 通过isChecked的方式判断某个单选按钮是否被勾选 if self.raido_yes.isChecked(): print('isChecked:是') elif self.raido_no.isChecked(): print('isChecked:否')if __name__ == "__main__": app = QtWidgets.QApplication(sys.argv) mainWin = ExampleWindow() mainWin.show() sys.exit( app.exec_() ) 常用语法1.实例化一个单选按钮 12self.radiobutton1 = QRadioButton('单选按钮1',self)self.radiobutton2 = QRadioButton('单选按钮2',self) 2.实例化一个按钮组 其中addButton接受两个参数，第一个参数是添加入按钮组的按钮，第二个参数是分配给该按钮的id 12self.buttongroup1 = QButtonGroup(self)self.buttongroup1.addButton(self.radiobutton1,11) 3.为按钮组连接槽函数 12self.buttongroup1.buttonClicked.connect(self.bgclicked)self.buttongroup2.buttonClicked.connect(self.bgclicked) 4.得知是那个按钮组被点击 123456def bgclicked(self): sender = self.sender() if sender == self.buttongroup1: ... elif sender == self.buttongroup1: ... 5.得知是哪个按钮被点击 123def bgclicked(self): if self.buttongroup1.checkedId() == 11: ...]]></content>
  </entry>
  <entry>
    <title><![CDATA[PyQt5学习教程11 checkbox]]></title>
    <url>%2F2018%2F09%2F17%2Fpyqt5%2FPyQt5%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B11-checkbox%2F</url>
    <content type="text"><![CDATA[可以使用QCheckBox小部件创建一个复选框。使用QCheckBox类创建新复选框时，第一个参数是label。 要将操作应用于切换开关，我们调用.stateChanged.connect()，然后调用回调方法。调用此方法时，它会将一个boolean值作为state的参数发送。如果选中，则其值为QtCore.Qt.checked 基本示例123456789101112131415161718192021222324252627282930import sysfrom PyQt5 import QtCore, QtWidgetsfrom PyQt5.QtWidgets import QMainWindow, QLabel, QCheckBox, QWidgetfrom PyQt5.QtCore import QSize class ExampleWindow(QMainWindow): def __init__(self): QMainWindow.__init__(self) self.setMinimumSize(QSize(140, 40)) self.setWindowTitle("Checkbox") # 1. 实例化一个QCheckBox self.b = QCheckBox("Awesome?",self) # 2. 当状态改变时，调用self.clickBox方法 self.b.stateChanged.connect(self.clickBox) self.b.move(20,20) self.b.resize(320,40) def clickBox(self, state): if state == QtCore.Qt.Checked: print('Checked') else: print('Unchecked')if __name__ == "__main__": app = QtWidgets.QApplication(sys.argv) mainWin = ExampleWindow() mainWin.show() sys.exit( app.exec_() ) 常用方法1.实例化一个QCheckBox： 1self.checkbox = QCheckBox("Awesome?",self) 2.当复选框的被选中或取消选中时，连接槽函数 1self.checkbox.stateChanged.connect(self.clickBox) 3. 判断复选框是否被选中 通过槽函数接受stateChanged信号emit的参数state判断,该方法比isCechked判断多了一种状态，半选状态。半选状态表现为，√ 显示为灰色 12345678def clickBox(self, state): if state == QtCore.Qt.Checked: print('Checked') elif state == QtCore.Qt.UnChecked: print('Unchecked') elif state == QtCore.Qt.PartiallyChecked: print('PartiallyChecked') 通QCheckBox的实例方法isChecked()判断，返回一个布尔值 123456def clickBox(self): if self.checkbox.isChecked() == True: print('Checked') else: print('Unchecked') 4. 设置复选框的选中状态 通过QCheckBox的实例方法setCheckState()设置状态 123self.checkbox.setCheckState(Qt.Checked) # 勾选复选框self.checkbox.setCheckState(Qt.PartiallyChecked) # 半勾选复选框self.checkbox.setCheckState(Qt.Unchecked) # # 取消勾选复选框 通过QCheckBox的实例方法setChecked()设置状态 12self.checkbox.setChecked(True) # 勾选复选框self.checkbox.setChecked(False) # 取消勾选复选框]]></content>
  </entry>
  <entry>
    <title><![CDATA[PyQt5学习教程9 textarea]]></title>
    <url>%2F2018%2F09%2F17%2Fpyqt5%2FPyQt5%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B9-textarea%2F</url>
    <content type="text"><![CDATA[QPlainTextEdit是PyQt中的多行文本区域。要设置文本，我们使用其方法insertPlainText()。我们可以使用方法move()和resize()来设置它的位置和大小。 Textarea下面的示例使用PyQt5创建文本区域。我们将创建通常的QMainWindow来添加小部件。它只适用于纯文本，如记事本。要添加新行，我们添加\n字符。 12345678910111213141516171819202122232425import sysfrom PyQt5.Qt import QApplication, QClipboardfrom PyQt5 import QtCore, QtWidgetsfrom PyQt5.QtWidgets import QMainWindow, QWidget, QPlainTextEditfrom PyQt5.QtCore import QSizeclass ExampleWindow(QMainWindow): def __init__(self): QMainWindow.__init__(self) self.setMinimumSize(QSize(440, 240)) self.setWindowTitle("PyQt5 Textarea example") # 实例化一个QPlainTextEdit self.b = QPlainTextEdit(self) self.b.insertPlainText("You can write text here.\n") self.b.move(10,10) self.b.resize(400,200)if __name__ == "__main__": app = QtWidgets.QApplication(sys.argv) mainWin = ExampleWindow() mainWin.show() sys.exit( app.exec_() )]]></content>
  </entry>
  <entry>
    <title><![CDATA[PyQt5学习教程8 groupbox]]></title>
    <url>%2F2018%2F09%2F17%2Fpyqt5%2FPyQt5%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B8-groupbox%2F</url>
    <content type="text"><![CDATA[您可以将widgets放在grip中。grip可以包含groupbox，其中每个groupbox都有一个或多个widgets。groupbox的作用是，我们可以现在groupbox中设置好布局，然后方便的在window中自由组合groupbox 示例可以使用PyQt创建gropbox和grip。它的工作原理如下： PyQt5窗口可以包含grip grip可以包含任意数量的gropbox gropbox可以包含小部件（按钮，文本，图像）以及适合的布局 可以使用类QGridLayout创建网格。正如您所想象的那样，必须将网格布局添加到窗口中。可以使用类QGroupBox创建groupbox。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import sysfrom PyQt5.QtCore import Qtfrom PyQt5.QtWidgets import (QApplication, QCheckBox, QGridLayout, QGroupBox, QMenu, QPushButton, QRadioButton, QVBoxLayout, QWidget)class Window(QWidget): def __init__(self, parent=None): # 1. 继承QWidget生成窗口 super(Window, self).__init__(parent) # 2.创建一个layout，将gropBox添加到layout中 grid = QGridLayout() grid.addWidget(self.createExampleGroup(), 0, 0) grid.addWidget(self.createExampleGroup(), 1, 0) grid.addWidget(self.createExampleGroup(), 0, 1) grid.addWidget(self.createExampleGroup(), 1, 1) # 3. 应用之前设置好的layout self.setLayout(grid) self.setWindowTitle("PyQt5 Group Box") self.resize(400, 300) def createExampleGroup(self): # 1.生成实例 groupBox =》 对应窗口 groupBox = QGroupBox("Best Food") # 2.创建一些小部件，比如button或者raido radio1 = QRadioButton("Radio pizza") radio2 = QRadioButton("Radio taco") radio3 = QRadioButton("Radio burrito") radio1.setChecked(True) # 3.创建一个layout，将小部件添加到layout中 vbox = QVBoxLayout() vbox.addWidget(radio1) vbox.addWidget(radio2) vbox.addWidget(radio3) vbox.addStretch(1) # 4. 应用之前设置好的layout groupBox.setLayout(vbox) return groupBoxif __name__ == '__main__': app = QApplication(sys.argv) clock = Window() clock.show() sys.exit(app.exec_())]]></content>
  </entry>
  <entry>
    <title><![CDATA[PyQt5学习教程7 menu]]></title>
    <url>%2F2018%2F09%2F17%2Fpyqt5%2FPyQt5%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B7-menu%2F</url>
    <content type="text"><![CDATA[你想要PyQt应用程序中的菜单吗？Pyqt有菜单支持。几乎每个GUI应用程序都在窗口顶部有一个主菜单。添加菜单与添加小部件略有不同。 菜单可以包含子菜单，它们通常类似于（文件，编辑，视图，历史记录，帮助）。每个菜单都有相依的动作。 完整示例: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import sysfrom PyQt5 import QtCore, QtWidgetsfrom PyQt5.QtWidgets import QMainWindow, QWidget, QPushButton, QActionfrom PyQt5.QtCore import QSizefrom PyQt5.QtGui import QIconclass MainWindow(QMainWindow): def __init__(self): QMainWindow.__init__(self) self.setMinimumSize(QSize(300, 100)) self.setWindowTitle("PyQt menu example - pythonprogramminglanguage.com") # 1.1 创建menuBar menuBar = self.menuBar() # 1.2 定义一个menu，并将该meun添加到menuBar fileMenu = menuBar.addMenu('&amp;File') # 2.1 创建一个Action newAction = QAction(QIcon('new.png'), '&amp;New', self) # 2.2 为Action添加快捷键 newAction.setShortcut('Ctrl+N') # 2.3 为Action添加setStatusTip newAction.setStatusTip('New document') # 2.4 为Action绑定相应的方法 newAction.triggered.connect(self.newCall) # Create new action openAction = QAction(QIcon('open.png'), '&amp;Open', self) openAction.setShortcut('Ctrl+O') openAction.setStatusTip('Open document') openAction.triggered.connect(self.openCall) # Create exit action exitAction = QAction(QIcon('exit.png'), '&amp;Exit', self) exitAction.setShortcut('Ctrl+Q') exitAction.setStatusTip('Exit application') exitAction.triggered.connect(self.exitCall) # 3. 将创建好的Action添加到Menu fileMenu.addAction(newAction) fileMenu.addAction(openAction) fileMenu.addAction(exitAction) def openCall(self): print('Open') def newCall(self): print('New') def exitCall(self): print('Exit app') def clickMethod(self): print('PyQt')if __name__ == "__main__": app = QtWidgets.QApplication(sys.argv) mainWin = MainWindow() mainWin.show() sys.exit( app.exec_() )]]></content>
  </entry>
  <entry>
    <title><![CDATA[PyQt5学习教程6 tooltip]]></title>
    <url>%2F2018%2F09%2F17%2Fpyqt5%2FPyQt5%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B6-tooltip%2F</url>
    <content type="text"><![CDATA[tooltip是图形界面中的提示。将鼠标悬停在窗口小部件上时（不单击），经常会出现tooltip。Pyqt支持tooltip，可以为小部件配置它们。 可以使用小部件setTooltip方法设置工具提示。 1pybutton.setToolTip('This is a tooltip for the QPushButton widget') 完整示例1234567891011121314151617181920212223242526import sysfrom PyQt5 import QtCore, QtWidgetsfrom PyQt5.QtWidgets import QMainWindow, QWidget, QPushButtonfrom PyQt5.QtCore import QSize class MainWindow(QMainWindow): def __init__(self): QMainWindow.__init__(self) self.setMinimumSize(QSize(300, 100)) self.setWindowTitle("PyQt tooltip example - pythonprogramminglanguage.com") pybutton = QPushButton('Pyqt', self) pybutton.clicked.connect(self.clickMethod) pybutton.resize(100,32) pybutton.move(50, 20) pybutton.setToolTip('This is a tooltip message.') def clickMethod(self): print('PyQt')if __name__ == "__main__": app = QtWidgets.QApplication(sys.argv) mainWin = MainWindow() mainWin.show() sys.exit( app.exec_() )]]></content>
  </entry>
  <entry>
    <title><![CDATA[PyQt5学习教程5 messagebox]]></title>
    <url>%2F2018%2F09%2F17%2Fpyqt5%2FPyQt5%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B5-messagebox%2F</url>
    <content type="text"><![CDATA[在创建Python GUI时，您可能希望在某个时刻显示消息框。Pyqt在PyQt4和PyQt5中都带有消息框支持。要使用的类是QMessageBox。在本教程中，您将学习如何在单击时显示消息框。 QMessageBox示例123456789101112131415161718192021222324252627282930from PyQt5.QtWidgets import QWidget, QApplication, QPushButton, QMessageBox, QLabel, QCheckBoxfrom PyQt5.QtGui import QPixmapimport sysclass Example(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): self.setGeometry(200, 200, 380,200) self.setWindowTitle('消息对话框') self.bt1 = QPushButton('提示',self) self.bt1.move(20,70) self.bt1.clicked.connect(self.info) self.show() def info(self): reply = QMessageBox.information(self,'提示','这是一个消息提示对话框!',QMessageBox.Ok | QMessageBox.Close, QMessageBox.Close) if reply == QMessageBox.Ok: print('你选择了Ok！') else: print('你选择了Close！')if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_()) 实例化一个消息盒子1reply = QMessageBox.information(self,'提示','这是一个消息提示对话框!',QMessageBox.Ok | QMessageBox.Close, QMessageBox.Close) 第一个参数指定信息盒子的父窗口 第二个参数指定消息盒子的标题 第三个参数指定消息盒子的文本 第四个参数指定消息盒子使用的按钮 第五个参数指定消息盒子的默认选中按钮 当然还有更多的按钮可以供我们选择，如下图： 这个函数中我们显示的按钮分别是Ok、Close，默认按钮是Close。 获取点击的按钮1234if reply == QMessageBox.Ok: print(&apos;你选择了Ok！&apos;)else: print(&apos;你选择了Close！&apos;) 自定义一个消息盒子我们可以通过设置QMessageBox属性自定义消息盒子，过程比较繁琐，但是可以灵活的修改按钮文本以及图标 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from PyQt5.QtWidgets import QWidget, QApplication, QPushButton, QMessageBox, QLabel, QCheckBoxfrom PyQt5.QtGui import QPixmapimport sysclass Example(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): self.setGeometry(200, 200, 380,200) self.setWindowTitle('消息对话框') self.bt1 = QPushButton('提示',self) self.bt1.move(20,70) self.bt1.clicked.connect(self.warning) self.show() def warning(self): checkbox = QCheckBox('所有文档都按此操作') msgBox = QMessageBox() msgBox.setWindowTitle('警告') msgBox.setIcon(QMessageBox.Warning) msgBox.setText('这是一个警告消息对话框') msgBox.setInformativeText('出现更改愿意保存吗?') Save = msgBox.addButton('保存', QMessageBox.AcceptRole) NoSave = msgBox.addButton('取消', QMessageBox.RejectRole) Cancel = msgBox.addButton('不保存', QMessageBox.DestructiveRole) msgBox.setDefaultButton(Save) msgBox.setCheckBox(checkbox) checkbox.stateChanged.connect(self.check) reply = msgBox.exec() if reply == QMessageBox.AcceptRole: print('你选择了保存！') elif reply == QMessageBox.RejectRole: print('你选择了取消！') else: print('你选择了不保存！') def check(self): if self.sender().isChecked(): print('isChecked') else: print('unChecked')if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_()) 自定义图标我们可以通过msgBox.setIconPixmap(QPixmap(&quot;icon.png&quot;))自定义提示信息的图标 123456from PyQt5.QtGui import QPixmapdef about(self): msgBox = QMessageBox(QMessageBox.NoIcon, '关于','关于的提示信息!') msgBox.setIconPixmap(QPixmap("beauty.png")) reply = msgBox.exec() ...]]></content>
  </entry>
  <entry>
    <title><![CDATA[PyQt5学习教程4 button]]></title>
    <url>%2F2018%2F09%2F17%2Fpyqt5%2FPyQt5%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B4-button%2F</url>
    <content type="text"><![CDATA[为了向窗口中添加按钮，我们需要了解如何将按钮小部件添加到现有的Pyqt窗口，以及了解如何将单击连接到Python方法。 示例123456789101112131415161718192021222324252627282930import sysfrom PyQt5 import QtCore, QtWidgetsfrom PyQt5.QtWidgets import QMainWindow, QLabel, QGridLayout, QWidgetfrom PyQt5.QtWidgets import QPushButtonfrom PyQt5.QtCore import QSize class MainWindow(QMainWindow): def __init__(self): QMainWindow.__init__(self) self.setMinimumSize(QSize(300, 200)) self.setWindowTitle("PyQt button example - pythonprogramminglanguage.com") # 1.向窗口添加Button pybutton = QPushButton('Click me', self) # 2.将点击事件与python方法连接 pybutton.clicked.connect(self.clickMethod) # 3.调整button大小 pybutton.resize(100,32) # 4.调整button位置 pybutton.move(50, 50) def clickMethod(self): print('Clicked Pyqt button.')if __name__ == "__main__": app = QtWidgets.QApplication(sys.argv) mainWin = MainWindow() mainWin.show() sys.exit( app.exec_() )]]></content>
  </entry>
  <entry>
    <title><![CDATA[PyQt5学习教程3 center window]]></title>
    <url>%2F2018%2F09%2F17%2Fpyqt5%2FPyQt5%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B3-center-window%2F</url>
    <content type="text"><![CDATA[为了使PyQt窗口居中，我们需要使用一些技巧：我们需要获取窗口属性，中心点并自己移动它。在程序开始时，它将位于屏幕的中心。 我们将QDesktopWidget添加到导入列表中，具有： 12from PyQt5.QtWidgets import QMainWindow, QLabelfrom PyQt5.QtWidgets import QGridLayout, QWidget, QDesktopWidget 完整示例： 12345678# 1. 得到窗口的几何形状： PyQt5.QtCore.QRect（0,0,640,480）qtRectangle = self.frameGeometry()# 2. 获取屏幕的中心坐标centerPoint = QDesktopWidget().availableGeometry().center()# 3. 将屏幕中心设置为几何形状中心qtRectangle.moveCenter(centerPoint)# 4. 使用move方法，将窗口左上角移动到几何形状中心左上角self.move(qtRectangle.topLeft())]]></content>
  </entry>
  <entry>
    <title><![CDATA[PyQt5学习教程2 Hello World]]></title>
    <url>%2F2018%2F09%2F17%2Fpyqt5%2FPyQt5%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B2-Hello-World%2F</url>
    <content type="text"><![CDATA[使用PyQt制作的图形界面可运行于 Microsoft Windows，Apple Mac OS X和Linux。我们将使用PyQt创建一个Hello World应用程序。 PyQt Hello World 我们编写的应用程序将在图形窗口中显示消息“Hello World”。 123456789101112131415161718192021222324252627282930import sysfrom PyQt5 import QtCore, QtWidgetsfrom PyQt5.QtWidgets import QMainWindow, QLabel, QGridLayout, QWidgetfrom PyQt5.QtCore import QSize class HelloWindow(QMainWindow): def __init__(self): super().__init__() # 1.设置窗口大小和标题 self.setMinimumSize(QSize(640, 480)) self.setWindowTitle("Hello world") # 2.为主窗口设置CentralWidget centralWidget = QWidget(self) self.setCentralWidget(centralWidget) # 3.为centralWidget设置Layout gridLayout = QGridLayout(self) centralWidget.setLayout(gridLayout) # 4.向Layout中添加Widget title = QLabel("Hello World from PyQt", self) title.setAlignment(QtCore.Qt.AlignCenter) gridLayout.addWidget(title, 0, 0)if __name__ == "__main__": app = QtWidgets.QApplication(sys.argv) mainWin = HelloWindow() mainWin.show() sys.exit( app.exec_() ) 该程序从main开始。我们初始化Qt并创建一个HelloWindow类型的对象。我们调用show()方法来显示窗口。 HelloWindow类继承自QMainWindow类。我们称其超级方法初始化窗口。 然后设置了几个类变量：大小和窗口标题。我们向窗口添加小部件，包括显示消息“Hello World”的标签小部件（QLabel）。]]></content>
  </entry>
  <entry>
    <title><![CDATA[PyQt5学习教程1 python GUI与PyQt5]]></title>
    <url>%2F2018%2F09%2F17%2Fpyqt5%2FPyQt5%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B1-python-GUI%E4%B8%8EPyQt%2F</url>
    <content type="text"><![CDATA[python GUI为了可以使用Python模块制作GUI（图形用户界面），python设计了许多模块。其中最古老的是标准图形界Tkinter。多年来，更多现代模块问世，例如：Qt和WxWidgets。 PyQtPyQT模块适用于所有桌面平台（Windows，Mac，Linux）。它具有现代外观，PyQt5是当前版本。QT包括GUI小部件，网络套接字，SQL数据库，多媒体框架和许多很酷的东西。 一个简单的PyQt应用程序。 Tkinter用于创建GUI应用程序的旧模块。它适用于Microsoft Windows和Mac OS X.此模块包含的小部件比QT少。它包含基本小部件，如按钮，文本框，列表视图和标签。如果您正在寻找高级小部件或不寻常的小部件，我推荐QT或WxPython。 wxPython这是PyQT和Tkinter的替代品。WxPython模块使用名为WxWidgets的C ++库。使用此模块，您可以为Windows，Mac OS X，Linux创建应用程序。应用程序在所有平台上都具有本机外观。与PyQt不同，WxPython不是由商业企业开发的。 PyQt5使用Python和Pyqt制作的应用程序可在Windows，Mac和Unix上运行。Pyqt有两个主要版本：PyQt4和PyQt5。 PyQt是Python最流行的GUI模块。 安装PyQt使用pip安装PyQt 1pip3 install pyqt5 或键入命令： 1pip install pyqt5 我可以用PyQt做什么？任何图形应用。Qt适用于Windows，Mac OS X和Linux。 web浏览器Konqueror是用Qt制作的：]]></content>
  </entry>
  <entry>
    <title><![CDATA[PyQt5学习教程10 line edit]]></title>
    <url>%2F2018%2F09%2F15%2Fpyqt5%2FPyQt5%E5%AD%A6%E4%B9%A0%E4%B9%8Bline-edit%2F</url>
    <content type="text"><![CDATA[PyQt line edit在本教程中，我们将创建一个显示输入字段的应用程序。 可以使用QLineEdit类创建文本框或LineEdit。许多应用程序具有诸如表单字段之类的文本输入。 介绍首先导入QLineEdit小部件： 1from PyQt5.QtWidgets import QLineEdit 我们还将添加一个文本标签，以向用户显示要键入的内容。导入QLabel： 1from PyQt5.QtWidgets import QLabel 然后将两者都添加到屏幕： 123456self.nameLabel = QLabel(self)self.nameLabel.setText('Name:')self.line = QLineEdit(self)self.line.move(80, 20)self.line.resize(200, 32)self.nameLabel.move(20, 20) 可以使用以下方式打印文本值： 1print('Your name: ' + self.line.text()) QLineEdit 示例12345678910111213141516171819202122232425262728293031323334import sysfrom PyQt5 import QtCore, QtWidgetsfrom PyQt5.QtWidgets import QMainWindow, QWidget, QLabel, QLineEditfrom PyQt5.QtWidgets import QPushButtonfrom PyQt5.QtCore import QSizeclass MainWindow(QMainWindow): def __init__(self): QMainWindow.__init__(self) self.setMinimumSize(QSize(320, 140)) self.setWindowTitle("PyQt Line Edit example (textfield) - pythonprogramminglanguage.com") self.nameLabel = QLabel(self) self.nameLabel.setText('Name:') self.line = QLineEdit(self) self.line.move(80, 20) self.line.resize(200, 32) self.nameLabel.move(20, 20) pybutton = QPushButton('OK', self) pybutton.clicked.connect(self.clickMethod) pybutton.resize(200,32) pybutton.move(80, 60) def clickMethod(self): print('Your name: ' + self.line.text())if __name__ == "__main__": app = QtWidgets.QApplication(sys.argv) mainWin = MainWindow() mainWin.show() sys.exit( app.exec_() )]]></content>
  </entry>
  <entry>
    <title><![CDATA[将模块分割成多个文件]]></title>
    <url>%2F2018%2F09%2F14%2F%E5%B0%86%E6%A8%A1%E5%9D%97%E5%88%86%E5%89%B2%E6%88%90%E5%A4%9A%E4%B8%AA%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[你想将一个模块分割成多个文件。但是你不想将分离的文件统一成一个逻辑模块时，使已有的代码遭到破坏(或者你书写了多个文件，并希望将这多个文件统一成一个逻辑模块)。解决方案是，在这个模块中，使用__init__.py将这几个文件粘合起来。 1. 示例程序模块可以通过变成包来分割成多个独立的文件。考虑下下面简单的模块：12345678# mymodule.pyclass A: def spam(self): print('A.spam')class B(A): def bar(self): print('B.bar') 假设你想mymodule.py分为两个文件，每个定义的一个类。要做到这一点，首先用mymodule目录来替换文件mymodule.py。这这个目录下，创建以下文件：1234mymodule/ __init__.py a.py b.py 在a.py文件中插入以下代码：1234# a.pyclass A: def spam(self): print('A.spam') 在b.py文件中插入以下代码：12345# b.pyfrom .a import Aclass B(A): def bar(self): print('B.bar') 最后，在 init.py 中，将2个文件粘合在一起：123# __init__.pyfrom .a import Afrom .b import B 如果按照这些步骤，所产生的包MyModule将作为一个单一的逻辑模块：12345678&gt;&gt;&gt; import mymodule&gt;&gt;&gt; a = mymodule.A()&gt;&gt;&gt; a.spam()A.spam&gt;&gt;&gt; b = mymodule.B()&gt;&gt;&gt; b.bar()B.bar&gt;&gt;&gt; 2. 讨论在这个章节中的主要问题是一个设计问题，不管你是否希望用户使用很多小模块或只是一个模块。举个例子，在一个大型的代码库中，你可以将这一切都分割成独立的文件，让用户使用大量的import语句，就像这样：123from mymodule.a import Afrom mymodule.b import B... 这样能工作，但这让用户承受更多的负担，用户要知道不同的部分位于何处。通常情况下，将这些统一起来，使用一条import将更加容易，就像这样：1from mymodule import A, B 对后者而言，让mymodule成为一个大的源文件是最常见的。但是，这一章节展示了如何合并多个文件合并成一个单一的逻辑命名空间。这样做的关键是创建一个包目录，使用 __init__.py 文件来将每部分粘合在一起。 当一个模块被分割，你需要特别注意交叉引用的文件名。举个例子，在这一章节中，B类需要访问A类作为基类。用包的相对导入 from .a import A 来获取。整个章节都使用包的相对导入来避免将顶层模块名硬编码到源代码中。这使得重命名模块或者将它移动到别的位置更容易。（见10.3小节） 3. 延迟导入作为这一章节的延伸，将介绍延迟导入。如图所示，init.py文件一次导入所有必需的组件的。但是对于一个很大的模块，可能你只想组件在需要时被加载。要做到这一点，init.py有细微的变化：12345678# __init__.pydef A(): from .a import A return A()def B(): from .b import B return B() 在这个版本中，类A和类B被替换为在第一次访问时加载所需的类的函数。对于用户，这看起来不会有太大的不同。例如：12345&gt;&gt;&gt; import mymodule&gt;&gt;&gt; a = mymodule.A()&gt;&gt;&gt; a.spam()A.spam&gt;&gt;&gt; 延迟加载的主要缺点是继承和类型检查可能会中断。你可能会稍微改变你的代码，例如:12345if isinstance(x, mymodule.A): # Error...if isinstance(x, mymodule.a.A): # Ok... 延迟加载的真实例子, 见标准库 multiprocessing/__init__.py 的源码. 参考文档 将模块分割成多个文件]]></content>
      <tags>
        <tag>python</tag>
        <tag>模块与包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将模块分割成多个文件]]></title>
    <url>%2F2018%2F09%2F14%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2F%E5%B0%86%E6%A8%A1%E5%9D%97%E5%88%86%E5%89%B2%E6%88%90%E5%A4%9A%E4%B8%AA%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[你想将一个模块分割成多个文件。但是你不想将分离的文件统一成一个逻辑模块时，使已有的代码遭到破坏(或者你书写了多个文件，并希望将这多个文件统一成一个逻辑模块)。解决方案是，在这个模块中，使用__init__.py将这几个文件粘合起来。 1. 示例程序模块可以通过变成包来分割成多个独立的文件。考虑下下面简单的模块：12345678# mymodule.pyclass A: def spam(self): print('A.spam')class B(A): def bar(self): print('B.bar') 假设你想mymodule.py分为两个文件，每个定义的一个类。要做到这一点，首先用mymodule目录来替换文件mymodule.py。这这个目录下，创建以下文件：1234mymodule/ __init__.py a.py b.py 在a.py文件中插入以下代码：1234# a.pyclass A: def spam(self): print('A.spam') 在b.py文件中插入以下代码：12345# b.pyfrom .a import Aclass B(A): def bar(self): print('B.bar') 最后，在 init.py 中，将2个文件粘合在一起：123# __init__.pyfrom .a import Afrom .b import B 如果按照这些步骤，所产生的包MyModule将作为一个单一的逻辑模块：12345678&gt;&gt;&gt; import mymodule&gt;&gt;&gt; a = mymodule.A()&gt;&gt;&gt; a.spam()A.spam&gt;&gt;&gt; b = mymodule.B()&gt;&gt;&gt; b.bar()B.bar&gt;&gt;&gt; 2. 讨论在这个章节中的主要问题是一个设计问题，不管你是否希望用户使用很多小模块或只是一个模块。举个例子，在一个大型的代码库中，你可以将这一切都分割成独立的文件，让用户使用大量的import语句，就像这样：123from mymodule.a import Afrom mymodule.b import B... 这样能工作，但这让用户承受更多的负担，用户要知道不同的部分位于何处。通常情况下，将这些统一起来，使用一条import将更加容易，就像这样：1from mymodule import A, B 对后者而言，让mymodule成为一个大的源文件是最常见的。但是，这一章节展示了如何合并多个文件合并成一个单一的逻辑命名空间。这样做的关键是创建一个包目录，使用 __init__.py 文件来将每部分粘合在一起。 当一个模块被分割，你需要特别注意交叉引用的文件名。举个例子，在这一章节中，B类需要访问A类作为基类。用包的相对导入 from .a import A 来获取。整个章节都使用包的相对导入来避免将顶层模块名硬编码到源代码中。这使得重命名模块或者将它移动到别的位置更容易。（见10.3小节） 3. 延迟导入作为这一章节的延伸，将介绍延迟导入。如图所示，init.py文件一次导入所有必需的组件的。但是对于一个很大的模块，可能你只想组件在需要时被加载。要做到这一点，init.py有细微的变化：12345678# __init__.pydef A(): from .a import A return A()def B(): from .b import B return B() 在这个版本中，类A和类B被替换为在第一次访问时加载所需的类的函数。对于用户，这看起来不会有太大的不同。例如：12345&gt;&gt;&gt; import mymodule&gt;&gt;&gt; a = mymodule.A()&gt;&gt;&gt; a.spam()A.spam&gt;&gt;&gt; 延迟加载的主要缺点是继承和类型检查可能会中断。你可能会稍微改变你的代码，例如:12345if isinstance(x, mymodule.A): # Error...if isinstance(x, mymodule.a.A): # Ok... 延迟加载的真实例子, 见标准库 multiprocessing/__init__.py 的源码. 参考文档 将模块分割成多个文件]]></content>
      <tags>
        <tag>python</tag>
        <tag>模块与包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python字符串加密与解密]]></title>
    <url>%2F2018%2F09%2F13%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E5%8A%A0%E5%AF%86%E4%B8%8E%E8%A7%A3%E5%AF%86%2F</url>
    <content type="text"><![CDATA[对于字符串加密与解密来说，PyCrypto这个库应该是python密码学方面最出名第三库了，但是它几经很久没有维护了，python3.6以上版本会安装失败，现在的解决方案是使用cryptography 官方示例1234567891011&gt;&gt;&gt; from cryptography.fernet import Fernet&gt;&gt;&gt; # Put this somewhere safe!&gt;&gt;&gt; key = Fernet.generate_key()&gt;&gt;&gt; key&gt;&gt;&gt; b'YDDzLAdujn-QYxq5tjnTrU9MOPiHCJ6tU9MgkQ0BS1E='&gt;&gt;&gt; f = Fernet(key)&gt;&gt;&gt; token = f.encrypt(b"A really secret message. Not for prying eyes.")&gt;&gt;&gt; token'...'&gt;&gt;&gt; f.decrypt(token)'A really secret message. Not for prying eyes.' 导入包Fernet 实例化Fernet需要唯一的参数key,这个参数要求有点高，不是随便一个字节序列就行，要求32位 + url-safe + base64-encoded 的bytes类型。为了方便，Fernet类内置了生成key的类方法: generate_key()，作为加密解密的钥匙，生成的key你要保存好，以供解密的时候使用 实例化一个Fernet对象。 接下来就是加密方法: fernet.encrypt(data) 接受一个bytes类型的数据，返回一个加密后的bytes类型数据(人类看不懂)，俗称 token-Fernet。 解密fernet.decrypt(token) 使用实例1234567891011121314151617181920212223242526272829from cryptography.fernet import Fernetclass Crypto(object): """docstring for ClassName""" def __init__(self, key): self.factory = Fernet(key) def generate_key(self): key = Fernet.generate_key() print(key) # 加密 def encrypt(self,string): token = self.factory.encrypt(string.encode('utf-8')) return token # 解密 def decrypt(self,token): string = self.factory.decrypt(token).decode('utf-8') return string# 密钥，需要保存好key = b'GqycX8dOsZThS25NRI7hwCJw3JcKebj8NnXfVvqRHSc='crypto = Crypto(key)if __name__ == '__main__': # 加密字符串 token = crypto.encrypt('A really secret message. Not for prying eyes.') print(token) # 解密字符串 string = crypto.decrypt(token) print(string)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python中logging模块的基本用法]]></title>
    <url>%2F2018%2F09%2F13%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2FPython%E4%B8%ADlogging%E6%A8%A1%E5%9D%97%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[logging模块的基本用法123456789import logging logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__) logger.info('This is a log info')logger.debug('Debugging')logger.warning('Warning exists')logger.info('Finish') 在这里我们首先引入了 logging 模块，然后进行了一下基本的配置，这里通过 basicConfig 配置了 level 信息和 format 信息，这里 level 配置为 INFO 信息，即只输出 INFO 级别的信息，另外这里指定了 format 格式的字符串，包括 asctime、name、levelname、message 四个内容，分别代表运行时间、模块名称、日志级别、日志内容，这样输出内容便是这四者组合而成的内容了，这就是 logging 的全局配置。 basicConfig 的参数 filename：即日志输出的文件名，如果指定了这个信息之后，实际上会启用 FileHandler，而不再是 StreamHandler，这样日志信息便会输出到文件中了。 filemode：这个是指定日志文件的写入方式，有两种形式，一种是 w，一种是 a，分别代表清除后写入和追加写入。 format：指定日志信息的输出格式，即上文示例所示的参数，部分参数如下所示： %(levelno)s：打印日志级别的数值。 %(levelname)s：打印日志级别的名称。 %(pathname)s：打印当前执行程序的路径，其实就是sys.argv[0]。 %(filename)s：打印当前执行程序名。 %(funcName)s：打印日志的当前函数。 %(lineno)d：打印日志的当前行号。 %(asctime)s：打印日志的时间。 %(thread)d：打印线程ID。 %(threadName)s：打印线程名称。 %(process)d：打印进程ID。 %(processName)s：打印线程名称。 %(module)s：打印模块名称。 %(message)s：打印日志信息。 datefmt：指定时间的输出格式。 style：如果 format 参数指定了，这个参数就可以指定格式化时的占位符风格，如 %、{、$ 等。 level：指定日志输出的类别，程序会输出大于等于此级别的信息。 stream：在没有指定 filename 的时候会默认使用 StreamHandler，这时 stream 可以指定初始化的文件流。 handlers：可以指定日志处理时所使用的 Handlers，必须是可迭代的。 Level首先我们来了解一下输出日志的等级信息，logging 模块共提供了如下等级，每个等级其实都对应了一个数值，列表如下： CRITICAL: 50 FATAL: 50 ERROR: 40 WARNING: 30 WARN: 30 INFO: 20 DEBUG: 10 NOTSET: 0 这里最高的等级是 CRITICAL 和 FATAL，两个对应的数值都是 50，另外对于 WARNING 还提供了简写形式 WARN，两个对应的数值都是 30。 我们设置了输出 level，系统便只会输出 level 数值大于或等于该 level 的的日志结果，例如我们设置了输出日志 level 为 INFO，那么输出级别大于等于 INFO 的日志，如 WARNING、ERROR 等，DEBUG 和 NOSET 级别的不会输出。 Handler下面我们先来了解一下 Handler 的用法，看下面的实例： 1234567891011121314import logginglogger = logging.getLogger(__name__)logger.setLevel(level=logging.INFO)formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')# 设置handler的输出文件，格式并应用该handlerhandler = logging.FileHandler('output.log')handler.setFormatter(formatter)logger.addHandler(handler)# 输出日志logger.info('This is a log info')logger.debug('Debugging')logger.warning('Warning exists')logger.info('Finish') 这里我们没有再使用 basicConfig 全局配置，而是先声明了一个 Logger 对象，然后指定了其对应的 Handler 为 FileHandler 对象，然后 Handler 对象还单独指定了 Formatter 对象单独配置输出格式，最后给 Logger 对象添加对应的 Handler 即可，最后可以发现日志就会被输出到 output.log 中，内容如下： 1232018-06-03 14:53:36,467 - __main__ - INFO - This is a log info2018-06-03 14:53:36,468 - __main__ - WARNING - Warning exists2018-06-03 14:53:36,468 - __main__ - INFO - Finish 另外我们还可以使用其他的 Handler 进行日志的输出，logging 模块提供的 Handler 有： StreamHandler：logging.StreamHandler；日志输出到流，可以是 sys.stderr，sys.stdout 或者文件。 FileHandler：logging.FileHandler；日志输出到文件。 BaseRotatingHandler：logging.handlers.BaseRotatingHandler；基本的日志回滚方式。 RotatingHandler：logging.handlers.RotatingHandler；日志回滚方式，支持日志文件最大数量和日志文件回滚。 TimeRotatingHandler：logging.handlers.TimeRotatingHandler；日志回滚方式，在一定时间区域内回滚日志文件。 SocketHandler：logging.handlers.SocketHandler；远程输出日志到TCP/IP sockets。 DatagramHandler：logging.handlers.DatagramHandler；远程输出日志到UDP sockets。 SMTPHandler：logging.handlers.SMTPHandler；远程输出日志到邮件地址。 SysLogHandler：logging.handlers.SysLogHandler；日志输出到syslog。 NTEventLogHandler：logging.handlers.NTEventLogHandler；远程输出日志到Windows NT/2000/XP的事件日志。 MemoryHandler：logging.handlers.MemoryHandler；日志输出到内存中的指定buffer。 HTTPHandler：logging.handlers.HTTPHandler；通过”GET”或者”POST”远程输出到HTTP服务器。 下面我们使用三个 Handler 来实现日志同时输出到控制台、文件、HTTP 服务器： 12345678910111213141516171819202122232425262728import loggingfrom logging.handlers import HTTPHandlerimport syslogger = logging.getLogger(__name__)logger.setLevel(level=logging.DEBUG)# StreamHandlerstream_handler = logging.StreamHandler(sys.stdout)stream_handler.setLevel(level=logging.DEBUG)logger.addHandler(stream_handler)# FileHandlerfile_handler = logging.FileHandler('output.log')file_handler.setLevel(level=logging.INFO)formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')file_handler.setFormatter(formatter)logger.addHandler(file_handler)# HTTPHandlerhttp_handler = HTTPHandler(host='localhost:8001', url='log', method='POST')logger.addHandler(http_handler)# Loglogger.info('This is a log info')logger.debug('Debugging')logger.warning('Warning exists')logger.info('Finish') Formatter在进行日志格式化输出的时候，我们可以不借助于 basicConfig 来全局配置格式化输出内容，可以借助于 Formatter 来完成，下面我们再来单独看下 Formatter 的用法： 123456789101112131415import logging logger = logging.getLogger(__name__)logger.setLevel(level=logging.WARN)formatter = logging.Formatter(fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%Y/%m/%d %H:%M:%S')handler = logging.StreamHandler()handler.setFormatter(formatter)logger.addHandler(handler) # Loglogger.debug('Debugging')logger.critical('Critical Something')logger.error('Error Occurred')logger.warning('Warning exists')logger.info('Finished') 在这里我们指定了一个 Formatter，并传入了 fmt 和 datefmt 参数，这样就指定了日志结果的输出格式和时间格式，然后 handler 通过 setFormatter() 方法设置此 Formatter 对象即可，输出结果如下： 1232018/06/03 15:47:15 - __main__ - CRITICAL - Critical Something2018/06/03 15:47:15 - __main__ - ERROR - Error Occurred2018/06/03 15:47:15 - __main__ - WARNING - Warning 这样我们可以每个 Handler 单独配置输出的格式，非常灵活。 异常处理另外在进行异常处理的时候，通常我们会直接将异常进行字符串格式化，但其实可以直接指定一个参数将 traceback 打印出来，示例如下： 12345678910111213import logginglogging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')try: result = 5 / 0except Exception as e: # bad logging.error('Error: %s', e) # good logging.error('Error', exc_info=True) # good logging.exception('Error') 如果我们直接使用字符串格式化的方法将错误输出的话，是不会包含 Traceback 信息的，但如果我们加上 exc_info 参数或者直接使用 exception() 方法打印的话，那就会输出 Traceback 信息了。 运行结果如下： 12345678910112018-06-03 22:24:31,927 - root - ERROR - Error: division by zero2018-06-03 22:24:31,927 - root - ERROR - ErrorTraceback (most recent call last): File &quot;/private/var/books/aicodes/loggingtest/demo9.py&quot;, line 6, in &lt;module&gt; result = 5 / 0ZeroDivisionError: division by zero2018-06-03 22:24:31,928 - root - ERROR - ErrorTraceback (most recent call last): File &quot;/private/var/books/aicodes/loggingtest/demo9.py&quot;, line 6, in &lt;module&gt; result = 5 / 0ZeroDivisionError: division by zero]]></content>
  </entry>
  <entry>
    <title><![CDATA[使用pyqt5的一些问题以及解决方案]]></title>
    <url>%2F2018%2F09%2F13%2Fpyqt5%2F%E4%BD%BF%E7%94%A8pyqt5%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[由于刚开始学习使用pyqt5，对于它的使用原理还不清楚，使用过程中遇到很多的坑，有些虽然解决了，但是还不清楚为什么这样可以解决。由于这个框架非常的庞大，需要长久的学习掌握，遇到的问题先记录下来，之后慢慢更新剖析其原理。 在窗口中使用多进程/弹窗报错12345class Uboot(QWidget): ... def submit(self): uboot_test = UbootTest() uboot_test.start() 报错: 1QThread: Destroyed while thread is still running 修改为: 12345class Uboot(QWidget): ... def submit(self): self.uboot_test = UbootTest() self.uboot_test.start() 可以正常运行 对于窗口的显示，同理:123class ZeusTester(QMainWindow): def __init__(self): uboot = Uboot() # 一个widget，这样uboot窗口会显示失败 修改为: 123class ZeusTester(QMainWindow): def __init__(self): self.uboot = Uboot() # 一个widget uboot窗口显示成功]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python相对导入与绝对导入]]></title>
    <url>%2F2018%2F09%2F12%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2FPython%E7%9B%B8%E5%AF%B9%E5%AF%BC%E5%85%A5%E4%B8%8E%E7%BB%9D%E5%AF%B9%E5%AF%BC%E5%85%A5%2F</url>
    <content type="text"><![CDATA[Python 相对导入与绝对导入，这两个概念是相对于包内导入而言的。包内导入即是包内的模块导入包内部的模块。 Python import 的搜索路径 在当前目录下搜索该模块 在环境变量 PYTHONPATH 中指定的路径列表中依次搜索 在 Python 安装路径的 lib 库中搜索 Python import 的步骤python 所有加载的模块信息都存放在 sys.modules 结构中，当 import 一个模块时，会按如下步骤来进行 如果是 import A，检查 sys.modules 中是否已经有 A，如果有则不加载，如果没有则为 A 创建 module 对象，并加载 A 如果是 from A import B，先为 A 创建 module 对象，再解析A，从中寻找B并填充到 A 的 __dict__ 中 相对导入与绝对导入绝对导入的格式为 import A.B 或 from A import B，相对导入格式为 from . import B 或 from ..A import B，.代表当前模块，..代表上层模块，...代表上上层模块，依次类推。 相对导入可以避免硬编码带来的维护问题，例如我们改了某一顶层包的名，那么其子包所有的导入就都不能用了。但是 存在相对导入语句的模块，不能直接运行，否则会有异常： 1ValueError: Attempted relative import in non-package 这是什么原因呢？我们需要先来了解下导入模块时的一些规则： 在没有明确指定包结构的情况下，Python 是根据 __name__ 来决定一个模块在包中的结构的，如果是 __main__ 则它本身是顶层模块，没有包结构，如果是A.B.C 结构，那么顶层模块是 A。基本上遵循这样的原则： 如果是绝对导入，一个模块只能导入自身的子模块或和它的顶层模块同级别的模块及其子模块（文件路径都是相对顶层模块而言） 如果是相对导入，一个模块必须有包结构且只能导入它的顶层模块内部的模块 如果一个模块被直接运行，则它自己为顶层模块，不存在层次结构，所以找不到其他的相对路径。 Python2.x 缺省为相对路径导入，Python3.x 缺省为绝对路径导入。绝对导入可以避免导入子包覆盖掉标准库模块（由于名字相同，发生冲突）。如果在 Python2.x 中要默认使用绝对导入，可以在文件开头加入如下语句： 1from __future__ import absolute_import from __future__ import absolute_import这句 import 并不是指将所有的导入视为绝对导入，而是指禁用 implicit relative import（隐式相对导入）, 但并不会禁掉 explicit relative import（显示相对导入）。 那么到底什么是隐式相对导入，什么又是显示的相对导入呢？我们来看一个例子，假设有如下包结构： 1234567891011121314thing├── books│ ├── adventure.py│ ├── history.py│ ├── horror.py│ ├── __init__.py│ └── lovestory.py├── furniture│ ├── armchair.py│ ├── bench.py│ ├── __init__.py│ ├── screen.py│ └── stool.py└── __init__.py 那么如果在 stool 中引用 bench，则有如下几种方式: 123import bench # 此为 implicit relative importfrom . import bench # 此为 explicit relative importfrom furniture import bench # 此为 absolute import 隐式相对就是没有告诉解释器相对于谁，但默认相对与当前模块；而显示相对则明确告诉解释器相对于谁来导入。以上导入方式的第三种，才是官方推荐的，第一种是官方强烈不推荐的，Python3 中已经被废弃，这种方式只能用于导入 path 中的模块。 相对与绝对仅针对包内导入而言最后再次强调，相对导入与绝对导入仅针对于包内导入而言，要不然本文所讨论的内容就没有意义。所谓的包，就是包含 __init__.py 文件的目录，该文件在包导入时会被首先执行，该文件可以为空，也可以在其中加入任意合法的 Python 代码。 相对导入可以避免硬编码，对于包的维护是友好的。绝对导入可以避免与标准库命名的冲突，实际上也不推荐自定义模块与标准库命令相同。 前面提到含有相对导入的模块不能被直接运行，实际上含有绝对导入的模块也不能被直接运行，会出现 ImportError： 1ImportError: No module named XXX 这与绝对导入时是一样的原因。要运行包中包含绝对导入和相对导入的模块，可以用 python -m A.B.C 告诉解释器模块的层次结构。 有人可能会问：假如有两个模块 a.py 和 b.py 放在同一个目录下，为什么能在 b.py 中 import a 呢？ 这是因为这两个文件所在的目录不是一个包，那么每一个 python 文件都是一个独立的、可以直接被其他模块导入的模块，就像你导入标准库一样，它们不存在相对导入和绝对导入的问题。相对导入与绝对导入仅用于包内部。 总结对于包的模块导入而言，建议使用绝对导入的方式导入模块，而对于任意子模块，绝对导入的路径都是相对于入口文件路径而言的 12345678demo│├── lib # 一个名为lib的包│ ├── m1.py│ ├── m2.py│ ├── __init__.py│└── main.py # 入口文件 文件: 123# m1.pydef a(): print('func_a: import moudu 1') 12345678# m2.py # 作为包内的文件，必须使用绝对导入或相对导入的方式导入依赖# 作为包内的文件，使用了绝对导入和相对导入都是不能直接运行的from lib.m1 import adef b(): a() print('func_b: import moudu 2') 1234# main.pyfrom lib.m2 import bif __name__ == '__main__': b() 执行与输出:1234$ python main.pyfunc_a: import moudu 1func_b: import moudu 2]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python全局变量的实现方式]]></title>
    <url>%2F2018%2F09%2F10%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[全局变量如何在一个项目中多个文件共享一个变量呢？使用一个config.py作为模块导入,并给需要作为全局变量的属性赋值即可 示例： config.py1234class Config(): def __init__(self): passconfig = Config() main.py12form config import configconfig.path = os.path.abspath(&apos;.&apos;) other.py12form config import configprint(config.path) 原理Python中所有加载到内存的模块都放在 sys.modules 。当 import 一个模块时首先会在这个列表中查找是否已经加载了此模块，如果加载了则只是将模块的名字加入到正在调用 import 的模块的 Local 名字空间中。如果没有加载则从 sys.path 目录中按照模块名称查找模块文件，模块可以是py、pyc、pyd，找到后将模块载入内存，并加到 sys.modules 中，并将名称导入到当前的 Local 名字空间。 也就是说，多个模块import同一个模块，这个模块只会运行(初始化)一次。]]></content>
  </entry>
  <entry>
    <title><![CDATA[將Python打包成 exe可执行文件]]></title>
    <url>%2F2018%2F09%2F10%2Fpython%2F4.python%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%2F%E5%B0%87Python%E6%89%93%E5%8C%85%E6%88%90exe%E5%8F%AF%E6%89%A7%E8%A1%8C%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[利用Python寫了一個小腳本想要傳給使用Windows但沒有裝Python的朋友執行，這時候就可以利用將檔案包裝成exe檔案，讓沒有Python的朋友也可以執行。本篇將介紹利用套件「PyInstaller」製作exe檔。 安裝方法1234# 安裝pyinstallerpip install pyinstaller# 安装依赖pip install pywin32-ctypes 常用参数介绍 pyinstaller -h 來查看參數 -F 打包成一個exe文件 -i 圖標路徑 -w 使用視窗，無控制台 -c 使用控制台，無視窗 -D 創建一個目錄，包含exe以及其他一些依賴性文件 编译文件时，我们通常使用如下命令: 1pyinstaller -F -w -i favo.ico main.py 静态文件对于python所依赖的图片,config.yaml等静态文件，pyinstall不会自动打包，需要手动在main.py(所需要打包的入口文件)的相同目录中寻找main.spec，修改datas依赖，才可以自动打包到exe的相同目录下，然后执行即可 datas是一个数组，每个子项是一个tuple，其中tuple的第一个参数是你要打包的源路径，第二个参数是打包后的名字 1234567891011121314151617# -*- mode: python -*-block_cipher = Nonea = Analysis([&apos;ui.py&apos;], pathex=[&apos;C:\\Users\\user\\Desktop\\resilio\\toyou\\code\\zeus_board_test&apos;], binaries=[], datas=[(&apos;C:\\Users\\user\\Desktop\\resilio\\toyou\\code\\zeus_board_test\\images&apos;,&apos;images&apos;),(&apos;C:\\Users\\user\\Desktop\\resilio\\toyou\\code\\zeus_board_test\\question.yaml&apos;,&apos;question.yaml&apos;)], hiddenimports=[], hookspath=[], runtime_hooks=[], excludes=[], win_no_prefer_redirects=False, win_private_assemblies=False, cipher=block_cipher, noarchive=False) 然后执行： 1pyinstaller -F main.spec 示例如下圖所示，我們編寫一個輸出 helow pyinstaller 的Python程式，並利用input()使程式可以暫時停在輸出畫面。 12print(&apos;helow pyinstaller&apos;)input(&apos;please wait:&apos;) 编译python为exe,编译过程中会自动安装相关依赖 1pyinstaller -F hello.py 编译过程中会产生如下输出: 1234567891011121314...6942 INFO: checking PYZ6944 INFO: Building because toc changed6944 INFO: Building PYZ (ZlibArchive) C:\Users\user\Desktop\test\build\test\PYZ-00.pyz7559 INFO: Building PYZ (ZlibArchive) C:\Users\user\Desktop\test\build\test\PYZ-00.pyz completed successfully.7568 INFO: checking PKG7569 INFO: Building because toc changed7569 INFO: Building PKG (CArchive) PKG-00.pkg9407 INFO: Building PKG (CArchive) PKG-00.pkg completed successfully.9409 INFO: Bootloader c:\users\user\appdata\local\programs\python\python36\lib\site-packages\PyInstaller\bootloader\Windows-64bit\run.exe9409 INFO: checking EXE9411 INFO: Building because toc changed9411 INFO: Building EXE from EXE-00.toc9412 INFO: Appending archive to EXE C:\Users\user\Desktop\test\dist\test.exe9455 INFO: Building EXE from EXE-00.toc completed successfully. 通过输出可以看出pyinstaller进行了如下操作: 會先建立一個 hello.spec 建立「build」 資料夾 建立 log紀錄檔與工作檔案於資料夾 build 中 建立 「dist 」資料夾 建立執行檔(.exe)在 「dist」 資料夾 注意事項執行檔案可在win8/win10,64位元的電腦運行，但win7 x64和其餘所有32位失敗，會提示不兼容，若要能32位元與64位元皆可運行，就要在Python 32位元的環境下編譯PyInstaller打包exe，才能在32位元與64位元成功。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python多进程锁 multiprocessing.Lock()与内存共享]]></title>
    <url>%2F2018%2F09%2F08%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2FPython%E5%A4%9A%E8%BF%9B%E7%A8%8B%E9%94%81-multiprocessing-Lock%2F</url>
    <content type="text"><![CDATA[多进程锁当我们用多进程来读写文件的时候，一个进程是写文件，一个进程是读文件，如果两个文件同时进行，肯定是不行的，必须是文件写结束后，才可以进行读操作。因此，我们需要使用进程锁来保证，写进程完成后读进程才开始运行。 123多进程的读写操作，由于进程同时进行会造成结果混乱p1 = multiprocessing.Process(target=write) p2 = multiprocessing.Process(target=read) 我们可以使用multiprocessing.Lock()生成一个锁，任何使用此锁的函数(同一个或不同)，被进程调用后，都会等待锁释放后才会被其它进程锁调用 123456lock = multiprocessing.Lock()lock.acquire() # 获得锁lock.release() # 释放锁with lock: # 获得锁，等待运行结束后释放锁，并自动处理错误 # 由于lock.acquire()需要手动处理错误，因此建议使用with lock 一个简单的示例： 12345678910111213141516171819202122232425262728import multiprocessingimport timedef add(number,value,lock): # 获取锁，lock是multiprocessing.Lock的一个实例 with lock: print("init add&#123;0&#125; number = &#123;1&#125;".format(value, number)) for i in range(1, 6): number += value time.sleep(1) print("add&#123;0&#125; number = &#123;1&#125;".format(value, number))def plus(number,value,lock): # 此处的lock与add中的lock是一个锁，所以会等待add的锁释放后才会运行plus锁内的代码 with lock: print("init plus&#123;0&#125; number = &#123;1&#125;".format(value, number)) for i in range(1, 6): number += value time.sleep(1) print("plus&#123;0&#125; number = &#123;1&#125;".format(value, number))if __name__ == "__main__": lock = multiprocessing.Lock() number = 0 p1 = multiprocessing.Process(target=add,args=(number, 1, lock)) p2 = multiprocessing.Process(target=plus,args=(number, 3, lock)) p1.start() p2.start() print("main end") 输出结果： 12345678910111213main endinit add1 number = 0add1 number = 1add1 number = 2add1 number = 3add1 number = 4add1 number = 5init plus3 number = 0plus3 number = 3plus3 number = 6plus3 number = 9plus3 number = 12plus3 number = 15 内存共享我们可以使用multiprocessing.Value(&#39;i&#39;, 0)生成一个保存数字的Value实例，这个实例对于所有调用它的进程而言是共享的. 12345678910111213141516171819import multiprocessingimport timedef add(number,value,lock): with lock: print("init add&#123;0&#125; number = &#123;1&#125;".format(value, number.value)) for i in range(1, 6): number.value += value time.sleep(1) print("add&#123;0&#125; number = &#123;1&#125;".format(value, number.value))if __name__ == "__main__": lock = multiprocessing.Lock() number = multiprocessing.Value('i', 0) p1 = multiprocessing.Process(target=add,args=(number, 1, lock)) # number.value的值此时已经变为了5 p2 = multiprocessing.Process(target=add,args=(number, 3, lock)) p1.start() p2.start() print("main end") 输出: 12345678910111213main endinit add1 number = 0add1 number = 1add1 number = 2add1 number = 3add1 number = 4add1 number = 5init add3 number = 5add3 number = 8add3 number = 11add3 number = 14add3 number = 17add3 number = 20]]></content>
  </entry>
  <entry>
    <title><![CDATA[squid 高匿代理设置]]></title>
    <url>%2F2018%2F09%2F08%2Fsquid%2Fsquid%E7%9A%84%E9%AB%98%E5%8C%BF%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[squid 高匿设置透明代理、匿名代理、混淆代理、高匿代理有什么区别这4种代理，主要是在代理服务器端的配置不同，导致其向目标地址发送请求时，REMOTE_ADDR， HTTP_VIA，HTTP_X_FORWARDED_FOR三个变量不同。 1、透明代理(Transparent Proxy) 123REMOTE_ADDR = Proxy IPHTTP_VIA = Proxy IPHTTP_X_FORWARDED_FOR = Your IP 透明代理虽然可以直接“隐藏”你的IP地址，但是还是可以从HTTP_X_FORWARDED_FOR来查到你是谁。 2、匿名代理(Anonymous Proxy) 123REMOTE_ADDR = proxy IPHTTP_VIA = proxy IPHTTP_X_FORWARDED_FOR = proxy IP 匿名代理比透明代理进步了一点：别人只能知道你用了代理，无法知道你是谁。 3、混淆代理(Distorting Proxies) 123REMOTE_ADDR = Proxy IPHTTP_VIA = Proxy IPHTTP_X_FORWARDED_FOR = Random IP address 如上，与匿名代理相同，如果使用了混淆代理，别人还是能知道你在用代理，但是会得到一个假的IP地址，伪装的更逼真 4、高匿代理(Elite proxy或High Anonymity Proxy) 123REMOTE_ADDR = Proxy IPHTTP_VIA = not determinedHTTP_X_FORWARDED_FOR = not determined 可以看出来，高匿代理让别人根本无法发现你是在用代理，所以是最好的选择。 环境12操作系统：CentOS 6.8Squid版本：squid-3.1.10-20.el6_5.3.x86_64 配置（vim /etc/squid/squid.conf，添加以下内容）配置文件说明 123456789101112131415161718192021222324252627282930313233http_port 3128 # 设置监听的IP与端口号cache_mem 64 MB # 额外提供给squid使用的内存，squid的内存总占用为 X * 10+15+“cache_mem”，其中X为squid的cache占用的容量（以GB为单位）， # 比如下面的cache大小是100M，即0.1GB，则内存总占用为0.1*10+15+64=80M，推荐大小为物理内存的1/3-1/2或更多。maximum_object_size 4 MB # 设置squid磁盘缓存最大文件，超过4M的文件不保存到硬盘minimum_object_size 0 KB # 设置squid磁盘缓存最小文件maximum_object_size_in_memory 4096 KB # 设置squid内存缓存最大文件，超过4M的文件不保存到内存cache_dir ufs /var/spool/squid 100 16 256 # 定义squid的cache存放路径 、cache目录容量（单位M）、一级缓存目录数量、二级缓存目录数量logformat combined %&amp;gt;a %ui %un [%tl] &quot;%rm %ru HTTP/%rv&quot; %Hs %&lt;st &quot;%&#123;Referer&#125;&gt;h&quot; &quot;%&#123;User-Agent&#125;&amp;gt;h&quot; %Ss:%Sh # log文件日志格式access_log /var/log/squid/access.log combined # log文件存放路径和日志格式cache_log /var/log/squid/cache.log # 设置缓存日志logfile_rotate 60 # log轮循 60天cache_swap_high 95 # cache目录使用量大于95%时，开始清理旧的cachecache_swap_low 90 # cache目录清理到90%时停止。acl localnet src 192.168.1.0/24 # 定义本地网段http_access allow localnet # 允许本地网段使用http_access deny all # 拒绝所有visible_hostname squid.david.dev # 主机名cache_mgr mchina_tang@qq.com # 管理员邮箱 需要添加的内容 12345678910111213http_port 3128 #端口cache_mem 64 MBmaximum_object_size 4 MBcache_dir ufs /var/spool/squid 100 16 256access_log /var/log/squid/access.logacl localnet src 10.60.20.0/24 #定义本地网段http_access allow localnethttp_access deny allvisible_hostname myserver01.lo #squid主机名cache_mgr test@qq.com #邮箱#以下是高匿的设置request_header_access Via deny allrequest_header_access X-Forwarded-For deny all]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux 技巧：使用 screen 管理你的远程会话]]></title>
    <url>%2F2018%2F09%2F08%2Flinux%2Flinux-%E6%8A%80%E5%B7%A7%EF%BC%9A%E4%BD%BF%E7%94%A8-screen-%E7%AE%A1%E7%90%86%E4%BD%A0%E7%9A%84%E8%BF%9C%E7%A8%8B%E4%BC%9A%E8%AF%9D%2F</url>
    <content type="text"><![CDATA[你是不是经常需要 SSH 或者 telent 远程登录到 Linux 服务器？你是不是经常为一些长时间运行的任务而头疼，比如系统备份、ftp传输等等。通常情况下我们都是为每一个这样的任务开一个远程终端窗口，因为他们执行的时间太长了。必须等待它执行完毕，在此期间可不能关掉窗口或者断开连接，否则这个任务就会被杀掉，一切半途而废了。所幸，通过使用screen命令可以有效解决这个问题。 元凶：SIGHUP信号让我们来看看为什么关掉窗口/断开连接会使得正在运行的程序死掉。 在Linux/Unix中，有这样几个概念： 进程组（process group）：一个或多个进程的集合，每一个进程组有唯一一个进程组ID，即进程组长进程的ID。 会话期（session）：一个或多个进程组的集合，有唯一一个会话期首进程（session leader）。会话期ID为首进程的ID。 会话期可以有一个单独的控制终端（controlling terminal）。与控制终端连接的会话期首进程叫做控制进程（controlling process）。当前与终端交互的进程称为前台进程组。其余进程组称为后台进程组。 根据POSIX.1定义： 挂断信号（SIGHUP）默认的动作是终止程序。 当终端接口检测到网络连接断开，将挂断信号发送给控制进程（会话期首进程）。 如果会话期首进程终止，则该信号发送到该会话期前台进程组。 一个进程退出导致一个孤儿进程组中产生时，如果任意一个孤儿进程组进程处于STOP状态，发送SIGHUP和SIGCONT信号到该进程组中所有进程。 因此当网络断开或终端窗口关闭后，控制进程收到SIGHUP信号退出，会导致该会话期内其他进程退出。 开始使用Screen简单来说，Screen是一个可以在多个进程之间多路复用一个物理终端的窗口管理器。Screen中有会话的概念，用户可以在一个screen会话中创建多个screen窗口，在每一个screen窗口中就像操作一个真实的telnet/SSH连接窗口那样。在screen中创建一个新的窗口有这样几种方式： 1．直接在命令行键入screen命令 [root@tivf06 ~]# screen Screen将创建一个执行shell的全屏窗口。你可以执行任意shell程序，就像在ssh窗口中那样。在该窗口中键入exit退出该窗口，如果这是该screen会话的唯一窗口，该screen会话退出，否则screen自动切换到前一个窗口。 2．Screen命令后跟你要执行的程序。 [root@tivf06 ~]# screen vi test.c Screen创建一个执行vi test.c的单窗口会话，退出vi将退出该窗口/会话。 3．以上两种方式都创建新的screen会话。我们还可以在一个已有screen会话中创建新的窗口。在当前screen窗口中键入C-a c，即Ctrl键+a键，之后再按下c键，screen在该会话内生成一个新的窗口并切换到该窗口。 screen还有更高级的功能。你可以不中断screen窗口中程序的运行而暂时断开（detach）screen会话，并在随后时间重新连接（attach）该会话，重新控制各窗口中运行的程序。例如，我们打开一个screen窗口编辑/tmp/abc文件： [root@tivf06 ~]# screen vi /tmp/abc之后我们想暂时退出做点别的事情，比如出去散散步，那么在screen窗口键入C-a d，Screen会给出detached提示： 半个小时之后回来了，找到该screen会话： 1234[root@tivf06 ~]# screen -lsThere is a screen on: 16582.pts-1.tivf06 (Detached)1 Socket in /tmp/screens/S-root. 重新连接会话： 1[root@tivf06 ~]# screen -r 16582 看看出现什么了，太棒了，一切都在。继续干吧。 你可能注意到给screen发送命令使用了特殊的键组合C-a。这是因为我们在键盘上键入的信息是直接发送给当前screen窗口，必须用其他方式向screen窗口管理器发出命令，默认情况下，screen接收以C-a开始的命令。这种命令形式在screen中叫做键绑定（key binding），C-a叫做命令字符（command character）。 可以通过C-a ?来查看所有的键绑定，常用的键绑定有： C-a ? 显示所有键绑定信息 C-a w 显示所有窗口列表 C-a C-a 切换到之前显示的窗口 C-a c 创建一个新的运行shell的窗口并切换到该窗口 C-a n 切换到下一个窗口 C-a p 切换到前一个窗口(与C-a n相对) C-a 0..9 切换到窗口0..9 C-a a 发送 C-a到当前窗口 C-a d 暂时断开screen会话 C-a k 杀掉当前窗口 C-a 进入拷贝/回滚模式 Screen常用命令 screen -d|-D [pid.tty.host] 不开启新的screen会话，而是断开其他正在运行的screen会话 screen -h num 指定历史回滚缓冲区大小为num行 screen -list|-ls 列出现有screen会话，格式为pid.tty.host screen -d -m 启动一个开始就处于断开模式的会话 screen -r sessionowner/ [pid.tty.host] 重新连接一个断开的会话。多用户模式下连接到其他用户screen会话需要指定sessionowner，需要setuid-root权限 screen -S sessionname 创建screen会话时为会话指定一个名字 screen -v 显示screen版本信息 screen -wipe [match] 同-list，但删掉那些无法连接的会话 下例显示当前有两个处于detached状态的screen会话，你可以使用screen -r &lt;screen_pid&gt;重新连接上： 1234567[root@tivf18 root]# screen –lsThere are screens on: 8736.pts-1.tivf18 (Detached) 8462.pts-0.tivf18 (Detached)2 Sockets in /root/.screen. [root@tivf18 root]# screen –r 8736 如果由于某种原因其中一个会话死掉了（例如人为杀掉该会话），这时screen -list会显示该会话为dead状态。使用screen -wipe命令清除该会话： 123456789101112131415161718192021[root@tivf18 root]# kill -9 8462[root@tivf18 root]# screen -ls There are screens on: 8736.pts-1.tivf18 (Detached) 8462.pts-0.tivf18 (Dead ???)Remove dead screens with &apos;screen -wipe&apos;.2 Sockets in /root/.screen. [root@tivf18 root]# screen -wipeThere are screens on: 8736.pts-1.tivf18 (Detached) 8462.pts-0.tivf18 (Removed)1 socket wiped out.1 Socket in /root/.screen. [root@tivf18 root]# screen -ls There is a screen on: 8736.pts-1.tivf18 (Detached)1 Socket in /root/.screen. [root@tivf18 root]# -d –m 选项是一对很有意思的搭档。他们启动一个开始就处于断开模式的会话。你可以在随后需要的时候连接上该会话。有时候这是一个很有用的功能，比如我们可以使用它调试后台程序。该选项一个更常用的搭配是：-dmS sessionname 启动一个初始状态断开的screen会话： 1[root@tivf06 tianq]# screen -dmS mygdb gdb execlp_test 连接该会话： 1[root@tivf06 tianq]# screen -r mygdb 参考链接linux 技巧：使用 screen 管理你的远程会话]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OSI模型]]></title>
    <url>%2F2018%2F08%2F24%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%9F%E7%90%86%2FOSI%E6%A8%A1%E5%9E%8B%E4%B8%8E%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[层次划分根据建议X.200，OSI将计算机网络体系结构划分为以下七层，标有1～7，第1层在底部。现“OSI/RM”是英文“Open Systems Interconnection Reference Model”的缩写。 第7层 应用层应用层（Application Layer）提供为应用软件而设的接口，以设置与另一应用软件之间的通信。例如: HTTP，HTTPS，FTP，TELNET，SSH，SMTP，POP3等。 第6层 表达层表达层（Presentation Layer）把数据转换为能与接收者的系统格式兼容并适合传输的格式。 第5层 会话层会话层（Session Layer）负责在数据传输中设置和维护计算机网络中两台计算机之间的通信连接。 第4层 传输层传输层（Transport Layer）把传输表头（TH）加至数据以形成数据包。传输表头包含了所使用的协议等发送信息。例如:传输控制协议（TCP）等。 第3层 网络层网络层（Network Layer）决定数据的路径选择和转寄，将网络表头（NH）加至数据包，以形成分组。网络表头包含了网络数据。例如:互联网协议（IP）等。 第2层 数据链路层数据链路层（Data Link Layer）负责网络寻址、错误侦测和改错。当表头和表尾被加至数据包时，会形成帧。数据链表头（DLH）是包含了物理地址和错误侦测及改错的方法。数据链表尾（DLT）是一串指示数据包末端的字符串。例如以太网、无线局域网（Wi-Fi）和通用分组无线服务（GPRS）等。 分为两个子层：逻辑链路控制（logic link control，LLC）子层和介质访问控制（media access control，MAC）子层。 第1层 物理层物理层（Physical Layer）在局部局域网上传送数据帧（data frame），它负责管理计算机通信设备和网络媒体之间的互通。包括了针脚、电压、线缆规范、集线器、中继器、网卡、主机适配器等。 ping与shadowsocks代理ip协议在OSI模型的第3层（网络层）工作，SOCKS协议在第5层（会话层）工作，http协议在第7层（应用层）工作。ping消息不能通过SOCKS传递，但http消息可以。因此如果使用shadowsocks(socks5)代理上网的话,ping google.com无法ping通,但是可以正常上网(http/https) 如果坚持要能Ping通才行，需要使用常规VPN（PPTP/L2PT/IPSec等)]]></content>
      <tags>
        <tag>网络原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python并发编程之多进程mutiprocessing]]></title>
    <url>%2F2018%2F08%2F22%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E4%B9%8B%E5%A4%9A%E8%BF%9B%E7%A8%8Bmutiprocessing%2F</url>
    <content type="text"><![CDATA[进程进程的概念进程是操作系统中最基本的概念。在多道程序系统出现后，为了刻画系统内部出现的动态情况，描述系统内部各道程序的活动规律引进的一个概念，所有多道程序设计操作系统都建立在进程的基础上。 狭义定义：程序的一个执行（运行）实例； 广义定义：进程是一个具有一定独立功能的程序，关于某个数据集合的一次运行活动，是系统进行资源分配和调度(执行)的基本单位，是操作系统结构的基础。在早期面向进程设计的计算机结构中，进程是程序的基本执行实体；在当代面向线程设计的计算机结构中，进程是线程的容器。 进程的概念主要有两点： 进程是一个实体。每一个进程都有它自己的地址空间。一般情况下，进程包括文本区域（text region）、数据区域（data region）和堆栈（stack region）。 文本区域，包含程序的源指令； 数据区域，包含了静态变量； 堆，动态内存分区区域； 栈，动态增长与收缩的段，保存本地变量； 进程是一个“执行中的程序”。 程序是一个没有生命的实体，只有处理器赋予程序生命时（执行程序），它才能成为一个活动的实体，我们称其为进程。程序是指令、数据及其组织形式的描述，进程是程序的实体。 进程的基本状态 就绪状态: 分配了除CPU以外所有的资源，只要获得cpu即可执行 执行状态 阻塞状态: 正在执行的进程由于一些事件无法继续执行，便放弃CPU处于暂停状态。使进程的执行收到阻塞（如访问临界区） 挂起状态: 如发现程序有问题，希望暂时停下来，即暂停运行 同步机制遵循的原则：空闲让进 忙则等待 有限等待 让权等待 进程的通信方式 管道（Pipe） 管道可用于具有亲缘关系进程间的通信，允许一个进程和另一个与它有共同祖先的进程之间进行通信 命名管道（namedpipe） 命名管道克服了管道没有名字的限制，因此，除了拥有管道的功能外，它还可用于无亲缘关系进程间的通信。命名管道在文件系统中有对应的文件名 信号（Signal） 信号是比较复杂的通信方式，用于通知接受进程有某种事件发生，除了用于进程间通信外，进程还可以发送信号给进程本身 消息队列 消息队列是消息的链接表，包括Posix消息队列system V消息队列。有足够权限的进程可以向队列中添加消息，被赋予读权限的进程则可以读走队列中的消息。 共享内存 使得多个进程可以访问同一块内存空间，是最快的可用IPC形式。是针对其他通信机制运行效率较低而设计的。往往与其它通信机制，如信号量结合使用，来达到进程间的同步及互斥 内存映射（mappedmemory） 内存映射允许任何多个进程间通信，每一个使用该机制的进程通过把一个共享的文件映射到自己的进程地址空间 信号量（semaphore） 主要作为进程间以及同一进程不同线程之间的同步手段 套接口（Socket） 常见的进程间通信机制，可用于不同机器之间的进程间通信 多进程进程可以创建子进程，子进程是完全独立运行的实体，每个子进程都拥有自己的私有系统状态和执行主线程。因为子进程是独立的，所以它可以与父进程并发执行。也就是说，父进程可以处理事件1，同时，子进程可以在后台处理事件2。 使用多个进程或线程时，操作系统负责安排它们的工作。具体做法是：给每个进程(线程)安排一个小的时间片，并在所有的任务之间快速轮询，给每个任务分配一部分可用的CPU时间。如系统同时运行10个进程，操作系统会给每个进程分配1/10的CPU时间，在10个进程之间快速轮询。在具有多个CPU的系统上，操作系统可以尽可能使用每个CPU，从而并发执行进程。 Python与并发编程Python支持线程，但是Python的线程受到很多限制，因为Python解释器使用了内部的全局解释锁(GIL)，Python的执行由Python虚拟机控制，Python解释器可以运行多个线程，但是任意时刻只允许单个线程在解释器中执行，对Python虚拟机的访问由全局解释锁(GIL)控制。 GIL保证同一个时刻仅有一个线程在解释器中执行。无论系统上有多少个CPU，Python只能在一个CPU上运行。(使用GIL的原因，在多线程访问数据时，保证数据安全) 如果线程涉及大量的CPU操作，使用线程会降低程序的运行速度。 常见解决GIL锁的方法：使用多进程，将程序设计为大量独立的线程集合。 multiprocessing模块multiprocessing模块为在子进程中运行任务、通信、共享数据，以及执行各种形式的同步提供支持。该模块更适合在UNIX下使用。 这个模块的接口与threading模块的接口类似。但是和线程不同，进程没有任何共享状态，因此，如果某个进程修改了数据，改动只限于该进程内，并不影响其他进程。 (不同进程内，id(10)是不一样的，因为每个进程是相互独立的) ProcessProcess(group=None, target=None, name=None, args=(), kwargs={}) 这个类构造了一个Process进程。表示一个运行在子进程中的任务，应使用关键字参数来指定构造函数中的参数。 如果一个类继承了Process，在进行有关进程的操作前，确保调用了Process的构造函数。 group 预留参数，一直为None target 进程启动时执行的可调用对象，由run()方法调用 name 进程名 args target处可调用对象的参数，如果可调用对象没有参数，不用赋值 kwargs target处可调用对象的关键字参数 Process的实例p具有的属性: p.is_alive() 如果p在运行，返回True p.join([timeout]) 等待进程p运行结束。timeout是可选的超时期限。如果timeout为None，则认为要无限期等待 p.run() 进程启动时运行的方法。默认情况下，会调用传递给Process构造函数中的target；定义进程的另一种方法是继承Process并重写run()方法 p.start() 运行进程p，并调用p.run() p.terminate() 强制杀死进程。如果调用此方法，进程p将被立即终止，同时不会进行任何清理动作。如果进程p创建了自己的子进程，这些进程将会变成 僵尸进程,此方法要小心使用 If this method is used when the associated process is using a pipe or queue then the pipe or queue is liable to become corrupted and may become unusable by other process. Similarly, if the process has acquired a lock or semaphore etc. then terminating it is liable to cause other processes to deadlock. p.authkey 进程的身份验证键 p.daemon 守护进程标志，布尔变量。指进程是否为后台进程。如果该进程为后台进程(daemon = True)，当创建它的Python进程终止时，后台进程将自动终止; 其中p.daemon的值要在使用p.start()启动进程之前设置,并且禁止后台进程创建子进程 p.daemon = True 主进程终止,子进程终止, p.daemon = False 主进程终止,子进程不会终止 p.exitcode 进程的整数退出码。如果进程仍在运行，值为None。如果值为-N，表示进程由信号N终止 p.name 进程名 p.pid 进程号 Note that the start(), join(), is_alive(), terminate() and exitcode methods should only be called by the process that created the process object. 一个最基本的例子123456789101112131415from multiprocessing import Processimport timedef print_hello(name): time.sleep(3) print('hello ' + name)def main(): process = Process(target=print_hello, args=('001',)) process.start() process.join() process.terminate()if __name__ == '__main__': main() Process创建一个子进程实例，接受两个参数，target表示要执行的函数，args是这个函数接受的参数 实例方法start()用于启动子进程，join()用于同步进程，主程序会等待进程运行结束后进行执行，如果不使用join()那么进程异步执行，terminate()用于强制终止子进程,不能与join同时使用。 使用要点： args=(‘001’,)参数是一个tuple,所以如果是一个参数则需要以 , 结尾 多进程调用要在if __name__ == &#39;__main__&#39;之下执行 Queuemultiprocessing模块支持的进程间通信的方式：管道和队列。这两种方法都是使用消息传递实现的。 Queue([size])创建一个共享的进程队列 size为队列的最大长度，默认为无大小限制。底层使用管道，锁和信号量实现。利用线程将队列中的数据传输到管道。 Queue的实例q具有以下方法。 q.qsize() 返回队列中成员的数量,但是此结果并不可靠，因为多线程和多进程，在返回结果和使用结果之间，队列中可能添加/删除了成员。 q.empty() 如果调用此方法时，q为空，返回True ,但是此结果并不可靠，因为多线程和多进程，在返回结果和使用结果之间，队列中可能添加/删除了成员。 q.full()如果调用此方法时，q已满，返回True,但是此结果并不可靠，因为多线程和多进程，在返回结果和使用结果之间，队列中可能添加/删除了成员。 q.put(obj[, block[, timeout]]) 将obj放入队列中。如果队列已满，此方法将阻塞至队列有空间可用为止 block 控制阻塞行为，默认为True；如果设置为False，将obj放入队列时，如果没有可用空间的话，将引发Queue.Full异常; timeout为阻塞时间，超时后将引发Queue.Full异常。默认为无限制等待。 q.put_nowait(obj) 等价于q.put(obj,False) q.get([block[, timeout]]) 返回q中的一个成员。如果队列为空，此方法将阻塞至队列中有成员可用为止 block控制阻塞行为，默认为True；如果设置为False，如果队列中没有可用成员，将引发Queue.Empty异常 timeout为阻塞的时间，超时后将引发Queue.Empty异常。默认为无限制等待。 q.get_nowait() 等价于q.(get,False) q.close() 关闭队列，防止队列中放入更多数据。调用此方法时，后台线程将继续写入那些已入队列但尚未写入的数据，待这些数据写入完成后将马上关闭队列 q在被垃圾回收时将调用此方法 q被关闭后，q.get()可以正常使用; q.put(),q.qsize(),q.empty(),q.full()等操作会抛出异常 q.join_thread() 此方法用于q.close()后，等待后台线程运行完成，阻塞主线程至后台线程运行结束，保证缓冲区中的数据放入管道调用q.cancel_join_thread()可禁止此行为。 q.cancel_join_thread() 阻止q.join_thread()阻塞主线 重写Process的run方法，实现生产者消费者模型： 123456789101112131415161718192021222324252627282930313233343536373839import multiprocessing,timeclass Consumer(multiprocessing.Process): def __init__(self,queue,lock): super().__init__() self.queue = queue self.lock = lock def run(self,): times = 5 while True: time.sleep(1) times -= 1 i = self.queue.get() print ('get = %s, %s'%(i,type(i)))class Producer(multiprocessing.Process): def __init__(self,queue,lock): multiprocessing.Process.__init__(self) self.queue = queue self.lock = lock def run(self,): times = 10 while times: times -= 1 self.queue.put(times) print ('put = %s'%(times)) time.sleep(0.5)if __name__ == "__main__": q = multiprocessing.Queue() lock = multiprocessing.Lock() a = Consumer(q,lock) a.start() b = Producer(q,lock) b.start() 在两个类中分别重写run方法，实现起来比较繁琐，而且人为的分离有时可能违背了面向对象编程的本意，建议在同一个类中实现多进程即可 在一个类中实现生成者消费者模型1234567891011121314151617181920212223242526272829303132333435363738394041424344import multiprocessing,timefrom functools import wrapsdef sync(func): '''装饰器sync''' '''开启一个新的进程启动该函数从而达到异步的效果''' @wraps(func) def wrapper(*args, **kwargs): result = multiprocessing.Process(target = func,args=args, kwargs=kwargs) result.start() return result return wrapperclass Readline(object): """在一个类内实现生产者消费者模型，使用queue进行通信""" def __init__(self): self.queue = multiprocessing.Queue() @sync def readline(self,msg): ''' 生产者:异步 模拟读取信息流等需要快速获取的信息，比如串口信息，如果在获取的同时进行处理等耗时操作，可能会导致信息丢失 ''' print(msg) for i in range(10): time.sleep(0.5) print('put line :',i) self.queue.put(i) def findline(self,line): ''' 消费者:同步 模拟一些耗时的操作 ''' while True: getline = self.queue.get() if line == getline: print('found:',line) breakif __name__ == '__main__': c = Readline() c.readline(msg = 'hellow world!') c.findline(3) c.findline(5) 输出结果：12345678910111213hellow world!put line : 0put line : 1put line : 2put line : 3found: 3put line : 4put line : 5found: 5put line : 6put line : 7put line : 8put line : 9 参考资料Python 多进程编程 - multiprocessing模块]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用python判断图片是否损坏]]></title>
    <url>%2F2018%2F08%2F20%2Fpython%2F4.python%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%2F%E7%94%A8python%E5%88%A4%E6%96%AD%E5%9B%BE%E7%89%87%E6%98%AF%E5%90%A6%E6%8D%9F%E5%9D%8F%2F</url>
    <content type="text"><![CDATA[起因要下载几千张图片，是 jpg 格式的，下下来之后才发现有部分图片预览不了，显示损坏了。损坏的图片有的只有几个字节，有的和正常图片有差不多的大小，所以不能根据文件大小来区分。 解决根据文件头python 标准库里面有个叫 imghdr 的模块，打开它的源代码可以看到它是根据文件前面几个字节来判断是哪种格式。所以不行，有些图片有文件头，但是后面缺失了数据，它是无法判断的。 根据文件结束符网上有说 jpg 尾部是 ff d9 , 我根据它来判断，还是不行，有一部分损坏的图片，它是有结束符的。 用PIL网上有说用 pil 里面 image 的 verify() 来判断，还是不行，去看了下它的代码，它主要针对的是 png 格式的。后来有人说用load() , 试了下可以。代码如下： 12345678910from PIL import Imagedef is_valid_image(filename): valid = True try: Image.open(filename).load() except OSError: valid = False return valid 总结要多读好的代码，比如说 imghdr 这是标准库里面的模块，以前从来没听说过，也没去看过它。它的代码很短，也很简单，很适合我们新手学习。比去网上看别人的文章还要好。网上充满了千篇一律的东西，我们应该好好的看一看标准库里面的代码。先挑简单的来，在慢慢看大一点，复杂一点的模块。学 python 这么久还是新手，主要就是不会思考，不会学习。 在这里我以过来人的身份给大家的忠告是：要读标准库里的源代码。 用python判断图片是否损坏]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为python创造隔离环境]]></title>
    <url>%2F2018%2F08%2F17%2Fpython%2F4.python%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%2F%E4%B8%BApython%E5%88%9B%E9%80%A0%E9%9A%94%E7%A6%BB%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[virtualenv 是一个非常流行的用于创建独立的python libraries环境的工具。我强烈推荐你学习并了解它，因为他非常实用，并且应用广泛，很多人用它来搭建python开发环境。后面其他工具来主要与virtualenv来进行比较以说明差异。 virtualenv1.在系统中安装virtualenv，建议用pip进行安装： 1pip install virtualenv 2.创建项目目录，为项目安装虚拟环境，首先创建了项目文件夹myproject，然后在该文件夹中安装了虚拟环境env。下面代码是在命令行（cmd）下输入。 123456# 创建项目目录mkdir myproject# 进入项目目录cd myproject# 创建虚拟环境envvirtualenv env 3.启动虚拟环境，在windows中虚拟环境的启动使用命令：your_env_dir\Scripts\activate 默认情况下，virtualenv已经安装好了pip。在启动虚拟环境后直接使用pip install 命令就可以为该虚拟环境安装类库。 12# 启动虚拟环境env\Scripts\activate 如果想退出虚拟环境，直接在命令行输入deactivate即可 离线使用virtualenv执行virtualenv env时，virtualenv会自动安装pip等工具，离线时就会导致安装失败； 通过virtualenv.py源代码，我们可以看到它在创建venv时使用pip来安装setuptools/pip/wheel。因此，我们可以利用pip中提供的离线安装选项即可： 首先下载setuptools/pip/wheel到本地setuptoolsPackages文件夹 123pip download setuptoolspip download pippip download wheel 执行以下命令生成ENV 12# --extra-search-dir can be set multiple times, then it produces a listvirtualenv --extra-search-dir path/to/setuptoolsPackages --no-download ENV 推出虚拟环境 1deactivate 重定位文件路径当我们将文件部署到其它的服务器上时，文件路径发生了变化导致虚拟环境运行失败，因此可以使用relocatable参数重新定位文件路径 1virtualenv --relocatable ENV 导出项目中所需要的依赖需要配合virtualenv环境使用,以避免导出整个环境的依赖。 1pip freeze &gt; requirements.txt 如果部署到离线服务器的话，可以: 1234mkdir packagesMirrorcd packagesMirrorpip freeze &gt; requirements.txtpip download -r requirements.txt]]></content>
  </entry>
  <entry>
    <title><![CDATA[ubantu18使用apt-get安装软件]]></title>
    <url>%2F2018%2F08%2F17%2Flinux%2Fubantu18%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[对于软件而言，软件厂商自己编译好了很多二进制文件，只要系统和环境对应，下载之后就能直接安装即可。 但是安装过程中还是会遇到很多痛点: 下载了很多软件想要管理怎么办? 下载一个软件还需要依赖很多别的软件怎么办? 想要及时更新怎么办? 因此我们需要把自己下载的历史信息记录下来，软件也记录自己的版本信息和依赖包。并且服务器也记录这些信息，这就是软件管理器了。 对于软件管理器,redhat主要是rpm和更高级的yum，debian主要是dpkg和更高级的apt。 apt-get软件源源和软件仓库实际上是一个意思，厂商将编译后的二进制文件和软件信息存放至服务器，用户需要安装软件时，包管理器自动分析本机和容器（repository）内的信息，下载需要的包并自动安装，安装后将新安装的软件信息存放至本地数据库。如果有前置软件没有安装，rpm和dpkg会提示安装失败，也可以强制安装，yum和apt会自动安装全部需要的依赖包。更新和卸载也同理。这些源的位置记录在/etc/apt/sources.list，我们可以手动修改这些文件，但是修改重要系统配置前先备份是一个好习惯。 apt-get相关目录/var/lib/dpkg/available 文件的内容是软件包的描述信息, 该软件包括当前系统所使用的 ubunt 安装源中的所有软件包,其中包括当前系统中已安装的和未安装的软件包. /var/cache/apt/archives 目录是在用 apt-get install 安装软件时，软件包的临时存放路径 /etc/apt/sources.list 存放的是软件源站点 /var/lib/apt/lists 使用apt-get update命令会从/etc/apt/sources.list中下载软件列表，并保存到该目录 安装位置 位置 信息 /usr/bin 二进制文件 /usr/lib 动态函数库文件 /usr/share/doc 使用手册 /usr/share/man man page apt-get updatesudo apt-get update 执行这条命令后计算机做了什么？ 无论用户使用哪些手段配置APT软件源，只是修改了配置文件——/etc/apt/sources.list，目的只是告知软件源镜像站点的地址。但那些所指向的镜像站点所具有的软件资源并不清楚，需要将这些资源列个清单，以便本地主机知晓可以申请哪些资源。 用户可以使用“apt-get update”命令刷新软件源，建立更新软件包列表。在Ubuntu Linux中，“apt-get update”命令会扫描每一个软件源服务器，并为该服务器所具有软件包资源建立索引文件，存放在本地的/var/lib/apt/lists/目录中。 使用apt-get执行安装、更新操作时，都将依据这些索引文件，向软件源服务器申请资源。因此，在计算机设备空闲时，经常使用“apt-get update”命令刷新软件源，是一个好的习惯。 apt-get installsudo apt-get install XXX 后计算机做了什么？ 使用“apt-get install”下载软件包大体分为4步： 扫描本地存放的软件包更新列表（由“apt-get update”命令刷新更新列表，也就是/var/lib/apt/lists/），找到最新版本的软件包； 进行软件包依赖关系检查，找到支持该软件正常运行的所有软件包； 从软件源所指 的镜像站点中，下载相关软件包，并存放在/var/cache/apt/archive； 第四步，解压软件包，并自动完成应用程序的安装和配置。 apt-get upgradesudo apt-get upgrade 后计算机做了什么? 使用“apt-get install”命令能够安装或更新指定的软件包。而在Ubuntu Linux中，只需一条命令就可以轻松地将系统中的所有软件包一次性升级到最新版本，这个命令就是“apt-get upgrade”，它可以很方便的完成在相同版本号的发行版中更新软件包。 在依赖关系检查后，命令列出了目前所有需要升级的软件包，在得到用户确认后，便开始更新软件包的下载和安装。当然，apt- get upgrade命令会在最后以合理的次序，安装本次更新的软件包。系统更新需要用户等待一段时间。 软件安装对于新的系统而言,首先需要升级apt-get,否则很多软件是找不到的 1sudo apt-get update 安装nodejs和npm12sudo apt-get -y install nodejssudo apt-get -y install npm 运行node提示错误1run npm command gives error &quot;/usr/bin/env: node: No such file or directory&quot; 1ln -s /usr/bin/nodejs /usr/bin/node 系统node无法更新我遇到了一个问题，即在我的ubantu16 上安装nodejs无法更新，始终显示4.2版本，通过以下方法解决了问题 1234sudo apt-get install curlsudo apt autoremovecurl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -sudo apt-get install nodejs 运行完成后，查看版本号，已经是第10版了，至此升级成功 1nodejs -v]]></content>
      <tags>
        <tag>linux</tag>
        <tag>ubantu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在macOS和Windows上如何同时操作多个文件夹中的文件]]></title>
    <url>%2F2018%2F08%2F17%2Fwindows%2F%E5%9C%A8macOS%E5%92%8CWindows%E4%B8%8A%E5%A6%82%E4%BD%95%E5%90%8C%E6%97%B6%E6%93%8D%E4%BD%9C%E5%A4%9A%E4%B8%AA%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%AD%E7%9A%84%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[文件夹目录层级的使用，一定程度上能够避免文件管理中的混乱，但在某些时候也带来了一些不便。上周，我就遇到了一个问题：下载一季的美剧后，视频与字幕文件分散在各个不同的文件夹中。这样的目录结构既无法自动连续播放，也不能匹配字幕。幸而，操作系统自带的文件类型筛选功能，可以解决这个问题。 文件目录 这种情况下，如果一遍遍地手动打开子文件夹、移动文件，那就未免太麻烦了。所幸经过一翻尝试，我找到了更加方便快捷的方法。 快捷整理 接下来，我会分别讲讲在 macOS 与 Windows 系统中具体怎么做。 macOS 系统其实方法很简单。在访达（Finder）中打开需要的文件夹，在搜索框中输入NOT 种类：文件夹（中文冒号）或者NOT kind:Folder（英文冒号）并回车。注意将搜索范围选择为当前文件夹。这时，就可以看到当前文件夹中的所有文件了（不含子文件夹）。 或者，也可以使用另一条搜索指令NOT *，可将子文件夹也包含在平铺的列表中。 macOS 搜索指令效果演示 得到上述的搜索结果后，可以点击各列的表头进行排序与分类。也可以直接选择你所需要的文件，进行复制、移动、拖拽等操作。还可以右键单击选中文件并选择「用所选项目新建文件夹」菜单项，以快速移动到一个文件夹中。 动图演示 Windows 系统在 Windows 的文件资源管理器，点击搜索框，会出现搜索选项卡，选择相应的类型就可以方便的筛选文件了 Windows 搜索指令效果演示 补充除了文章开头的使用场景，我们还可以在这些情况下使用这个技巧： 将各子文件夹中的部分文档，挑选出来打包并发送给他人。 将分散在不同的歌手文件夹中的部分歌曲收集到一个文件夹中以便临时使用。 有了本文中的方法，就不必再像以前那样一个个复制到新建的文件夹中了。 参考文档]]></content>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10下linux子系统安装与使用]]></title>
    <url>%2F2018%2F08%2F16%2Fwindows%2Fwin10%E4%B8%8Blinix%E5%AD%90%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[微软大概为了和OS X竞争，在windows环境下加入了一个linux子系统，据说是目前最好用的Linux发行版(滑稽)。我个人看来的话是有学习linux或者某些业务场景需要使用但不想装虚拟机的话wsl确实是一个不错的选择，事实上也确实比虚拟机流畅。 linux子系统安装步骤：1. 设置 ——&gt; 更新和安全 2. 开发者选项 ——&gt; 开发人员模式 3. 程序和功能 ——&gt; 启用和关闭Windows 功能 ——&gt; 勾选最后的Lunix子系统 4. 在Microsoft Store搜索关键字linux,选择需要的Lunix分发版本 5. 打开cmd，输入bash,进入linux子系统 WSL与Windows的互操作性Windows的Linux子系统（WSL）不断改进Windows和Linux之间的集成。您可以： 从Linux控制台调用Windows二进制文件。 从Windows控制台调用Linux二进制文件。 Windows Insiders在Linux和Windows之间构建17063+共享环境变量。 这提供了Windows和WSL之间的无缝体验。技术细节在WSL博客上 从WSL运行Windows程序WSL可以使用直接从WSL命令行调用Windows二进制文件[binary name].exe。例如，notepad.exe。为了使Windows可执行文件更易于运行 比如运行安装在windows中的sublime: 12# 路径中文件名含有空格的话，要使用""括起文件路径即可$ '/mnt/c/Program Files/Sublime Text 3/sublime.exe' 如果sublime在你的系统路径中的话，直接调用sublime.exe即可 1$ sublime.exe 为了方便的运行sublime,我们需要给它创建一个软连接 12$ ln -s "/mnt/c/Program Files/Sublime Text 3/sublime.exe" /bin/sublime$ sublime 从WSL修改Windows文件 首先了解一点，不支持使用WSL中的Windows应用程序修改位于VolF（不在/mnt/下）的文件. 使用sublime打开位于/mnt/下的某个文件,修改后保存,然后使用(linux的)python执行它,这是一个完美的无缝工作流，非常了不起。 12$ sublime "/mnt/c/simple.py"$ python simple.py 安装并使用autojump由于windows挂在到mnt路径下，你的项目路径一般比较深，cd转换目录非常的麻烦，利用autojump可以解决这个问题。 autojump的工作方式很简单：它会在你每次启动命令时记录你当前位置，并把它添加进它自身的数据库中。这样，某些目录比其它一些目录添加的次数多，这些目录一般就代表你最重要的目录，而它们的“权重”也会增大。 在Linux上安装autojump在Ubuntu或Debian上安装autojump： 1$ sudo apt-get install autojump 要在CentOS或Fedora上安装autojump，请使用yum命令。在CentOS上，你需要先启用EPEL仓库才行。 1$ sudo yum install autojump 在Archlinux上安装autojump： 1$ sudo pacman -S autojump 如果你找不到适合你的版本的包，你可以从GitHub上下载源码包来编译。 启用autojump123456chmod 755 /usr/share/autojump/autojump.bash# 如果使用bash:echo "source /usr/share/autojump/autojump.bash" &gt;&gt; ~/.bashrc &amp;&amp; source ~/.bashrc# 如果使用zsh:echo "source /usr/share/autojump/autojump.zsh" &gt;&gt; ~/.zshrc &amp;&amp; source ~/.zshrc 使用autojump假如我曾经访问过/mnt/c/Users/user/gaianote.github.io，那么直接执行： 1j gaianote # 全称或者部分名称 便可以快捷的跳转到相应的目录了 zsh 与 antigen 目前常用的 Linux 系统和 OS X 系统的默认 Shell 都是 bash，但是真正强大的 Shell 是深藏不露的 zsh， 这货绝对是马车中的跑车，跑车中的飞行车，史称『终极 Shell』，但是由于配置过于复杂，所以初期无人问津，很多人跑过来看看 zsh 的配置指南，什么都不说转身就走了。直到有一天，国外有个穷极无聊的程序员开发出了一个能够让你快速上手的zsh项目，叫做 oh my zsh这玩意就像「X天叫你学会 C++」系列，可以让你神功速成，而且是真的。 但是oh my zsh 仍然需要很多配置,如果希望更简单的使用，推荐使用zsh 的包管理器： antigen来管理所有功能 安装zsh并启用1sudo apt-get -y install zsh 设置终端的 shell 环境默认为 zsh，输入以下命令(需要重启) 12# 加 sudo 是修改 root 帐号的默认 shellchsh -s `which zsh` 如果上面命令无效，修改 ~/.bashrc 在开头输入： 123if [ -t 1 ]; then exec zshfi 安装antigen并启用12345curl -L git.io/antigen &gt; antigen.zsh# 修改配置 ~/.zshrc（如果切换帐号后无法使用 zsh 则把该用户的配置文件再配一遍）curl -L https://raw.githubusercontent.com/skywind3000/vim/master/etc/zshrc.zsh &gt; ~/.zshrc# 使用 zsh 主题 robbyrussellantigen theme robbyrussell 通常我们希望默认启用antigen theme,而不是每次打开终端都需要重新设置,此时只需要vi ~/.zshrc:在设置文件antigen apply之前添加一行antigen theme robbyrussell即可。 123antigen theme robbyrussell# Tell Antigen that you&apos;re done.antigen apply 更多的主题可以到oh-my-zsh项目中查看 一些问题输入zsh,启动zsh时会提示: zsh compinit: insecure directories, run compaudit for list.Ignore insecure directories and continue [y] or abort compinit [n]? 在stackoverflow中找到了解决办法： setting the current user as the owner of all the directories/subdirectories/files in cause: 1compaudit | xargs chown -R "$(whoami)" removing write permissions for group/others for the files in cause: 1compaudit | xargs chmod go-w 终端Hyperwindows下比较出名的终端当属cmder了，但是我在使用过程中遇到许多问题，最主要的一个就是复制多行文本时,会变成一行。机缘巧合下了解了hyper，尝试几次后便喜欢上了这款终端。 设置默认打开bash而不是win cmd 选择≡ -&gt; Edit -&gt; Prefrence打开配置文件 找到以下文字，将shell:&#39;&#39;改为&#39;C:\\Windows\\System32\\bash.exe&#39; 12345678910// Windows// - Make sure to use a full path if the binary name doesn&apos;t work// - Remove `--login` in shellArgs//// Bash on Windows// - Example: `C:\\Windows\\System32\\bash.exe`//// PowerShell on Windows// - Example: `C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\powershell.exe`shell: &apos;C:\\Windows\\System32\\bash.exe&apos;, 设置默认打开路径在linux系统中，.bashrc的第一行添加cd ~即可 windows子系统下ls难看的底色问题ls 颜色设置一个在实际使用中一个比较困扰我的问题是在bash on windows这边，使用ls显示的目录大都有很难看的底色；而对于正常的linux环境下，则没有这个问题； 这个的原因其实是因为windows的目录在linux这边的权限不同，导致windows的目录在ls的默认颜色中就应该有那种底色；所以只要修改环境变量$LS_COLORS就可以了 具体的不同：在windows下，目录都是ow，即Directory that is other-writable (o+w) and not sticky；而正常linux系统下的目录都是di默认的$LS_COLORS如下： 1rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lz=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.rar=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.axa=00;36:*.oga=00;36:*.spx=00;36:*.xspf=00;36: 可以看到其中ow的配色是34;42,其中那个42就是指定颜色的底色的意思，我们只要在配置中删掉它就可以啦~修改方式，还是在~/.bashrc文件里加上相关设置，把它放到开头,然后执行source命令就可以了=，=我的修改示例： 123# for ls colorsLS_COLORS=&quot;rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=01;34:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.axa=00;36:*.oga=00;36:*.spx=00;36:*.xspf=00;36:&quot;export LS_COLORS 样式列表样式： 00 — Normal (no color, no bold)01 — Bold //粗体 文字颜色 30 — Black //黑色31 — Red //红色32 — Green //绿色33 — Yellow //黄色34 — Blue //蓝色35 — Magenta //洋红色36 — Cyan //蓝绿色37 — White //白色 背景颜色 40 — Black41 — Red42 — Green43 — Yellow44 — Blue45 — Magenta46 — Cyan47 – White 白色： 表示普通文件蓝色： 表示目录绿色： 表示可执行文件红色： 表示压缩文件蓝绿色： 链接文件红色闪烁：表示链接的文件有问题黄色： 表示设备文件灰色： 表示其他文件 文字大小最新版本(截止至1808月)的hyper有个bug,使用插件verminal的时候无法改变fontSize，字体显得非常小，删掉它之后就可以正常工作了。 参考文档Windows Subsystem for Linux使用全记录windows命令行工具 Hyper全面介绍]]></content>
      <tags>
        <tag>windows linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在ubantu16.04上搭建shadowsocks服务]]></title>
    <url>%2F2018%2F08%2F15%2Flinux%2F3.%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%BB%B4%2F%E5%9C%A8ubantu16-04%E4%B8%8A%E6%90%AD%E5%BB%BAshadowsocks%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[以前我们科学上网的时候最常用的就是vpn了，而2年前，ss被开源(ss出现一年后，开源社区的破娃小姐姐在ss的基础上发布了ssr)，现在已经是最流行的科学上网方案。 vpn与shadowsocks什么是vpn在很多人心目中就是用来翻墙的工具，其实不是。vpn最主要的功能，并不是用来翻墙，只是它可以达到翻墙的目的。 vpn指虚拟专用网络，它的功能是：在公用网络上建立专用网络，进行加密通讯。在企业网络和高校的网络中应用很广泛。你接入vpn，其实就是接入了一个专有网络，你的网络访问都从这个出口出去，你和vpn之间的通信是否加密，取决于你连接vpn的方式或者协议。 ss与ssrss作者是clowwindy，大约两年前，他自己为了翻墙写了shadowsocks，简称ss或者叫影梭，后来他觉得这个东西非常好用，速度快，而且不会被封锁，他就把源码共享在了github上，然后就火了，但是后来作者被请去喝茶，删了代码，并且保证不再参与维护更新。现在这个好像是一个国外的大兄弟在维护。 ssr：在ss作者被喝茶之后，github上出现了一个叫breakwa11(破娃)的帐号，声称ss容易被防火墙检测到，所以在混淆和协议方面做了改进，更加不容易被检测到，而且兼容ss，改进后的项目叫shadowsocks-R，简称ssr，然后ss用户和ssr用户自然分成了两个派别，互相撕逼，直到前阵子，破娃被人肉出来，无奈之下删除了ssr的代码，并且解散了所有相关群组。 ss和ssr它的原理都是一样的，就是socks5代理。socks代理只是简单的传递数据包，而不必关心是何种协议，所以socks代理比其他应用层代理要快的多。socks5代理是把你的网络数据请求通过一条连接你和代理服务器之间的通道，由服务器转发到目的地，这个过程中你是没有通过一条专用通道的，只是数据包的发出，然后被代理服务器收到，整个过程并没有额外的处理。通俗的说，现在你有一个代理服务器在香港，比如你现在想要访问google，你的电脑发出请求，流量通过socks5连接发到你在香港的服务器上，然后再由你在香港的服务器去访问google，再把访问结果传回你的电脑，这样就实现了翻墙。 直连模式就是流量不走代理 ，PAC模式简单说就是国内地址不走代理，国外走代理，全局模式就是不管国内国外，所有流量通过代理服务器访问 vpn和ss/ssr的区别和优缺点通过上面的介绍，其实基本已经能看出vpn和ss/ssr的区别了，那么他们到底孰优孰劣。 因为vpn是走的专用通道，它是用来给企业传输加密数据用的，所以vpn的流量特征很明显，以openvpn为例，更详细的在这里不说了，流量特征明显，防火墙直接分析你的流量，如果特征匹配，直接封掉。目前就翻墙来说，PPTP类型的vpn基本死的差不多了，L2TP大部分地区干扰严重很不稳定。 ss/ssr的目的就是用来翻墙的，而vpn的目的是用来加密企业数据的，对于vpn来说安全是第一位的，而对于ss/ssr来说穿透防火墙是第一位，抗干扰性强，而且对流量做了混淆，所有流量在通过防火墙的时候，基本上都被识别为普通流量，也就是说你翻墙了，但是政府是检测不到你在翻墙的。两者的出发点和着重点就不同，ss/ssr更注重流量的混淆加密。如果要安全匿名上网，可以用vpn+tor或者ss/ssr+tor。 而安全性方面还要补充的一点就是，国内vpn服务商，政府是很容易拿到他们的服务器日志的，如果他们真的这样做了，你翻墙做了什么，一览无余 云主机提供商VultrVultr官网是一家提供日本、美国、欧洲等多个国家和地区机房的VPS主机商，硬盘都是采用SSD，VPS主机都是KVM架构，VPS配置最少的内存512MB、硬盘为15GB的VPS只要2.5美元/月，vultr是根据VPS使用小时来计费（0.007/h,折合人民币4分6/小时）的，使用多长时间就算多长时间，计费对应的款。Vultr是KVM系统架构，目前已开通15个机房，比较适合国内的是日本东京（tokyo）,美国洛杉矶（ Los Angeles ），美国西雅图（ Seattle ）这三个机房相对国内线路较好。vultr支持支付宝（Alipay）付费使用。 使用要点： Vultr注册时的邮箱必须是真实有效的，后续开通主机需要验证邮箱 不要选择2.5美元/月的主机，这个主机只支持ipv6，目前大部分网站和网络运营商都是不支持的 通过http://www.pingms.com/查看各个云主机到本地的延迟，东京一般情况是延迟最低的 shadowsocks 服务器安装首先选择os版本 ubantu16.04 , 本文是根据这个系统及版本部署的。 更新软件源 1sudo apt-get update 然后安装 PIP 环境 12sudo apt-get install python-setuptoolssudo apt-get install python-pip 安装 shadowsocks 1sudo pip install shadowsocks 运行 shadowsocks 服务器命令行直接启动启动命令如下：如果要停止运行，将命令中的start改成stop。 1234sudo ssserver -p 8388 -k password -m rc4-md5 -d start# -p 端口# -k 密码# -m 加密方式 停止 1sudo ssserver -d stop 使用配置文件启动也可以使用配置文件进行配置，方法创建/etc/shadowsocks.json文件，填入如下内容： 12345678910&#123; "server":"0.0.0.0", "server_port":8388, "local_address": "127.0.0.1", "local_port":1080, "password":"mypassword", "timeout":300, "method":"rc4-md5", "fast_open":true &#125; 各字段的含义： 字段 含义 server 服务器 IP (IPv4/IPv6)，注意这也将是服务端监听的 IP 地址 server_port 服务器端口 local_port 本地端端口 password 用来加密的密码 timeout 超时时间（秒） method 加密方法，可选择 “bf-cfb”, “aes-256-cfb”, “des-cfb”, “rc4″, 等等。 加密方式推荐使用rc4-md5，因为 RC4 比 AES 速度快好几倍，如果用在路由器上会带来显著性能提升。旧的 RC4 加密之所以不安全是因为 Shadowsocks 在每个连接上重复使用 key，没有使用 IV。现在已经重新正确实现，可以放心使用。更多可以看 issue。 如果需要配置多个用户,可以这样来设置: 12345678910&#123; "server":"my_server_ip", "port_password": &#123; "端口1": "密码1", "端口2": "密码2" &#125;, "timeout":300, "method":"aes-256-cfb", "fast_open": true&#125; 然后使用配置文件在后台运行： 1sudo ssserver -c /etc/shadowsocks.json -d start ipv6的支持VPS 开启 IPv6以 Vultr 为例，其他 VPS 类似。 1.已创建 VPS 开启 IPv6 进入控制面板，选择 “Settings” ——&gt; “IPv6” ——&gt; “Assign Ipv6 Network” ，然后重启 VPS 即可。 2.新创建VPS开启IPv6 在创建 VPS 时，在第四项的 “Enable IPv6” 前打勾即可创建一个开启 IPv6 的机器。 3. 开启后同样前往 “Settings” 的 “IPv6” 中即可查看到 IPv6 地址。 SS 配置 IPv6 支持打开 SS 配置文件 vi /etc/shadowsocks.json|1|vi/etc/shadowsocks.json| 编辑配置文件，使 ss 支持 IPv4 和 IPv6 地址 12345&#123; "server":"::", //同时支持 IPv4 和 IPv6 "port_password": "mypassword", // ...&#125; Esc，输入 :wq 保存退出。重启 SS 即可。 客户端设置客户端的设置和以前一样，只不过把 IP 地址改为 IPv6 的地址即可。 在使用时，可以先用 IPv4 的地址进行测试，看能不能上 Google 等，如果正常，再换位 IPv6 的地址进行测试。 要先查看自己的网络是否支持 IPv6，右键任务栏“win10设置” ——&gt; “网络和Internet”——&gt; “以太网” ——&gt;“打开网络共享中心” ，点击 “宽带连接”（拨号上网）或 “以太网”（路由器上网），查看网络状态，看 IPv6 是否有 Internet 连接。 没有的话可以点击下面的“属性” ——&gt; “网络”，查看是否勾选 “IPv6” Internet 协议，一般如果已经勾选，网络支持 IPv6 会自动启用，如果勾选了 IPv6 仍不可用，可以咨询运营商是否支持。不支持的话就没办法了。 配置开机自启动 编辑 /etc/rc.local 文件 1sudo vi /etc/rc.local 在 exit 0 这一行的上边加入如下 1/usr/local/bin/ssserver –c /etc/shadowsocks.json 或者 不用配置文件 直接加入命令启动如下： 1/usr/local/bin/ssserver -p 8388 -k password -m aes-256-cfb -d start 到此重启服务器后，会自动启动。 安装 TCP BBRBBR 是来自 Google 的一个 TCP 拥塞控制算法，单边加速，可以提升你的网络利用率。 确认你可以使用 BBR直接运行以下命令即可： 1wget --no-check-certificate -qO &apos;BBR.sh&apos; &apos;https://moeclub.org/attachment/LinuxShell/BBR.sh&apos; &amp;&amp; chmod a+x BBR.sh &amp;&amp; bash BBR.sh -f 脚本会自动安装并重启 此脚本运行时会自动选择最新的非rc版本（非候选发布版）内核进行安装，并且自动卸载旧内核（无需人工干预）。安装完成后，执行以下命令： 1lsmod | grep &apos;bbr&apos; 如果结果不为空，则说明成功开启了 BBR，那么你就可以使用后续的 BBR 加强版。 修改/关闭bbr方法 使用root用户登录，运行以下命令： 1vim /etc/sysctl.conf 删除或注释掉其中的两行： 12# net.core.default_qdisc = fq 用#注释掉# net.ipv4.tcp_congestion_control = bbr 用#注释掉 执行命令： 1sysctl -p 最后重启服务器生效！ 客户端下载ios客户端：搜索windy，需要日区的apple id，因为中国区没有上架，日区美区都可以pc以及其它客户端: 建议到shadowsocks.org进行下载，或者github进行下载 使用电脑打开客户端，将上面记录的相应连接信息填入客户端，确定。 右键任务栏托盘小飞机图标，“启动”，可以选择合适的代理模式。 PAC：只代理国外网站； 全局：所有网站都通过SS。 其它系统客户端的设置基本如此 网络测试对我而言还是推荐bbr的加速方式 测试方法秉承着怎么方便怎么来的做法，我在CentOS 6.9系统下进行什么都不装和锐速的网络测试，在Ubuntu 16.04下进行BBR和BBR魔改的网络测试 网络测试有两部分： 使用ZBench-CN.sh进行测试 在深圳天威联通100M网络环境下进行H5网页测速 用到的代码： 123456789101112# CentOS 6.9 锐速wget --no-check-certificate -O appex.sh https://raw.githubusercontent.com/0oVicero0/serverSpeeder_Install/master/appex.sh &amp;&amp; chmod +x appex.sh &amp;&amp; bash appex.sh install '2.6.32-642.el6.x86_64'# Ubuntu/Debian BBRwget --no-check-certificate -qO 'BBR.sh' 'https://moeclub.org/attachment/LinuxShell/BBR.sh' &amp;&amp; chmod a+x BBR.sh &amp;&amp; bash BBR.sh -f# Ubuntu/Debian BBR魔改wget --no-check-certificate -qO 'BBR_POWERED.sh' 'https://moeclub.org/attachment/LinuxShell/BBR_POWERED.sh' &amp;&amp; chmod a+x BBR_POWERED.sh &amp;&amp; bash BBR_POWERED.sh# ZBench-CN.sh 测速脚本wget https://raw.githubusercontent.com/FunctionClub/ZBench/master/ZBench-CN.sh &amp;&amp; bash ZBench-CN.sh# Docker 一键安装脚本wget -qO- https://get.docker.com/ | sh# H5网页测速 Dockerdocker run -d -p 2333:80 ilemonrain/html5-speedtest:latest 测试结果Vultr 日本（低延迟低丢包）H5网页测速 什么都不装 锐速 BBR BBR魔改 测速脚本 什么都不装 锐速 BBR BBR魔改 参考链接[小实验] 锐速&amp;BBR究竟哪家强？个人PC有必要上锐速吗？Vpn与ss/ssr的区别vultr-vps-搭建-shadowsocks（ss）教程（新手向）]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows美化与效率工具指南]]></title>
    <url>%2F2018%2F08%2F13%2Fwindows%2Fwindows%E7%BE%8E%E5%8C%96%E4%B8%8E%E6%95%88%E7%8E%87%E5%B7%A5%E5%85%B7%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[Apple 是一家标榜设计的公司，macOS 的桌面美学确实让包括我在内的很多人赞叹。当然，经过 Fluent Design 重新设计的 Windows 10 也相比之前的老一代 Windows 有着巨大的美学改善。进一步借助下面这些小工具，我找回了不输 macOS 的美观桌面。 设计桌面① TranslucentTB：开源的任务栏透明工具&nbsp;美化桌面的步骤中，必不可少修改任务栏，改掉它不透明的生硬效果。有人会选择修改注册表，这不仅很危险，还只能将任务栏半透明化，而另外一些如 StarDock 的美化软件又很占内存。这个开源的 TranslucentTB 则不然，既小巧、不占内存、还可以将任务栏透明、半透明、模糊、或是直接显示纯色，可以说是功能全面了。显示效果就像上图那样，能够完整的呈现我们的壁纸而不必担心任务栏挡掉一部分。 TranslucentTB源码文件需要自己编译，所以直接下载release版本即可 ② Rainmeter：Windows 桌面美化插件对于深度美化 Windows 桌面的同学来说，Rainmeter 的重要性可想而知。在 Rainmeter 的 官网、官方用户社区、和 DeviantArt 设计站点，都有大量设计精美的 Rainmeter 插件，但是我也并不推荐将插件直接堆砌在桌面上，一团糟的样子不仅不美观，还占用了大量的系统资源。我只小小利用了其桌面时钟、日期的显示。 我目前使用的 Rainmeter 主题在这里可以下载到 → Rainmeter Elegance 2.0 ③ Simple Desktops：壁纸提供站美化桌面方面，壁纸可以说是重中之重。一张简单可爱的壁纸可以奠定整个工作的态度。在 Simple Desktops 里我总能找到可爱的壁纸。 Simple Desktop除此之外，无版权图片社区 Unsplash、免费图片社区 Pixabay、甚至是必应每日壁纸等等都是优秀的壁纸来源。 系统工具① launchy Launchy：自由的跨平台工具，帮助你忘记开始菜单、桌面图标甚至文件管理器。 经过简单的设置，便可以将你希望搜寻的文件夹，可执行文件，或者图片等任意你希望快速寻找到的文件录入launchy，以进行快速打开/启动，设置方法如下所示： ② QuickLook ：空格键预览工具QuickLookmacOS 上的「一指禅」—— 空格预览文件内容，绝对是一个经典、令人印象深刻的功能。在 Windows 上，QuickLook 可以实现空格预览的功能，并且支持的格式也很丰富，能够预览图片、文本、docx 文档、甚至是各种源码等等一系列文件。macOS 一指禅在 Windows 上也能完美践行了。🦄 空格预览图片、文本和 MarkdownQuickLook 可以直接在 Windows 应用商店免费下载得到。 ③&nbsp;快贴：云剪贴板工具快贴是一个免费的跨平台的剪贴板同步工具，能够在多端设备同步剪贴板，并对剪贴板涉密内容进行自动识别、加密传输。 快贴我在 iOS 和 Windows 端同时下载了快贴，在 iOS 保持后台运行的情况下，我在手机上复制的内容，能够很快的同步到云端，进而在 Windows 端能够粘贴。但是这个过程比 macOS 繁琐的地方在于，我需要通过全局快捷键来手动触发粘贴端的同步，这样的多一步操作虽说影响体验，也肯定不如 macOS 闭源的生态系统好，但是至少我不必用微信、QQ 之流当作我电脑与手机沟通的渠道了。 参考：好用的全平台剪切板工具，我们为你找到了这&nbsp;3&nbsp;款 ④ Send Anywhere：文件传输工具Send Anywhere 将文件上传到一个 p2p 网络上面，并非其服务器上，接受端通过随机六位接收码进行文件接受。这样的传输方式保证了文件的安全性和完整性，又能有相当的上传、下载和传输速度。Send Anywhere 可以说是全平台 Airdrop 了。 Send Anywhere同时 Send Anywhere 有设备记忆功能，在曾经传输过文件的设备上，下一次传输的时候，六位接收码都不必输入。这样的分享文件的特性可以说是跨平台的救命稻草了。我在使用过程中除了在 Windows 平台传输结束之后 Send Anywhere 本身有几次会卡死，其他体验都极佳。 参考：免费全平台的文件分享利器：SendAnywhere 效率工具Snipaste：截图工具SnipasteSnipaste&nbsp;着实是 Windows 上最好的截图工具。但 Snipaste 除了我们常见的截图标注、窗口检测、全局快捷键等等简朴必备技能，还有贴图、取色等等高阶可玩性。目前 Snipaste 在 Windows 商店就可以下载得到，并且就在最近几天 Snipaste 也发布了 Pro 版本，增加了更多的玩法。 Snipaste In ActionSnipaste 的开发者对 Snipaste 很是上心，我派单独采访了这位同学，在这里有更加具体的介绍 →&nbsp;幕后丨他做了最强免费「截图」工具 Snipaste 后，还有上万字的话想说 上面介绍的工具中，除了 Typora 和图床上传工具 smpic 以外（smpic 因为免安装，所以不支持开机自启动），剩下的都有幸被我加入开机自启动名单。🎉 其它软件解压缩软件Bandizip for Windows 开发这个软件的公司叫 Bandisoft，是一家挺低调的韩国软件公司，正是知名录屏软件 Bandicam 的初开发公司（后来由 Bandicam 公司接手开发） 优点是界面清爽，功能全面，没有广告和弹窗，国内竞品好压之类广告弹窗着实烦人。 简单FTP Server单文件程序，大小仅400多K,点击此处下载 软件截图 用法可以设定访问用户名密码，设置权限，自定义端口，限制最大连接数 共享目录即为 FTP 根目录 开启后右上角“×”关掉即可最小化到任务栏图标 然后访问ftp://你的内网IP:服务端口即可，如果服务端口是默认的21，那么直接访问ftp://你的内网IP即可 建议在windows资源管理器中访问，这样就可以直接跟在本地复制粘贴一样，批量下载文件 下载工具IDMInternet Download Manager（简称IDM）是一个用于Windows系统的下载管理器，它是共享软件，免费试用期为30天，但是每月均有一段时间优惠。 IDM可以让用户自动下载某些类型的文件，它可将文件划分为多个下载点以更快下载，并列出最近的下载，方便访问文件。相对于其它下载管理器，它具有独特的动态档案分区技术。 它的出色之处在于，能分析下载地址中的实际地址，但若地址无效时则跳换成失效网页，当地址重新有效而无法恢复成原来的地址。 代理工具ccproxy服务端设置 ccproxy在代理机安装ccproxy,打开设置,勾选除Nat和远程拨号的所有选项后即可 客户端设置 在chrome中搜索代理，打开代理设置 win10中设置代理,如果你需要使用代理访问192.*,注意修改第二个方框中的匹配规则 两个代理同时修改后，代理设置生效 参考文档Mac To Win | 不完全迁移体验指北LOLI NIKO]]></content>
      <tags>
        <tag>windows</tag>
        <tag>软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[同时使用有线网上内网、无线网上外网]]></title>
    <url>%2F2018%2F08%2F13%2Fwindows%2F%E5%90%8C%E6%97%B6%E4%BD%BF%E7%94%A8%E6%9C%89%E7%BA%BF%E7%BD%91%E4%B8%8A%E5%86%85%E7%BD%91%E3%80%81%E6%97%A0%E7%BA%BF%E7%BD%91%E4%B8%8A%E5%A4%96%E7%BD%91%2F</url>
    <content type="text"><![CDATA[在真实的工作环境中，通常有一台连接外网的笔记本A，和一台物理格隔绝台式机B.查询或下载资料时，使用主机A切换外网进行操作，操作完成后切换网络到局域网，将资料传输给主机B。频繁的网络切换比较繁琐。如何让主机A同时连接外网和内网，而无需切换网络环境呢? 假如192.168.71.58是我们需要访问的内网地址，我们只需进行如下操作即可： 首先，主机A连接好内网和外网的网络环境（局域网WiFi，宽带和蓝牙混合，无法同时连接两个WiFi） 管理员权限执行ipconfig，查看网关和它们的默认网关 输入以下命令（替换为自己的网关） 1234route delete 0.0.0.0route add 0.0.0.0 mask 0.0.0.0 172.20.10.1route add 192.168.71.0 mask 255.255.255.0 192.168.1.1 上面的命令指，对于所有IP地址的访问，都从172.20.10.1走；对于192.168.71开头的地址的访问，从192.168.1.1网关走]]></content>
      <tags>
        <tag>网络连接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百喻经·木柱]]></title>
    <url>%2F2018%2F08%2F10%2F%E9%9A%8F%E7%AC%94%2F%E7%99%BE%E5%96%BB%E7%BB%8F%C2%B7%E6%9C%A8%E6%9F%B1%2F</url>
    <content type="text"><![CDATA[大夜弥天，夜颇自得，作大言曰: “ 万物皆备于我，舍我岂有万物哉!”是时，一小木柱自燃其身，灿烂放光明。夜为之辟易，退三舍之地，然犹讥木柱曰:“自焚其身，以照百物，尔何愚也!”木柱作答:“ 碌碌而生，何若烈烈而死。投一光明，破弥天夜幕，吾何憾焉!”夜嘿然无以对。 尔时，万千木柱，闻是言语，齐发大心愿，皆自燃其身，放大光明，光射斗牛，遍烛万物]]></content>
      <tags>
        <tag>文摘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网页各种性能指标分析]]></title>
    <url>%2F2018%2F08%2F10%2F%E5%89%8D%E7%AB%AF%2F%E7%BD%91%E9%A1%B5%E5%90%84%E7%A7%8D%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87%E5%88%86%E6%9E%90%E4%B8%8Ewindow-performance%2F</url>
    <content type="text"><![CDATA[首屏和白屏时间影响着用户的体验，如果超过3秒网站还没有加载完成的话，用户可能就会关闭网页浏览其它内容了。因此，网站前端性能调优就显得尤为重要。 一般我们可以通过浏览器的调试工具-网络面板，或者代理工具查看网页加载过程中的各个阶段的耗时。而利用window.performance属性则可以获得更为精确的原始数据，以毫秒为单位，精确到微秒。 1. 什么是首屏和白屏时间？白屏时间是指浏览器从响应用户输入网址地址，到浏览器开始显示内容的时间。首屏时间是指浏览器从响应用户输入网络地址，到首屏内容渲染完成的时间。 白屏时间 = 地址栏输入网址后回车 - 浏览器出现第一个元素首屏时间 = 地址栏输入网址后回车 - 浏览器第一屏渲染完成 影响白屏时间的因素：网络，服务端性能，前端页面结构设计。影响首屏时间的因素：白屏时间，资源下载执行时间。 以百度为例，将 chrome 网速调为 Fast 3G，然后打开 Performance 工具，点击 “Start profiling and reload page” 按钮，查看 Screenshots 如下图： 2. 白屏时间通常认为浏览器开始渲染 &lt;body&gt; 或者解析完 &lt;head&gt; 的时间是白屏结束的时间点。 1234567891011121314151617181920212223&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;白屏&lt;/title&gt; &lt;script&gt; // 不兼容 performance.timing 的浏览器 window.pageStartTime = Date.now() &lt;/script&gt; &lt;!-- 页面 CSS 资源 --&gt; &lt;link rel="stylesheet" href="xx.css"&gt; &lt;link rel="stylesheet" href="zz.css"&gt; &lt;script&gt; // 白屏结束时间 window.firstPaint = Date.now() // 白屏时间 console.log(firstPaint - performance.timing.navigationStart) &lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;Hello World&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 白屏时间 = firstPaint - performance.timing.navigationStart || pageStartTime 3. 首屏时间关于首屏时间是否包含图片加载网上有不同的说法，个人认为，只要首屏中的图片加载完成，即是首屏完成，不在首屏中的图片可以不考虑。 计算首屏时间常用的方法有： (1) 首屏模块标签标记法 由于浏览器解析 HTML 是按照顺序解析的，当解析到某个元素的时候，你觉得首屏完成了，就在此元素后面加入 script 计算首屏完成时间。 123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;首屏&lt;/title&gt; &lt;script&gt; // 不兼容 performance.timing 的浏览器 window.pageStartTime = Date.now() &lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;!-- 首屏可见内容 --&gt; &lt;div class=""&gt;&lt;/div&gt; &lt;!-- 首屏可见内容 --&gt; &lt;div class=""&gt;&lt;/div&gt; &lt;script type="text/javascript"&gt; // 首屏屏结束时间 window.firstPaint = Date.now() // 首屏时间 console.log(firstPaint - performance.timing.navigationStart) &lt;/script&gt; &lt;!-- 首屏不可见内容 --&gt; &lt;div class=""&gt;&lt;/div&gt; &lt;!-- 首屏不可见内容 --&gt; &lt;div class=""&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; (2) 统计首屏内加载最慢的图片/iframe 通常首屏内容中加载最慢的就是图片或者 iframe 资源，因此可以理解为当图片或者 iframe 都加载出来了，首屏肯定已经完成了。 由于浏览器对每个页面的 TCP 连接数有限制，使得并不是所有图片都能立刻开始下载和显示。我们只需要监听首屏内所有的图片的 onload 事件，获取图片 onload 时间最大值，并用这个最大值减去 navigationStart 即可获得近似的首屏时间。 123456789101112131415161718192021222324&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;首屏&lt;/title&gt; &lt;script&gt; // 不兼容 performance.timing 的浏览器 window.pageStartTime = Date.now() &lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;img src="https://lz5z.com/assets/img/google_atf.png" alt="img" onload="load()"&gt; &lt;img src="https://lz5z.com/assets/img/css3_gpu_speedup.png" alt="img" onload="load()"&gt; &lt;script&gt; function load () &#123; window.firstScreen = Date.now() &#125; window.onload = function () &#123; // 首屏时间 console.log(window.firstScreen - performance.timing.navigationStart) &#125; &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; Performance APIPerformance 接口可以获取到当前页面与性能相关的信息。 (1) Performance.timing 在 chrome 中查看 performance.timing 对象： 与浏览器对应的状态如下图： 左边红线代表的是网络传输层面的过程，右边红线代表了服务器传输回字节后浏览器的各种事件状态，这个阶段包含了浏览器对文档的解析，DOM 树构建，布局，绘制等等。 navigationStart: 表示从上一个文档卸载结束时的 unix 时间戳，如果没有上一个文档，这个值将和 fetchStart 相等。 unloadEventStart: 表示前一个网页（与当前页面同域）unload 的时间戳，如果无前一个网页 unload 或者前一个网页与当前页面不同域，则值为 0。 unloadEventEnd: 返回前一个页面 unload 时间绑定的回掉函数执行完毕的时间戳。 redirectStart: 第一个 HTTP 重定向发生时的时间。有跳转且是同域名内的重定向才算，否则值为 0。 redirectEnd: 最后一个 HTTP 重定向完成时的时间。有跳转且是同域名内部的重定向才算，否则值为 0。 fetchStart: 浏览器准备好使用 HTTP 请求抓取文档的时间，这发生在检查本地缓存之前。 domainLookupStart/domainLookupEnd: DNS 域名查询开始/结束的时间，如果使用了本地缓存（即无 DNS 查询）或持久连接，则与 fetchStart 值相等 connectStart: HTTP（TCP）开始/重新 建立连接的时间，如果是持久连接，则与 fetchStart 值相等。 connectEnd: HTTP（TCP） 完成建立连接的时间（完成握手），如果是持久连接，则与 fetchStart 值相等。 secureConnectionStart: HTTPS 连接开始的时间，如果不是安全连接，则值为 0。 requestStart: HTTP 请求读取真实文档开始的时间（完成建立连接），包括从本地读取缓存。 responseStart: HTTP 开始接收响应的时间（获取到第一个字节），包括从本地读取缓存。 responseEnd: HTTP 响应全部接收完成的时间（获取到最后一个字节），包括从本地读取缓存。 domLoading: 开始解析渲染 DOM 树的时间，此时 Document.readyState 变为 loading，并将抛出 readystatechange 相关事件。 domInteractive: 完成解析 DOM 树的时间，Document.readyState 变为 interactive，并将抛出 readystatechange 相关事件，注意只是 DOM 树解析完成，这时候并没有开始加载网页内的资源。 domContentLoadedEventStart: DOM 解析完成后，网页内资源加载开始的时间，在 DOMContentLoaded 事件抛出前发生。 domContentLoadedEventEnd: DOM 解析完成后，网页内资源加载完成的时间（如 JS 脚本加载执行完毕）。 domComplete: DOM 树解析完成，且资源也准备就绪的时间，Document.readyState 变为 complete，并将抛出 readystatechange 相关事件。 loadEventStart: load 事件发送给文档，也即 load 回调函数开始执行的时间。 loadEventEnd: load 事件的回调函数执行完毕的时间。 1234567891011121314151617181920212223242526// 计算加载时间function getPerformanceTiming() &#123; var t = performance.timing var times = &#123;&#125; // 页面加载完成的时间，用户等待页面可用的时间 times.loadPage = t.loadEventEnd - t.navigationStart // 解析 DOM 树结构的时间 times.domReady = t.domComplete - t.responseEnd // 重定向的时间 times.redirect = t.redirectEnd - t.redirectStart // DNS 查询时间 times.lookupDomain = t.domainLookupEnd - t.domainLookupStart // 读取页面第一个字节的时间 times.ttfb = t.responseStart - t.navigationStart // 资源请求加载完成的时间 times.request = t.responseEnd - t.requestStart // 执行 onload 回调函数的时间 times.loadEvent = t.loadEventEnd - t.loadEventStart // DNS 缓存时间 times.appcache = t.domainLookupStart - t.fetchStart // 上个页面卸载的时间 times.unloadEvent = t.unloadEventEnd - t.unloadEventStart // TCP 建立连接完成握手的时间 times.connect = t.connectEnd - t.connectStart return times&#125; (2) Performance.navigation redirectCount: 0 // 页面经过了多少次重定向 type: 00 表示正常进入页面； 1 表示通过 window.location.reload() 刷新页面； 2 表示通过浏览器前进后退进入页面； 255 表示其它方式 (3) Performance.memory jsHeapSizeLimit: 内存大小限制 totalJSHeapSize: 可使用的内存 usedJSHeapSize: JS 对象占用的内存 DOMContentLoaded vs load(1) DOMContentLoaded 是指页面元素加载完毕，但是一些资源比如图片还无法看到，但是这个时候页面是可以正常交互的，比如滚动，输入字符等。 jQuery 中经常使用的 $(document).ready() 其实监听的就是 DOMContentLoaded 事件。 (2) load 是指页面上所有的资源（图片，音频，视频等）加载完成。jQuery 中 $(document).load() 监听的是 load 事件。 12345678910111213141516171819202122232425262728293031// loadwindow.onload = function () &#123;&#125;// DOMContentLoadedfunction ready (fn) &#123; if (document.addEventListener) &#123; document.addEventListener('DOMContentLoaded', function () &#123; document.removeEventListener('DOMContentLoaded', arguments.callee, false) fn() &#125;, false) &#125; // 如果 IE else if (document.attachEvent) &#123; // 确保当页面是在iframe中加载时，事件依旧会被安全触发 document.attachEvent('onreadystatechange', function() &#123; if (document.readyState == 'complete') &#123; document.detachEvent('onreadystatechange', arguments.callee) fn() &#125; &#125;) // 如果是 IE 且页面不在 iframe 中时，轮询调用 doScroll 方法检测DOM是否加载完毕 if (document.documentElement.doScroll &amp;amp;&amp;amp; typeof window.frameElement === 'undefined') &#123; try &#123; document.documentElement.doScroll('left') &#125; catch(error) &#123; return setTimeout(arguments.callee, 20) &#125; fn() &#125; &#125;&#125; 优化方案1.DNS寻址时间：t.domainLookupEnd - t.domainLookupStart。 优化方法：检查页面是否添加了DNS预解析代码。 是否合理利用域名发散与域名收敛的策略。 2.TCP连接耗时：t.connectEnd - t.connectStart。 3 首包时间: t.responseStart - t.navigationStart。 优化方法：是否加cdn，数据可否静态化等。 4.request请求耗时：t.responseEnd - t.requestStart。 优化方法：返回内容是否已经压缩过，静态资源是否打包好等。 5.白屏时间。 白屏时间是最影响用户体验的，时间越久，用户等待就越久。 6.解析DOM树结构的时间：t.domComplete - t.domLoading。 优化方法：检查dom节点是否过多，dom是否嵌套过深。 7.页面加载完成的时间：t.loadEventEnd - t.fetchStart。 优化方法：考虑延迟加载，懒加载，部分加载，减少首屏渲染时间。 方法（JS） performance.getEntries()：每个资源请求的时间数据。 performance.now() 计算网页从performance.timing.navigationStart到当前时间的毫秒数。 精确计算某个操作，或某个方法执行的耗时。 varstart =performance.now(); dosomething();varend =performance.now();vartime = end -start; console.log(‘耗时’ + time + ‘毫秒。’); 或者console.time(‘aa’),console.timeEnd(‘aa’)计算。 performance.mark()给相应的视点做标记。结合performance.measure()使用也可以算出各个时间段的耗时。 如何分析页面整体加载速度 主要是查看指标值PAGET_页面加载时间,此指标指的是页面整体加载时间但不含(onload事件和redirect), 此指标值可直接反应用户体验, 从此项指标可以知道指定某时间段的页面加载速度值,以及和天,周,月的对比状况. 也可以查询指标ALLT_页面完全加载时间, 可以查询到从浏览器开始导航(用户点击链接或在地址栏输入url或点刷新,后退按钮)到页面onload 事件js完全跑完的所有时间. 如果发现页面加载速度有增加或减少,则可以分项查询前面表格中的每个指标值,总的来说他们的关系如下: dom开始加载前所有花费时间=重定向时间+域名解析时间+建立连接花费时间+请求花费时间+接收数据花费时间 pageLoadTime页面加载时间=域名解析时间+建立连接花费时间+请求花费时间+接收数据花费时间+解析dom花费时间+加载dom花费时间 allLoadTime页面完全加载时间=重定向时间+域名解析时间+建立连接花费时间+请求花费时间+接收数据花费时间+解析dom花费时间+加载dom花费时间+执行onload事件花费时间 resourcesLoadedTime资源加载时间=解析dom花费时间+加载dom花费时间 参考文档 Web 性能优化-首屏和白屏时间 如何计算首屏加载时间 7 天打造前端性能监控系统 白屏时间 白屏时间 = 页面开始展示的时间点 - 开始请求时间点 开始请求时间点可以通过performance.timing.navigationStart获得。那么页面开始展示的时间点怎么获取呢。已经知道渲染过程是逐步完成的，而且页面解析是按照文档流从上至下解析的，因此一般认为开始解析body的时间点就是页面开始展示的时间。所以可以通过在head标签的末尾插入script来统计时间节点作为页面开始展示时间节点。但是这种方式需要打点，因此也有很多项目为了简化白屏时间的获取会选择忽略head解析时间直接用来performance.timing.domLoading.表示页面开始展示的时间，即使用performance.timing.domLoading - performance.timing.navigationStart来表示白屏时间。 首屏时间=首屏内容渲染结束时间点-开始请求时间点。 同样开始请求时间点可以通过performance.timing.navigationStart获取。首屏内容渲染结束的时间点通常有以下几种方法获取： 首屏模块标签标记法 适用于于首屏内容不需要通过拉取数据才能生存以及页面不考虑图片等资源加载的情况。通过在 HTML 文档中对应首屏内容的标签结束位置，使用内联的 JavaScript 代码记录当前时间戳作为首屏内容渲染结束的时间点。 统计首屏内加载最慢的图片的时间 通常首屏内容加载最慢的就是图片资源，因此可以把首屏内加载最慢的图片加载完成的时间作为首屏时间。由于浏览器对每个页面的 TCP 连接数有限制，使得并不是所有图片都能立刻开始下载和显示。因此在 DOM树 构建完成后会通过遍历首屏内的所有图片标签，并且监听所有图片标签 onload 事件，最终遍历图片标签的加载时间获取最大值，将这个最大值作为首屏时间。 自定义首屏内容计算法 由于统计首屏内图片完成加载的时间比较复杂。所以在项目中通常会通过自定义模块内容，来简化计算首屏时间。例如忽略图片等资源加载情况，只考虑页面主要 DOM；只考虑首屏的主要模块，而不是严格意义首屏线以上的所有内容。 可交互时间 = 用户可以正常进行事件输入时间点-开始请求时间点。 performance.timing有一个domInteractive属性，代表了DOM结构结束解析的时间点，就是document.readyState属性变为“interactive”]]></content>
      <tags>
        <tag>web前端</tag>
        <tag>javascript</tag>
        <tag>性能测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python与web服务器]]></title>
    <url>%2F2018%2F08%2F10%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E4%B8%8Eweb%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[python 中自带简易的web服务器,首先cd到准备作为服务器的根目录的目录下，然后执行python -m http.server 80命令，即可访问目录内的相关文件。目录下需要index.html作为主页。 在python2中 1python -m SimpleHTTPServer 80 在python3中 1python -m http.server 80]]></content>
  </entry>
  <entry>
    <title><![CDATA[随机的文件名与UUID]]></title>
    <url>%2F2018%2F08%2F09%2Fpython%2F4.python%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%2F%E7%94%A8python%E7%94%9F%E6%88%90%E4%B8%80%E4%B8%AA%E9%9A%8F%E6%9C%BA%E7%9A%84%E6%96%87%E4%BB%B6%E5%90%8D%2F</url>
    <content type="text"><![CDATA[使用uuid来生成一个独一无二的名字不失为一个好办法： 12import uuidfilename = uuid.uuid1().hex # '1540c76e9bc811e88581b2fc364fb8e1' UUID是什么：UUID： 通用唯一标识符 ( Universally Unique Identifier )，对于所有的UUID它可以保证在空间和时间上的唯一性，也称为GUID，全称为： UUID —— Universally Unique IDentifier Python中称为 UUID GUID —— Globally Unique IDentifier C#中称为 GUID 它是通过MAC地址、 时间戳、 命名空间、 随机数、 伪随机数来保证生成ID的唯一性,，有着固定的大小( 128 bit位 )，通常由 32 字节的字符串（十六进制）表示。 它的唯一性和一致性特点，使得可以无需注册过程就能够产生一个新的UUID；UUID可以被用作多种用途, 既可以用来短时间内标记一个对象，也可以可靠的辨别网络中的持久性对象。 UUID模块提供的UUID类和函数python的uuid模块提供的UUID类和函数uuid1()，uuid3()，uuid4()，uuid5() 来生成1, 3, 4, 5各个版本的UUID ( 需要注意的是：python中没有uuid2()这个函数)。 对uuid模块中最常用的几个函数总结如下: uuid.uuid1([node[, clock_seq]]) – 基于时间戳 由 MAC 地址（主机物理地址）、当前时间戳、随机数生成。可以保证全球范围内的唯一性， 但 MAC 的使用同时带来安全性问题，局域网中可以使用 IP 来代替MAC。 该函数有两个参数, 如果 node 参数未指定, 系统将会自动调用 getnode() 函数来获取主机的硬件地址. 如果 clock_seq 参数未指定系统会使用一个随机产生的14位序列号来代替. 注意： uuid1() 返回的不是普通的字符串，而是一个 uuid 对象，其内含有丰富的成员函数和变量。 uuid.uuid2() – 基于分布式计算环境DCE（Python中没有这个函数） 算法与uuid1相同，不同的是把时间戳的前 4 位置换为 POSIX 的 UID。 实际中很少用到该方法。 uuid.uuid3(namespace, name) – 基于名字的MD5散列值 通过计算名字和命名空间的MD5散列值得到，保证了同一命名空间中不同名字的唯一性， 和不同命名空间的唯一性，但同一命名空间的同一名字生成相同的uuid。 uuid.uuid4() – 基于随机数 由伪随机数得到，有一定的重复概率，该概率可以计算出来。 uuid.uuid5() – 基于名字的SHA-1散列值 算法与uuid3相同，不同的是使用 Secure Hash Algorithm 1 算法 上述几个函数的使用方法：1234567891011121314151617181920212223242526272829303132&gt;&gt;&gt; import uuid # 导入UUID模块&gt;&gt;&gt; # make a UUID based on the host ID and current time&gt;&gt;&gt; uuid.uuid1()UUID('a8098c1a-f86e-11da-bd1a-00112444be1e')&gt;&gt;&gt; # make a UUID using an MD5 hash of a namespace UUID and a name&gt;&gt;&gt; uuid.uuid3(uuid.NAMESPACE_DNS, 'python.org')UUID('6fa459ea-ee8a-3ca4-894e-db77e160355e')&gt;&gt;&gt; # make a random UUID&gt;&gt;&gt; uuid.uuid4()UUID('16fd2706-8baf-433b-82eb-8c7fada847da')&gt;&gt;&gt; # make a UUID using a SHA-1 hash of a namespace UUID and a name&gt;&gt;&gt; uuid.uuid5(uuid.NAMESPACE_DNS, 'python.org')UUID('886313e1-3b8a-5372-9b90-0c9aee199e5d')&gt;&gt;&gt; # make a UUID from a string of hex digits (braces and hyphens ignored)&gt;&gt;&gt; x = uuid.UUID('&#123;00010203-0405-0607-0809-0a0b0c0d0e0f&#125;')&gt;&gt;&gt; # convert a UUID to a string of hex digits in standard form&gt;&gt;&gt; str(x)'00010203-0405-0607-0809-0a0b0c0d0e0f'&gt;&gt;&gt; # get the raw 16 bytes of the UUID&gt;&gt;&gt; x.bytes'\x00\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\x0c\r\x0e\x0f'&gt;&gt;&gt; # make a UUID from a 16-byte string&gt;&gt;&gt; uuid.UUID(bytes=x.bytes)UUID('00010203-0405-0607-0809-0a0b0c0d0e0f') 实例首先，Python中没有基于 DCE 的，所以uuid2可以忽略； 其次，uuid4存在概率性重复，由无映射性，最好不用； 再次，若在Global的分布式计算环境下，最好用uuid1； 最后，若有名字的唯一性要求，最好用uuid3或uuid5。 12345678import uuidname = "test_name"namespace = "test_namespace"print uuid.uuid1() # 带参的方法参见Python Docprint uuid.uuid3(namespace, name)print uuid.uuid4()print uuid.uuid5(namespace, name)]]></content>
  </entry>
  <entry>
    <title><![CDATA[pyqt5学习手册]]></title>
    <url>%2F2018%2F08%2F03%2Fpyqt5%2Fpyqt5%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[PyQt5 是Digia的一套Qt5与python绑定的应用框架，同时支持2.x和3.x。本教程使用的是3.x。Qt库由Riverbank Computing开发，是最强大的GUI库之一 ，官方网站：www.riverbankcomputing.co.uk/news。 关于 PyQt5PyQt5是由一系列Python模块组成。超过620个类，6000和函数和方法。能在诸如Unix、Windows和Mac OS等主流操作系统上运行。PyQt5有两种证书，GPL和商业证书。 PyQt5类分为很多模块，主要模块有： QtCore 包含了核心的非GUI的功能。主要和时间、文件与文件夹、各种数据、流、URLs、mime类文件、进程与线程一起使用。 QtGui 包含了窗口系统、事件处理、2D图像、基本绘画、字体和文字类。 QIcon(&#39;web.png&#39;) 创建一个QIcon(显示图标)对象，接受一个图片路径作为参数 QtWidgets 类包含了一系列创建桌面应用的UI元素[必要] app = QApplication(sys.argv) 用于创建一个应用对象[必要] window = QWidget() 用于创建用户界面 QColorDialog 提供颜色的选择 QFontDialog 能做字体的选择 QInputDialog 提供了一个简单方便的对话框，可以输入字符串，数字或列表 QFileDialog 给用户提供文件或者文件夹选择的功能。能打开和保存文件 hbox = QHBoxLayout() 水平盒子布局 vbox = QVBoxLayout() 垂直盒子布局 grid = QGridLayout() 栅格布局 QtMultimedia 包含了处理多媒体的内容和调用摄像头API的类。 QtBluetooth 模块包含了查找和连接蓝牙的类。 QtNetwork 包含了网络编程的类，这些工具能让TCP/IP和UDP开发变得更加方便和可靠。 QtPositioning 包含了定位的类，可以使用卫星、WiFi甚至文本。 Engine 包含了通过客户端进入和管理Qt Cloud的类。 QtWebSockets 包含了WebSocket协议的类。 QtWebKit 包含了一个基WebKit2的web浏览器。 QtWebKitWidgets 包含了基于QtWidgets的WebKit1的类。 QtXml 包含了处理xml的类，提供了SAX和DOM API的工具。 QtSvg 提供了显示SVG内容的类，Scalable Vector Graphics (SVG)是一种是一种基于可扩展标记语言（XML），用于描述二维矢量图形的图形格式（这句话来自于维基百科）。 QtSql 提供了处理数据库的工具。QtTest提供了测试PyQt5应用的工具。 安装PyQt51pip install pyqt5 hello PyQt5！例一： 简单的小窗口这个简单的小例子展示的是一个小窗口。但是我们可以在这个小窗口上面做很多事情，改变大小，最大化，最小化等，这需要很多代码才能实现。这在很多应用中很常见，没必要每次都要重写这部分代码，Qt已经提供了这些功能。PyQt5是一个高级的工具集合，相比使用低级的工具，能省略上百行代码。 1234567891011121314import sysfrom PyQt5.QtWidgets import QApplication, QWidgetif __name__ == '__main__': app = QApplication(sys.argv) window = QWidget() window.resize(250, 150) window.move(300, 300) window.setWindowTitle('Simple') window.show() sys.exit(app.exec_()) 运行上面的代码，能展示出一个小窗口。 基本流程解析 创建一个应用实例和用户界面实例，这是所有PyQt5程序所必须的操作 12app = QApplication(sys.argv)window = QWidget() 使用 window 实例方法操作 window 1234window.resize(250, 150)window.move(300, 300)window.setWindowTitle('Simple')window.show() 其中: resize() 方法能改变控件的大小，这里的意思是窗口宽250px，高150px。move() 是修改控件位置的的方法。它把控件放置到屏幕坐标的(300, 300)的位置。注：屏幕坐标系的原点是屏幕的左上角。setWindowTitle() 用于为窗口添加标题，标题在标题栏展示show()能让控件在桌面上显示出来。控件在内存里创建，之后才能在显示器上显示出来。 安全退出主循环 1sys.exit(app.exec_()) 程序预览： 例2，面向对象编程123456789101112131415161718192021222324252627import sysfrom PyQt5.QtWidgets import QApplication, QWidgetfrom PyQt5.QtGui import QIconclass Example(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): self.setGeometry(300, 300, 300, 220) self.setWindowTitle('Icon') self.setWindowIcon(QIcon('web.png')) self.show()if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_()) 面向对象编程最重要的三个部分是类(class)、数据和方法。我们创建了一个类的调用，这个类继承自QWidget。这就意味着，我们调用了两个构造器，一个是这个类本身的，一个是这个类继承的。super()构造器方法返回父级的对象。__init__()方法是构造器的一个方法。 创建新类并继承QWidget ，紧接着调用super().__init__()方法，如此 self 便能在类中调用 QWidget 实例的全部方法了,通常我们对于程序的操作都是基于 QWidget 进行的 1234class Example(QWidget): def __init__(self): super().__init__() 创建 initUI 并在 __init__方法内调用它， 1234567891011def __init__(self): # ... self.initUI()def initUI(self): self.setGeometry(300, 300, 300, 220) self.setWindowTitle('Icon') self.setWindowIcon(QIcon('web.png')) self.show() 应用和示例的对象创立，主循环开始。 12345if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_()) 程序预览： 例3，提示框1234567891011121314151617181920212223242526272829303132333435import sysfrom PyQt5.QtWidgets import (QWidget, QToolTip, QPushButton, QApplication)from PyQt5.QtGui import QFontclass Example(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): QToolTip.setFont(QFont('SansSerif', 10)) self.setToolTip('This is a &lt;b&gt;QWidget&lt;/b&gt; widget') btn = QPushButton('Button', self) btn.setToolTip('This is a &lt;b&gt;QPushButton&lt;/b&gt; widget') btn.resize(btn.sizeHint()) btn.move(50, 50) self.setGeometry(300, 300, 300, 200) self.setWindowTitle('Tooltips') self.show()if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_()) 在这个例子中，我们为应用创建了一个提示框。 1QToolTip.setFont(QFont('SansSerif', 10)) 这个静态方法设置了提示框的字体，我们使用了10px的SansSerif字体。 1self.setToolTip('This is a &lt;b&gt;QWidget&lt;/b&gt; widget') 调用setTooltip()创建提示框可以使用富文本格式的内容。 12btn = QPushButton('Button', self)btn.setToolTip('This is a &lt;b&gt;QPushButton&lt;/b&gt; widget') 创建一个按钮，并且为按钮添加了一个提示框。 12btn.resize(btn.sizeHint())btn.move(50, 50) 调整按钮大小，并让按钮在屏幕上显示出来，sizeHint()方法提供了一个默认的按钮大小。程序预览： 例4，关闭窗口关闭一个窗口最直观的方式就是点击标题栏的那个叉，这个例子里，我们展示的是如何用程序关闭一个窗口。这里我们将接触到一点single和slots的知识。本例使用的是QPushButton组件类。 1QPushButton(string text, QWidget parent = None) text参数是想要显示的按钮名称，parent参数是放在按钮上的组件，在我们的 例子里，这个参数是QWidget。应用中的组件都是一层一层（继承而来的？）的，在这个层里，大部分的组件都有自己的父级，没有父级的组件，是顶级的窗口。 123456789101112131415161718192021222324252627282930import sysfrom PyQt5.QtWidgets import QWidget, QPushButton, QApplicationfrom PyQt5.QtCore import QCoreApplicationclass Example(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): qbtn = QPushButton('Quit', self) qbtn.clicked.connect(QCoreApplication.instance().quit) qbtn.resize(qbtn.sizeHint()) qbtn.move(50, 50) self.setGeometry(300, 300, 250, 150) self.setWindowTitle('Quit button') self.show()if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_()) 这里创建了一个点击之后就退出窗口的按钮。 1from PyQt5.QtCore import QCoreApplication 程序需要QtCore对象。 1qbtn = QPushButton('Quit', self) 创建一个继承自QPushButton的按钮。第一个参数是按钮的文本，第二个参数是按钮的父级组件，这个例子中，父级组件就是我们创建的继承自Qwidget的Example类。 1qbtn.clicked.connect(QCoreApplication.instance().quit) 事件传递系统在PyQt5内建的single和slot机制里面。点击按钮之后，信号会被捕捉并给出既定的反应。QCoreApplication包含了事件的主循环，它能添加和删除所有的事件，instance()创建了一个它的实例。QCoreApplication是在QApplication里创建的。 点击事件和能终止进程并退出应用的quit函数绑定在了一起。在发送者和接受者之间建立了通讯，发送者就是按钮，接受者就是应用对象。程序预览： 例5，消息盒子 QMessageBox默认情况下，我们点击标题栏的×按钮，QWidget就会关闭。但是有时候，我们修改默认行为。比如，如果我们打开的是一个文本编辑器，并且做了一些修改，我们就会想在关闭按钮的时候让用户进一步确认操作。 12345678910111213141516171819202122232425262728293031323334353637import sysfrom PyQt5.QtWidgets import QWidget, QMessageBox, QApplicationclass Example(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): self.setGeometry(300, 300, 250, 150) self.setWindowTitle('Message box') self.show() def closeEvent(self, event): reply = QMessageBox.question(self, 'Message', "Are you sure to quit?", QMessageBox.Yes | QMessageBox.No, QMessageBox.No) if reply == QMessageBox.Yes: event.accept() else: event.ignore()if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_()) 如果关闭QWidget，就会产生一个QCloseEvent。改变控件的默认行为，就是替换掉默认的事件处理。 123reply = QMessageBox.question(self, 'Message', "Are you sure to quit?", QMessageBox.Yes | QMessageBox.No, QMessageBox.No) 我们创建了一个消息框，上面有俩按钮：Yes和No.第一个字符串显示在消息框的标题栏，第二个字符串显示在对话框，第三个参数是消息框的俩按钮，最后一个参数是默认按钮，这个按钮是默认选中的。返回值在变量reply里。 1234if reply == QtGui.QMessageBox.Yes: event.accept()else: event.ignore() 这里判断返回值，如果点击的是Yes按钮，我们就关闭组件和应用，否者就忽略关闭事件。程序预览： 1QMessageBox.information(NULL, &quot;Title&quot;, &quot;Content&quot;, QMessageBox.Yes | QMessageBox.No, QMessageBox.Yes); 下面是一个简单的例子： 现在我们从 API 中看看它的函数签名： 1static StandardButton QMessageBox.information ( QWidget * parent, const QString &amp;amp; title, const QString &amp;amp; text, StandardButtons buttons = Ok, StandardButton defaultButton = NoButton ); 首先，它是 static 的，所以我们能够使用类名直接访问到(怎么看都像废话…)；然后看它那一堆参数，第一个参数 parent，说明它的父组件；第二个参数 title，也就是对话框的标题；第三个参数 text，是对话框显示的内容；第四个参数 buttons，声明对话框放置的按钮，默认是只放置一个 OK 按钮，这个参数可以使用或运算，例如我们希望有一个 Yes 和一个 No 的按钮，可以使用 QMessageBox.Yes | QMessageBox.No，所有的按钮类型可以在 QMessageBox 声明的 StandarButton 枚举中找到；第五个参数 defaultButton 就是默认选中的按钮，默认值是 NoButton，也就是哪个按钮都不选中。这么多参数，豆子也是记不住的啊！所以，我们在用 QtCreator 写的时候，可以在输入QMessageBox.information 之后输入(，稍等一下，QtCreator 就会帮我们把函数签名显示在右上方了，还是挺方便的一个功能！ 其它接口Qt 提供了五个类似的接口，用于显示类似的窗口。具体代码这里就不做介绍，只是来看一下样子吧！ 1QMessageBox.critical(NULL, "critical", "Content", QMessageBox.Yes | QMessageBox.No, QMessageBox.Yes); 1QMessageBox.warning(NULL, "warning", "Content", QMessageBox.Yes | QMessageBox.No, QMessageBox.Yes); 1QMessageBox.question(NULL, "question", "Content", QMessageBox.Yes | QMessageBox.No, QMessageBox.Yes); 1QMessageBox.about(NULL, "About", "About this application"); 请注意，最后一个 about()函数是没有后两个关于 button 设置的按钮的！ QMessageBox 对话框的文本信息时可以支持 HTML 标签的。例如： 1QMessageBox.about(NULL, &quot;About&quot;, &quot;About this &lt;font color=&apos;red&apos;&gt;application&lt;/font&gt;&quot;); 运行效果如下： 如果我们想自定义图片的话，也是很简单的。这时候就不能使用这几个 static 的函数了，而是要我们自己定义一个 QMessagebox 来使用： 123message = QMessageBox(QMessageBox.NoIcon, "Title", "Content with icon.");message.setIconPixmap(QPixmap("icon.png"));message.exec(); 这里我们使用的是 exec()函数，而不是 show()，因为这是一个模态对话框，需要有它自己的事件循环，否则的话，我们的对话框会一闪而过哦(感谢 laetitia 提醒). 需要注意的是，同其他的程序类似，我们在程序中定义的相对路径都是要相对于运行时的.exe 文件的地址的。比如我们写”icon.png”，意思是是在.exe 的当前目录下寻找一个”icon.png”的文件。这个程序的运行效果如下： 还有一点要注意，我们使用的是 png 格式的图片。因为 Qt 内置的处理图片格式是 png，所以这不会引起很大的麻烦，如果你要使用 jpeg 格式的图片的话，Qt 是以插件的形式支持的。在开发时没有什么问题，不过如果要部署的话，需要注意这一点。 最后再来说一下怎么处理对话框的交互。我们使用 QMessageBox 类的时候有两种方式，一是使用static`函数，另外是使用构造函数。 首先来说一下 static 函数的方式。注意，static 函数都是要返回一个 StandardButton，我们就可以通过判断这个返回值来对用户的操作做出相应。 123replay = QMessageBox.question(NULL, "Show Qt", "Do you want to show Qt dialog?", QMessageBox.Yes | QMessageBox.No, QMessageBox.Yes);if replay == QMessageBox.Yes: print('you press yes') 如果要使用构造函数的方式，那么我们就要自己运行判断一下啦： 123 message = QMessageBox(QMessageBox.NoIcon, "Show Qt", "Do you want to show Qt dialog?", QMessageBox.Yes | QMessageBox.No, NULL);if message.exec() == QMessageBox.Yes: QMessageBox.aboutQt(NULL, "About Qt"); 其实道理上也是差不多的。 例6，窗口居中12345678910111213141516171819202122232425262728293031323334import sysfrom PyQt5.QtWidgets import QWidget, QDesktopWidget, QApplicationclass Example(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): self.resize(250, 150) self.center() self.setWindowTitle('Center') self.show() def center(self): qr = self.frameGeometry() cp = QDesktopWidget().availableGeometry().center() qr.moveCenter(cp) self.move(qr.topLeft())if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_()) QtGui.QDesktopWidget提供了用户的桌面信息，包括屏幕的大小。 1self.center() 这个方法是调用我们下面写的，实现对话框居中的方法。 1qr = self.frameGeometry() 得到了主窗口的大小。 1cp = QDesktopWidget().availableGeometry().center() 获取显示器的分辨率，然后得到中间点的位置。 1qr.moveCenter(cp) 然后把自己窗口的中心点放置到qr的中心点。 1self.move(qr.topLeft()) 然后把窗口的坐上角的坐标设置为qr的矩形左上角的坐标，这样就把窗口居中了。程序预览： QWidget，QDialog和Qmainwindow的区别QWidgetQWidget类是所有用户界面对象的基类。 窗口部件是用户界面的一个原子：它从窗口系统接收鼠标、键盘和其它事件，并且将自己的表现形式绘制在屏幕上。每一个窗口部件都是矩形，并且它们按Z轴顺序排列。一个窗口部件可以被它的父窗口部件或者它前面的窗口部件盖住一部分。 QWidget有很多成员函数，但是它们中的一些有少量的直接功能：例如，QWidget有字体属性，但是自己从来不用。为很多继承它的子类提供了实际的功能，比如QLabel、QPushButton、QCheckBox等等。 没有父窗体的小部件始终是一个独立的窗口（顶级窗口部件）。非窗口的小部件为子部件，它们在父窗口中显示。Qt中大多数部件主要被用作子部件。例如：可以显示一个按钮作为顶层窗口，但大多数人更喜欢将按钮内置于其它部件，如QDialog。 QDialogQDialog类是对话框窗口的基类。 对话框窗口是一个顶级窗体，主要用于短期任务以及和用户进行简要通讯。QDialog可以是模式的也可以是非模式的。QDialog支持扩展性并且可以提供返回值。它们可以有默认按钮。QDialog也可以有一个QSizeGrip在它的右下角，使用setSizeGripEnabled()。 注意：QDialog（以及其它使用Qt.Dialog类型的widget）使用父窗口部件的方法和Qt中其它类稍微不同。对话框总是顶级窗口部件，但是如果它有一个父对象，它的默认位置就是父对象的中间。它也将和父对象共享工具条条目。 模式对话框阻塞同一应用程序中其它可视窗口输入的对话框。模式对话框有自己的事件循环，用户必须完成这个对话框中的交互操作，并且关闭了它之后才能访问应用程序中的其它任何窗口。模式对话框仅阻止访问与对话相关联的窗口，允许用户继续使用其它窗口中的应用程序。 显示模态对话框最常见的方法是调用其exec()函数，当用户关闭对话框，exec()将提供一个有用的返回值，并且这时流程控制继续从调用exec()的地方进行。通常情况下，要获得对话框关闭并返回相应的值，我们连接默认按钮，例如：”确定”按钮连接到accept()槽，”取消”按钮连接到reject()槽。另外我们也可以连接done()槽，传递给它Accepted或Rejected。 非模式对话框和同一个程序中其它窗口操作无关的对话框。在文字处理中的查找和替换对话框通常是非模式的，允许用户同时与应用程序的主窗口和对话框进行交互。调用show()来显示非模式对话框，并立即将控制返回给调用者。 如果隐藏对话框后调用show()函数，对话框将显示在其原始位置，这是因为窗口管理器决定的窗户位置没有明确由程序员指定，为了保持被用户移动的对话框位置，在closeEvent()中进行处理，然后在显示之前，将对话框移动到该位置。 半模式对话框调用setModal(true)或者setWindowModality()，然后show()。有别于exec()，show() 立即返回给控制调用者。 对于进度对话框来说，调用setModal(true)是非常有用的，用户必须拥有与其交互的能力，例如：取消长时间运行的操作。如果使用show()和setModal(true)共同执行一个长时间操作，则必须定期在执行过程中调用QApplication.processEvents()，以使用户能够与对话框交互（可以参考QProgressDialog）。 QMainWindowQMainWindow类提供一个有菜单条、工具栏、状态条的主应用程序窗口（例如：开发Qt常用的IDE-Visual Studio、Qt Creator等）。 一个主窗口提供了构建应用程序的用户界面框架。Qt拥有QMainWindow及其相关类来管理主窗口。 QMainWindow拥有自己的布局，我们可以使用QMenuBar（菜单栏）、QToolBar（工具栏）、QStatusBar（状态栏）以及QDockWidget（悬浮窗体），布局有一个可由任何种类小窗口所占据的中心区域。 例如： 使用原则 如果需要嵌入到其他窗体中，则基于QWidget创建。 如果是顶级对话框，则基于QDialog创建。 如果是主窗体，则基于QMainWindow创建。 QMainWindow与Qwidget通信12345678910111213141516171819202122class Example(QMainWindow): def __init__(self): super().__init__() self.initUI() def initUI(self): self.setFixedSize(600, 220) self.setWindowTitle('Statusbar') self.setWindowIcon(QIcon('icon.png')) self.statusBar().showMessage('Ready') # Querstion是一个Qwidget，忽略相关代码 self.widget = Querstion() self.widget.btn_submit.clicked.connect(self.submit) self.setCentralWidget(self.widget) self.show() def submit(self): print('submit') if self.q.yes.isChecked(): print('yes') self.q.question.setText('請查看LED是否亮起绿燈?') 布局管理在一个GUI程序里，布局是一个很重要的方面。布局就是如何管理应用中的元素和窗口。有两种方式可以搞定：绝对定位和PyQt5的layout类 绝对定位就是通过 move(x,y) 方法定位每一个元素，而layout类分为盒布局和栅格布局 盒布局1234567891011121314151617181920212223242526272829303132333435363738import sysfrom PyQt5.QtWidgets import QWidget, QPushButton, QHBoxLayout, QVBoxLayout, QApplicationclass Example(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): okButton = QPushButton("OK") cancelButton = QPushButton("Cancel") hbox = QHBoxLayout() hbox.addStretch(1) hbox.addWidget(okButton) hbox.addWidget(cancelButton) vbox = QVBoxLayout() vbox.addStretch(1) vbox.addLayout(hbox) self.setLayout(vbox) self.setGeometry(300, 300, 300, 150) self.setWindowTitle('Buttons') self.show()if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_()) 核心步骤解析 创建一个水平布局盒子实例 hbox 1hbox = QHBoxLayout() 在水平布局盒子左面添加一个弹性空间 1hbox.addStretch(1) 将创建好的组件添加进布局盒子中 12hbox.addWidget(okButton)hbox.addWidget(cancelButton) 创建一个竖直布局盒子,在组件上面加入弹性空间，接着将 hbox 添加入 vbox 123vbox = QVBoxLayout()vbox.addStretch(1)vbox.addLayout(hbox) 最后，将布局添加入windows 1self.setLayout(vbox) 栅格布局12345678910111213141516171819202122232425262728293031323334353637383940414243import sysfrom PyQt5.QtWidgets import (QWidget, QGridLayout, QPushButton, QApplication)class Example(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): grid = QGridLayout() self.setLayout(grid) names = ['Cls', 'Bck', '', 'Close', '7', '8', '9', '/', '4', '5', '6', '*', '1', '2', '3', '-', '0', '.', '=', '+'] positions = [(i,j) for i in range(5) for j in range(4)] for position, name in zip(positions, names): if name == '': continue button = QPushButton(name) grid.addWidget(button, *position) self.move(300, 150) self.setWindowTitle('Calculator') self.show()if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_()) 生成grid布局 12grid = QGridLayout()self.setLayout(grid) 写出列表形式的布局组件和定位，它们应该是一一对应的关系 12345678910names = ['Cls', 'Bck', '', 'Close', '7', '8', '9', '/', '4', '5', '6', '*', '1', '2', '3', '-', '0', '.', '=', '+']positions = [(i,j) for i in range(5) for j in range(4)]# (0,0) (0,1) (0,2) (0,3)# (0,0) (1,1) (1,2) (1,3)# ... 将names和positions组成对应关系,并将组件和定位add入布局中 1234567# zip 将可迭代对象对应的关系打包成一个个元组for position, name in zip(positions, names): if name == '': continue button = QPushButton(name) grid.addWidget(button, *position) 程序预览： 跨行显示grid.addWidget支持五个参数，第一个是需要加入的组件，第二个和第三个分别是行和列的定位，第四个和第五个表示跨行和跨列数 1grid.addWidget(reviewE`t, 3, 1, 5, 1) grid.setSpacing(10)用来生成组件之间的空间 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import sysfrom PyQt5.QtWidgets import (QWidget, QLabel, QLineEdit, QTextEdit, QGridLayout, QApplication)class Example(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): title = QLabel('Title') author = QLabel('Author') review = QLabel('Review') titleEdit = QLineEdit() authorEdit = QLineEdit() reviewEdit = QTextEdit() grid = QGridLayout() grid.setSpacing(10) grid.addWidget(title, 1, 0) grid.addWidget(titleEdit, 1, 1) grid.addWidget(author, 2, 0) grid.addWidget(authorEdit, 2, 1) grid.addWidget(review, 3, 0) grid.addWidget(reviewEdit, 3, 1, 5, 1) self.setLayout(grid) self.setGeometry(300, 300, 350, 300) self.setWindowTitle('Review') self.show()if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_()) 程序预览： 事件和信号所有的应用都是事件驱动的。事件大部分都是由用户的行为产生的，当然也有其他的事件产生方式，比如网络的连接，窗口管理器或者定时器等。调用应用的exec_()方法时，应用会进入主循环，主循环会监听和分发事件。 在事件模型中，有三个角色： 事件源 事件 事件目标 事件源就是发生了状态改变的对象。事件是这个对象状态的改变撞他改变的内容。事件目标是事件想作用的目标。事件源绑定事件处理函数，然后作用于事件目标身上。 PyQt5处理事件方面有个signal and slot机制。Signals and slots用于对象间的通讯。事件触发的时候，发生一个signal，slot是用来被Python调用的，slot只有在事件触发的时候才能调用。 事件对象事件对象是用python来描述一系列的事件自身属性的对象。+ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import sysfrom PyQt5.QtCore import Qtfrom PyQt5.QtWidgets import QWidget, QApplication, QGridLayout, QLabelclass Example(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): grid = QGridLayout() grid.setSpacing(10) x = 0 y = 0 self.text = "x: &#123;0&#125;, y: &#123;1&#125;".format(x, y) self.label = QLabel(self.text, self) grid.addWidget(self.label, 0, 0, Qt.AlignTop) self.setMouseTracking(True) self.setLayout(grid) self.setGeometry(300, 300, 350, 200) self.setWindowTitle('Event object') self.show() def mouseMoveEvent(self, e): x = e.x() y = e.y() text = "x: &#123;0&#125;, y: &#123;1&#125;".format(x, y) self.label.setText(text)if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_()) 这个示例中，我们在一个组件里显示鼠标的X和Y坐标。+ 123self.text = "x: &#123;0&#125;, y: &#123;1&#125;".format(x, y)self.label = QLabel(self.text, self) X Y坐标显示在QLabel组件里+ 1self.setMouseTracking(True) 鼠标追踪默认没有开启，当有鼠标点击事件发生后才会开启。+ 1234567def mouseMoveEvent(self, e): x = e.x() y = e.y() text = "x: &#123;0&#125;, y: &#123;1&#125;".format(x, y) self.label.setText(text) e代表了事件对象。里面有我们触发事件（鼠标移动）的事件对象。x()和y()方法得到鼠标的x和y坐标点，然后拼成字符串输出到QLabel组件里。+ 程序展示： 事件发送有时候我们会想知道是哪个组件发出了一个信号，PyQt5里的sender()方法能搞定这件事。+ 1234567891011121314151617181920212223242526272829303132333435363738394041import sysfrom PyQt5.QtWidgets import QMainWindow, QPushButton, QApplicationclass Example(QMainWindow): def __init__(self): super().__init__() self.initUI() def initUI(self): btn1 = QPushButton("Button 1", self) btn1.move(30, 50) btn2 = QPushButton("Button 2", self) btn2.move(150, 50) btn1.clicked.connect(self.buttonClicked) btn2.clicked.connect(self.buttonClicked) self.statusBar() self.setGeometry(300, 300, 290, 150) self.setWindowTitle('Event sender') self.show() def buttonClicked(self): sender = self.sender() self.statusBar().showMessage(sender.text() + ' was pressed')if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_()) 这个例子里有俩按钮，buttonClicked()方法决定了是哪个按钮能调用sender()方法。+ 12btn1.clicked.connect(self.buttonClicked)btn2.clicked.connect(self.buttonClicked) 两个按钮都和同一个slot绑定。+ 1234def buttonClicked(self): sender = self.sender() self.statusBar().showMessage(sender.text() + &apos; was pressed&apos;) 我们用调用sender()方法的方式决定了事件源。状态栏显示了被点击的按钮。+ 程序展示：+ 不同窗口的通信与信号发送A 类通过 QObject 实例定义一个信号并使用emit()广播它,B 类通过 connect(self.method...) 将自身的方法与该信号进行绑定，这就是一个信号槽机制 1234567891011121314151617from PyQt5.QtCore import pyqtSignal, QObjectfrom PyQt5.QtWidgets import QMainWindow, QApplicationclass Communicate(QObject): closeApp = pyqtSignal()class A(QtWidgets): def __init__(): self.c = Communicate() self.c.closeApp.emit()class B(QtWidgets): def __init__(): a = A() a.c.closeApp.connect(B.method(...)) 控件1控件就像是应用这座房子的一块块砖。PyQt5有很多的控件，比如按钮，单选框，滑动条，复选框等等。在本章，我们将介绍一些很有用的控件：QCheckBox，ToggleButton，QSlider，QProgressBar和QCalendarWidget。 QCheckBoxQCheckBox组件有俩状态：开和关。通常跟标签一起使用，用在激活和关闭一些选项的场景。+ 12345678910111213141516171819202122232425262728293031323334353637from PyQt5.QtWidgets import QWidget, QCheckBox, QApplicationfrom PyQt5.QtCore import Qtimport sysclass Example(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): cb = QCheckBox('Show title', self) cb.move(20, 20) cb.toggle() cb.stateChanged.connect(self.changeTitle) self.setGeometry(300, 300, 250, 150) self.setWindowTitle('QCheckBox') self.show() def changeTitle(self, state): if state == Qt.Checked: self.setWindowTitle('QCheckBox') else: self.setWindowTitle(' ')if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_()) 图片QPixmap是处理图片的组件。本例中，我们使用QPixmap在窗口里显示一张图片。+ 1234567891011121314151617181920212223242526272829303132333435from PyQt5.QtWidgets import (QWidget, QHBoxLayout, QLabel, QApplication)from PyQt5.QtGui import QPixmapimport sysclass Example(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): hbox = QHBoxLayout(self) pixmap = QPixmap("redrock.png") lbl = QLabel(self) lbl.setPixmap(pixmap) hbox.addWidget(lbl) self.setLayout(hbox) self.move(300, 200) self.setWindowTitle('Red Rock') self.show()if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_()) 1pixmap = QPixmap(&quot;redrock.png&quot;) 创建一个QPixmap对象，接收一个文件作为参数。+ 12lbl = QLabel(self)lbl.setPixmap(pixmap) 把QPixmap实例放到QLabel组件里。 程序展示： 多线程默认情况下PyQt5的界面是阻塞的，当程序内部处理一些耗时的工作时，界面就会等待该工作处理完成才会响应其它事件(点击下一步的按钮)，这样就造成了程序卡顿。我们可以使用多线程处理耗时的工作来避免这个问题。 12345678910111213141516171819202122from PyQt5.QtCore import pyqtSignal, QObject,QThreadclass WorkThread(QThread): sig = pyqtSignal() def __int__(self): super(WorkThread,self).__init__() def run(self): # 模拟工作线程 time.sleep(5) self.sig.emit()work_thread = WorkThread()class widget(Qwidget): """docstring for ClassName""" def __init__(self, arg): # 当work_thread处理完毕时，调用某些方法 work_thread.sig.connect(self.method) def some_method(self): # 启动线程 work_thread.start() 常用API或方法文字与样式设置1lable = QLabel(&apos;請查看LED是否亮起紅燈?&apos;) 设置字体样式使用setStyleSheet设置样式 1lable.setStyleSheet("color:red;font-size:12px;font-style:Microsoft YaHei"); 在创建时直接使用css设置样式 1question = QLabel(&apos;請查看LED是否亮起&lt;span style = &apos;color:red;font-size:14px&apos;&gt;紅燈?&lt;/span&gt;&apos;) 修改文字内容 1lable.setText(&apos;請查看LED是否亮起绿燈?&apos;) 设置自动换行 默认是不会换行的，如果文字过长则会超出窗口从而不显示，而且使用setStyleSheet无效 1lable.setWordWrap(True) 设置lable显示或隐藏1QWidge.setVisible(bool) show()、hide()、setHidden()都能控制元素显示或隐藏，并且他们的内部都是通过setVisible(bool)实现的 参考手册PyQt5中文教程]]></content>
  </entry>
  <entry>
    <title><![CDATA[磁盘测试工具之fio]]></title>
    <url>%2F2018%2F07%2F31%2F%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%2F%E7%A3%81%E7%9B%98%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7%E4%B9%8Bfio%2F</url>
    <content type="text"><![CDATA[FIO是测试IOPS的非常好的工具，用来对硬件进行压力测试和验证，支持13种不同的I/O引擎 安装centos可以使用yum安装 1yum install fio 命令行模式安装完成后，使用命令行输入以下命令，便可以开始测试了。 1fio -filename=/tmp/test_randread -direct=1 -iodepth 1 -thread -rw=randread -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=mytest 以上命令的含义是随机读写：(可直接用，向磁盘写一个2G文件，10线程，随机读1分钟，给出结果) Job file模式需要测试多种情况下的多个job时，可以将所有配置参数写在一个配置文件当中，然后使用fio config_file方式运行即可。 说明： 配置文件划分为多个区域，每个区域的参数设置均作用于其下方的区域； global区域进行全局的参数配置； 非特定名字的区域（fist_job）则被视为一个运行任务。 配置文件 config_file 示例： 1234567891011121314#-start job file-[global]ioengine=psynciodepth=1size=20gbs=4kdirect=1runtime = 180filename=/dev/sdb1[fist_job]rw=write[second_job]rw=randwrite#-end job file- 测试结果分析123456789101112131415161718192021222324252627282930fio-3.1Starting 10 threadsJobs: 10 (f=10): [r(10)][100.0%][r=2944KiB/s,w=0KiB/s][r=184,w=0 IOPS][eta 00m:00s]mytest: (groupid=0, jobs=10): err= 0: pid=345: Tue Jul 31 02:51:46 2018 read: IOPS=192, BW=3083KiB/s (3157kB/s)(181MiB/60061msec) clat (usec): min=981, max=578165, avg=51855.31, stdev=54091.70 lat (usec): min=981, max=578166, avg=51857.14, stdev=54091.67 clat percentiles (msec): | 1.00th=[ 7], 5.00th=[ 9], 10.00th=[ 12], 20.00th=[ 16], | 30.00th=[ 21], 40.00th=[ 27], 50.00th=[ 34], 60.00th=[ 44], | 70.00th=[ 59], 80.00th=[ 80], 90.00th=[ 113], 95.00th=[ 146], | 99.00th=[ 253], 99.50th=[ 321], 99.90th=[ 558], 99.95th=[ 567], | 99.99th=[ 575] bw ( KiB/s): min= 32, max= 673, per=10.00%, avg=308.35, stdev=102.75, samples=1200 iops : min= 2, max= 42, avg=19.26, stdev= 6.42, samples=1200 lat (usec) : 1000=0.01% lat (msec) : 2=0.35%, 4=0.03%, 10=6.73%, 20=22.14%, 50=35.07% lat (msec) : 100=22.67%, 250=11.95%, 500=0.86%, 750=0.19% cpu : usr=0.05%, sys=0.18%, ctx=11629, majf=0, minf=40 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0% issued rwt: total=11573,0,0, short=0,0,0, dropped=0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1Run status group 0 (all jobs): READ: bw=3083KiB/s (3157kB/s), 3083KiB/s-3083KiB/s (3157kB/s-3157kB/s), io=181MiB (190MB), run=60061-60061msecDisk stats (read/write): sda: ios=11540/10, merge=0/6, ticks=596750/360, in_queue=597720, util=99.95% bw：磁盘的吞吐量，这个是顺序读写考察的重点 。 iops：磁盘的每秒读写次数，这个是随机读写考察的重点。 runt：线程运行时长。 slat（usec）：提交延迟，单位是微秒，向内核提交一个IO请求所需的时间。 clat（usec）：完成延迟，它是提交到内核和IO完成经过的时间，不包括提交延迟。 min：最快时间；max：最慢时间；avg：平均时间；stdev： lat:一个设置数据块大小的读/写请求从提交到完成所需的时间。 clat percentiles（usec）:完成时间的百分比分布，如大约99%的请求是在14毫秒之内完成。 CPU：cpu使用率。 issued:total=r=0/w=16777216 总共发出0个读请求，16777216个写请求。 参数说明IO类型rw=str,向文件发起的IO类型 rw=read 顺序读 rw=write 顺序写 rw=randwrite 随机写 rw=randread 随机读 rw=rw 顺序混合读写 rw=randrw 随机混合读写 单次IO的数据大小产生的IO单元的大小，可以是一个孤立的值，也可以是一个范围。 bs=int,单次IO的block size,默认为4k。如果是单个值的话，将会对读写都生效。如果是一个逗号，再跟一个int值的话，则是仅对于写有效。 bs = 8k 读写都使用8k的块 bs = 4k,8k 读使用4k的块，写使用8k的块 bs = ,8k 使得写采用8k的块，读采用默认的值。 IO数据总量size=int,将会读/写多少数据 这个job IO总共要传输的数据的大小。FIO将会执行到所有的数据传输完成，除非设定了运行时间（runtime选项）。除非有特定的nrfiles选项和filesize选项被设置，fio将会在job定义的文件中平分这个大小。如果这个值不设置的话，fio将会使用这个文件或设备的总大小。如果这些文件不存在的话，size选项一定要给出。也可以给出一个1到100的百分比。e.g. size=20%，fio将会使用给定的文件或设备的20%的空间。 IO引擎ioengine=str,定义job向文件发起IO的方式。 sync 基本的read,write.lseek用来作定位 psync 基本的pread,pwrite vsync 基本的readv,writev libaio Linux专有的异步IO。Linux仅支持非buffered IO的队列行为。 posixaio glibc posix异步IO solarisaio solaris独有的异步IO windowsaio windows独有的异步IO mmap 文件通过内存映射到用户空间，使用memcpy写入和读出数据 splice 使用splice和vmsplice在用户空间和内核之间传输数据 syslet-rw 使用syslet 系统调用来构造普通的read/write异步IO sg SCSI generic sg v3 io.可以是使用SG_IO ioctl来同步，或是目标是一个sg字符设备，我们使用read和write执行异步IO null 不传输任何数据，只是伪装成这样。主要用于训练使用fio，或是基本debug/test的目的。 net 根据给定的host:port通过网络传输数据。根据具体的协议，hostname,port,listen,filename这些选项将被用来说明建立哪种连接，协议选项将决定哪种协议被使用。 netsplice 像net，但是使用splic/vmsplice来映射数据和发送/接收数据。 cpuio 不传输任何的数据，但是要根据cpuload=和cpucycle=选项占用CPU周期.e.g. cpuload=85将使用job不做任何的实际IO，但要占用85%的CPU周期。在SMP机器上，使用numjobs=来获取需要的CPU，因为cpuload仅会载入单个CPU，然后占用需要的比例。 guasi GUASI IO引擎是一般的用于异步IO的用户空间异步系统调用接口 rdma RDMA I/O引擎支持RDMA内存语义（RDMA_WRITE/RDMA_READ）和通道主义(Send/Recv）用于InfiniBand,RoCE和iWARP协议 external 指明要调用一个外部的IO引擎（二进制文件）。e.g. ioengine=external:/tmp/foo.o将载入/tmp下的foo.o这个IO引擎 IO depthiodepth=int,如果IO引擎是异步的，这个指定我们需要保持的队列深度 默认对于每个文件来说是1，可以设置一个更大的值来提供并发度。iodepth大于1不会影响同步IO引擎（除非verify_async这个选项被设置） IO类型direct=bool,true,则标明采用non-buffered io.同O_DIRECT效果一样。ZFS和Solaris不支持direct io，在windows同步IO引擎不支持direct iobuffered=bool,true,则标明采用buffered io。是direct的反义词，默认是true job文件数量nrfiles=int,用于这个job的文件数目,默认为1,表示负载将分发到几个文件之中openfiles=int,在同一时间可以同时打开的文件数目，默认同nrfiles相等，可以设置小一些，来限制同时打开的文件数目。 线程数numjobs=int,创建大量的线程/进程来执行同一件事。我们将这样一系列的job，看作一个特定的group 其它 name=str,job名，用于输出信息用的名字。如果不设置的话，fio输出信息时将采用job name，如果设置的话，将用设置的名字。在命令行中，这个参数有特殊的作用，标明一个新job的开始。 description=str,job的说明信息,在job运行的时候不起作用，只是在输出文件描述信息的时候才起作用。 directory=str,使用的文件的路径前缀，默认是./ filename=str,一般情况下，fio会根据job名，线程号，文件名来产生一个文件名。如果，想在多个job之间共享同一个文件的话，可以设定一个名字来代替默认的名字.如果ioengine是‘net’的话，文件名则是以这种格式=host,port,protocol.如果ioengine是基于文件的话，可以通过‘:’分割来设定一系列的文件。e.g. filename=/dev/sda:/dev/sdb 希望job打开/dev/sda和/dev/sdb作为两个工作文件。 opendir=str,让fio递归的添加目录下和子目录下的所有文件。 lockfile=str,fio在文件上执行IO之前默认是不锁文件的，这样的话，当有多个线程在此文件上执行IO的话，会造成结果的不一致。这个选项可以用来共享文件的负载，支持的锁类型： none 默认不使用锁 exclusive 排它锁 readwrite 读写锁 在后面可以加一个数字后缀，如果设置的话，每一个线程将会执行这个数字指定的IO后才会放弃锁，因为锁的开销是比较大的，所以这种方式可以加速IO。 kb_base=int,size换算单位，1000/1024,默认为1024 randrepeat=bool,对于随机IO负载，配置生成器的种子，使得路径是可以预估的，使得每次重复执行生成的序列是一样的。 use_os_rand=bool,fio可以使用操作系统的随机数产生器，也可以使用fio内部的随机数产生器（基于tausworthe），默认是采用fio内部的产生器,质量更好，速度更快。 fallocate=str,如何准备测试文件 none 不执行预分配空间 posix 通过posix_fallocate()预分配空间 keep 通过fallocate()（设置FALLOC_FL_KEEP_SIZE）预分配空间 0 none的别名,出于兼容性,1 posix的别名，出于兼容性，并不是在所有的平台上都有效，‘keep’仅在linux上有效，ZFS不支持。默认为‘posix’ fadvise_hint=bool,默认fio将使用fadvise()来告知内核fio要产生的IO类型，如果不想告诉kernel来执行一些特定的IO类型的话，可行关闭这个选项。如果设置的话，fio将使用POSIX_FADV_SEWUENTIAL来作顺序IO，使用POSIX_FADV_RANDOM来做随机IO filesize=int,单个文件的大小，可以是一个范围，在这种情况下，fio将会在一个范围内选择一个大小来决定单个文件大小，如果没有设置的话，所有的文件将会是同样的大小。 fill_device=bool,fill_fs=bool,填满空间直到达到终止条件ENOSPC，只对顺序写有意义。对于读负载，首行要填满挂载点，然后再启动IO，对于裸设备结点，这个设置则没有什么意义，因为，它的大小已被被文件系统知道了，此外写的超出文件将不会返回ENOSPC. blockalign=int,ba=int,配置随机io的对齐边界。默认是与blocksize的配置一致，对于direct_io，最小为512b,因为它与依赖的硬件块大小，对于使用文件的随机map来说，这个选项不起作用。 blocksize_range=irange,bsrange=irange,不再采用单一的块大小，而是定义一个范围，fio将采用混合io块大小.IO单元大小一般是给定最小值的备数。同时应用于读写，当然也可以通过‘,’来隔开分别配置读写。 bssplit=str,可以更为精确的控制产生的block size.这个选项可以用来定义各个块大小所占的权重.格式是 12bssplit=blocksize/percentage;blocksize/percentagebssplit=4k/10:64k/50;32k/40 产生的这样的负载：50% 64k的块，10% 4k的块, 40% 32k的块 可以分别为读和写来设置 1bssplit=2k/50:4k/50,4k/90:8k/10 产生这样的负载：读（50% 64k的块，50% 4k的块），写（90% 4k的块, 10% 8k的块） blocksize_unaligned,bs_unaligned,如果这个选项被设置的，在bsrange范围内的大小都可以产生，这个选项对于direct io没有作用，因为对于direct io至少需要扇区对齐。 zero_buffers,如果这个选项设置的话，IO buffer全部位将被初始为0,如果没有置位的话，将会是随机数. refill_buffers,如果这个选项设置的话，fio将在每次submit之后都会将重新填满IO buffer,默认都会在初始是填满，以后重复利用。这个选项只有在zero_buffers没有设置的话，这个选项才有作用。 scramble_buffer=bool,如果refilee_buffers成本太高的话，但是负载要求不使用重复数据块，设置这个选项的话，可以轻微的改动IO buffer内容，这种方法骗不过聪明的块压缩算法，但是可以骗过一些简单的算法。 buffer_compress_percentage=int,如果这个设置的话，fio将会尝试提供可以压缩到特定级别的Buffer内容。FIO是能完提供混合的0和随机数来实现的 file_service_type=str,fio切换job时，如何选择文件，支持下面的选项 random 随机选择一个文件 roundrobin 循环使用打开的文件，默认 sequential 完成一个文件后，再移动到下一个文件这个选项可以加后缀数字，标明切换到下一个新的频繁程度。random:4 每4次IO后，将会切换到一下随机的文件 iodepth_batch_submit=int,iodepth_batch=int,这个定义了一次性提交几个IO，默认是1，意味着一旦准备好就提交IO，这个选项可以用来一次性批量提交IO iodepth_batch_complete=int,这个选项定义了一次取回多少个IO，如果定义为1的话，意味着我们将向内核请求最小为1个IO. iodepth_low=int,这个水位标志标明什么时候开始重新填充这个队列，默认是同iodepth是一样的，意味着，每时每刻都在尝试填满这个队列。如果iodepth设置为16，而iodepth设置为4的话，那么fio将等到depth下降到4才开始重新填充 offset=int,在文件特定的偏移开始读数据,在这个offset之前的数据将不会被使用，有效的文件大小=real_size-offset offset_increment=int,如果这个选项被设置的话，实际的offset=offset+offset_increment * thread_number,线程号是从0开始的一个计数器，对于每一个job来说是递增的。这个选项对于几个job同时并行在不交界的地方操作一个文件是有用的。 fsync=int,如果写一个文件的话，每n次IO传输完block后，都会进行一次同步脏数据的操作。 fdatasync=int,同fsync，但是采用fdatasync()来同步数据，但不同步元数据 sync_file_range=str:val,对于每‘val’个写操作，将执行sync_file_range()。FIO将跟踪从上次sync_file_range()调用之扣的写范围，‘str’可以是以下的选择 wait_before SYNC_FILE_RANGE_WAIT_BEFORE write SYNC_FILE_RANGE_WRITE wait_after SYNC_FILE_RANGE_WAIT_AFTER 示例：sync_file_range=wait_before,write:8,fio将在每8次写后使用SYNC_FILE_RANGE_WAIT_BEFORE|SYNC_FILE_RANGE_WRITE overwrite=bool,如果是true的话，写一个文件的话，将会覆盖已经存在的数据。如果文件不存在的话，它将会在写阶段开始的时候创建这个文件。 end_fsync=bool,如果是true的话，当job退出的话，fsync将会同步文件内容 fsync_on_close=bool,如果是true的话，关闭时，fio将会同步脏文件，不同于end_fsync的时，它将会在每个文件关闭时都会发生，而不是只在job结束时。 rwmixread=int,混合读写中，读占的百分比 rwmixwrite=int,混合读写中，写占的百分比；如果rwmixread=int和rwmixwrite=int同时使用的话并且相加不等于100%的话，第二个值将会覆盖第一个值。这可能要干扰比例的设定,如果要求fio来限制读和写到一定的比率。在果在这种情况下，那么分布会的有点的不同。 norandommap,一般情况下，fio在做随机IO时，将会覆盖文件的每一个block.如果这个选项设置的话，fio将只是获取一个新的随机offset,而不会查询过去的历史。这意味着一些块可能没有读或写，一些块可能要读/写很多次。在个选项与verify=互斥，并只有多个块大小（bsrange=）正在使用，因为fio只会记录完整的块的重写。 nice=int,根据给定的nice值来运行这个job prio=int,设置job的优先级，linux将这个值限制在0-7之间，0是最高的。 prioclass=int,设置优先级等级。 thinktime=int,上一个IO完成之后，拖延x毫秒，然后跳到下一个。可以用来访真应用进行的处理。 thinktime_spin=int,只有在thinktime设置时才有效，在为了sleep完thinktime规定的时间之前，假装花费CPU时间来做一些与数据接收有关的事情。 thinktime_blocks,只有在thinktime设置时才有效，控制在等等‘thinktime’的时间内产生多少个block，如果没有设置的话，这个值将是1，每次block后，都会将等待‘thinktime’us。 rate=int,限制job的带宽。 rate=500k,限制读和写到500k/s rate=1m,500k,限制读到1MB/s，限制写到500KB/s rate=,500k , 限制写到500kb/s rate=500k, 限制读到500KB/s ratemin=int,告诉fio尽最在能力来保证这个最小的带宽，如果不能满足这个需要，将会导致程序退出。 rate_iops=int,将带宽限制到固定数目的IOPS，基本上同rate一样，只是独立于带宽，如果job是指定了一个block size范围，而不是一个固定的值的话，最小blocksize将会作为标准。 rate_iops_min=int,如果fio达不到这个IOPS的话，将会导致job退出。 ratecycle=int,几个毫秒内的平均带宽。用于‘rate’和‘ratemin’ cpumask=int,设置job使用的CPU.给出的参数是一个掩码来设置job可以运行的CPU。所以，如果要允许CPU在1和5上的话，可以通过10进制数来设置（1&lt;&lt;1 | 1&lt;&lt;5），或是34 cpus_allowed=str,功能同cpumask一样，但是允许通过一段文本来设置允许的CPU。e.g.上面的例子可是这样写cpus_allowed=1,5。这个选项允许设置一个CPU范围，如cpus_allowed=1,5,8-15startdelay=time,fio启动几秒后再启动job。只有在job文件包含几个jobs时才有效，是为了将某个job延时几秒后执行。 runtime=time,控制fio在执行设定的时间后退出执行。很难来控制单个job的运行时间，所以这个参数是用来控制总的运行时间。 time_based,如果设置的话，即使file已被完全读写或写完，也要执行完runtime规定的时间。它是通过循环执行相同的负载来实现的。 ramp_time=time,设定在记录任何性能信息之前要运行特定负载的时间。这个用来等性能稳定后，再记录日志结果，因此可以减少生成稳定的结果需要的运行时间。 sync=bool,使用sync来进行buffered写。对于多数引擎，这意味着使用O_SYNC iomem=str,mem=str,fio可以使用各种各样的类型的内存用来io单元buffer. malloc 使用malloc() shm 使用共享内存.通过shmget()分配 shmhuge 同shm一样，可以使用huge pages mmap 使用mmap。可以是匿名内存，或是支持的文件，如果一个文件名在选项后面设置的话，格式是mem=mmap:/path/to/file mmaphuge 使用mmapped huge file.在mmaphuge扣面添加文件名，alamem=mmaphuge:/hugetlbfs/file分配的区域是由job允许的最大block size * io 队列的长度。对于shmhuge和mmaphuge，系统应该有空闲的页来分配。这个可以通过检测和设置reading/writing /proc/sys/vm/nr_hugepages来实现（linux）。FIO假设一个huge page是4MB。所以要计算对于一个JOB文件需要的Huge page数量，加上所有jobs的队列长度再乘以最大块大小，然后除以每个huge page的大小。可以通过查看/proc/meminfo来看huge pages的大小。如果通过设置nr_hugepages=0来使得不允许分配huge pages，使用mmaphug或是shmhuge将会失败。 mmaphuge 需要挂载hugelbfs而且要指定文件的位置，所以如果要挂载在/huge下的话，可以使用mem=mmaphuge:/huge/somefile iomem_align=int,标明IO内存缓冲的内存对齐方式。 hugepage-size=int,设置huge page的大小。至少要与系统的设定相等。默认是4MB，必然是MB的倍数，所以用hugepage-size=Xm是用来避免出现不是2的整数次方的情况。 exitall当一个job退出时，会终止运行其它的job，默认是等待所有的job都完成，FIO才退出，但有时候这并不是我们想要的。 bwavgtime=int,在给定时间内的平均带宽。值是以毫秒为单位的 iopsavgtime=int,在给定时间内的平均IOPS，值是以毫秒为单位的 create_serialize=bool,job将会串行化创建job,这将会用来避免数据文件的交叉，这依赖于文件系统和系统的CPU数 create_fsync=bool,创建后同步数据文件，这是默认的值 create_on_open=bool,不会为IO预先创建文件，只是在要向文件发起IO的时候，才创建open() create_only=bool,如果设置为true的话，fio将只运行到job的配置阶段。如果文件需要部署或是更新的磁盘的话，只有上面的事才会做，实际的文件内容并没有执行。 pre_read=bool,如果这个选项被设置的话，在执行IO操作之前，文件将会被预读到内存.这会删除‘invalidate’标志，因为预读数据，然后丢弃cache中的数据的话，是没有意义的。这只是对可以seek的IO引擎有效，因为这允许读相同的数据多次。因此对于network和splice不起作用。 unlink=bool,完成后将删除job产生的文件。默认是not,如果设置为true的话，将会花很多时间重复创建这些文件。 loops=int,重复运行某个job多次，默认是1 do_verify=bool,写完成后，执行一个校验的阶段，只有当verify设置的时候才有效。默认是true verify=str,写一个文件时，每次执行完一个job扣，fio可以检验文件内容.允许的校验算法是：md5,crc64,crc32c,crc32c-intel,crc32,crc16,crc7,sha512,sha256,sha1,meta,null.这个选项可以用来执行重复的burn-in测试，来保证写数据已经正确的读回。如果是read或随机读，fio将假设它将会检验先前写的文件。如果是各种格式的写，verify将会是对新写入的数据进行校验。 stonewall,wait_for_previous,等待先前的job执行完成后，再启动一个新的job。可以用来在job文件中加入串行化的点。stone wall也用来启动一个新reporting group new_group,启动一个新的reporting group。如果这个选项没有设置的话，在job文件中的job将属于相同的reporting group，除非通过stonewall隔开 group_reporting,如果‘numjobs’设置的话，我们感兴趣的可能是打印group的统计值，而不是一个单独的job。这在‘numjobs’的值很大时，一般是设置为true的，可以减少输出的信息量。如果‘group_reporting’设置的话，fio将会显示最终的per-groupreport而不是每一个job都会显示 thread,fio默认会使用fork()创建job，如果这个选项设置的话，fio将使用pthread_create来创建线程 zonesize=int,将一个文件分为设定的大小的zone zoneskip=int,跳过这个zone的数据都被读完后，会跳过设定数目的zone. write_iolog=str,将IO模式写到一个指定的文件中。为每一个job指定一个单独的文件，否则iolog将会分散的的，文件将会冲突。 read_iolog=str,将开一个指定的文件，回复里面的日志。这可以用来存储一个负载，并进行重放。给出的iolog也可以是一个二进制文件，允许fio来重放通过blktrace获取的负载。 replay_no_stall,当使用read_iolog重放I/O时，默认是尝试遵守这个时间戳，在每个IOPS之前会有适当的延迟。通过设置这个属性，将不会遵守这个时间戳，会根据期望的顺序，尝试回复，越快越好。结果就是相同类型的IO，但是不同的时间 replay_redirect,当使用read_iolog回放IO时，默认的行为是在每一个IOP来源的major/minor设备上回放IOPS。这在有些情况是不是期望的，比如在另一台机器上回放，或是更换了硬件，使是major/minor映射关系发生了改变。Replay_redirect将会导致所有的IOPS回放到单个设备上，不管这些IO来源于哪里.比如：replay_redirect=/dev/sdc将会使得所有的IO都会重定向到/dev/sdc.这就意味着多个设备的数据都会重放到一个设置，如果想来自己多个设备的数据重放到多个设置的话，需要处理我们的trace，生成独立的trace，再使用fio进行重放，不过这会破坏多个设备访问的严格次序。 write_bw_log=str,在job file写这个job的带宽日志。可以在他们的生命周期内存储job的带宽数据。内部的fio_generate_plots脚本可以使用gnuplot将这些文本转化成图。 write_lat_log=str,同write_bw_log类似，只是这个选项可以存储io提交，完成和总的响应时间。如果没有指定文件名，默认的文件名是jobname_type.log。即使给出了文件名，fio也会添加两种类型的log。如果我们指定write_lat_log=foo,实际的log名将是foo_slat.log,foo_slat.log和foo_lat.log.这会帮助fio_generate_plot来自动处理log write_iops_log=str,类似于write_bw_log,但是写的是IOPS.如果没有给定文件名的话，默认的文件名是jobname_type.log。 log_avg_msec=int,默认，fio每完成一个IO将会记录一个日志（iops,latency,bw log）。当向磁盘写日志的时候，将会很快变的很大。设置这个选项的话，fio将会在一定的时期内平均这些值，指少日志的数量，默认是0 lockmem=int,使用mlock可以指定特定的内存大小，用来访真少量内存 exec_preren=str,运行job之前，通过过system执行指定的命令 exec_postrun=str,job执行完成后，通过system执行指定的命令 ioscheduler=str,在运行之前，尝试将文件所在的设备切换到指定的调度器。 cpuload=int,如果job是非常占用CPU周期的，可以指定战胜CPU周期的百分比。 cpuchunks=int,如果job是非常战胜CPU周期的，将load分拆为时间的cycles，以毫秒为单位 disk_util=bool,产生磁盘利用率统计信息。默认是打开的 disable_lat=bool,延迟的有效数字。 clat_percentiles=bool,允许报告完成完成响应时间的百分比 continue_on_error=str,一般情况下，一旦检测到错误，fio将会退出这个job.如果这个选项设置的话，fio将会一直执行到有‘non-fatal错误‘（EIO或EILSEQ）或是执行时间耗完，或是指定的I/Osize完成。如果这个选项设置的话，将会添加两个状态，总的错误计数和第一个error。允许的值是 none 全部IO或检验错误后，都会退出 read 读错误时会继续执行，其它的错误会退出 write 写错误时会继续执行，其它的错误会退出 io 任何IO error时会继续执行，其它的错误会退出 verify 校验错误时会继续执行，其它的错误会退出 all 遇到所有的错误都会继续执行 uid=int,不是使用调用者的用户来执行，而是指定用户ID gid=int,设置group id]]></content>
  </entry>
  <entry>
    <title><![CDATA[centos使用yum安装python3以及pip3]]></title>
    <url>%2F2018%2F07%2F30%2Flinux%2Fcentos%E4%BD%BF%E7%94%A8yum%E5%AE%89%E8%A3%85python3%E4%BB%A5%E5%8F%8Apip3%2F</url>
    <content type="text"><![CDATA[因为CentOS 7上默认的Python版本是2.7, 所以我们可以通过添加其他源方式再安装Python3.6. 安装EPEL和IUS软件源 12yum install epel-releaseyum install https://centos7.iuscommunity.org/ius-release.rpm 安装Python3.6 1yum install python36u 创建python3连接符 1ln -s /bin/python3.6 /bin/python3 安装pip3 1yum install python36u-pip 创建pip3链接符 1ln -s /bin/pip3.6 /bin/pip3 这样就完成了 安装一些常用的支持 1234pip3 install requestspip3 install pymysqlpip3 install xmltodictpip3 install six Yum 简介Yum（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。 常用的 Yum 命令1、显示已经安装的软件包1yum list installed 2、查找可以安装的软件包 （以 tomcat 为例）1yum list tomcat 3、安装软件包 （以 tomcat 为例）1yum install tomcat 4、卸载软件包 （以 tomcat 为例）1yum remove tomcat 5、列出软件包的依赖 （以 tomcat 为例）1yum deplist tomcat 6、-y 自动应答yes在安装软件的时候，会有中断，让用户选择是否要继续，如下图： 我们可以用 -y 来应答所有的 yes , 比如我们安装 tomcat 的时候，用下面的命令，将安装任务一气呵成，不会中断。1yum -y install tomcat 7、info 显示软件包的描述信息和概要信息以 tomcat 为例1yum info tomcat 8、升级软件包升级所有的软件包1yum update 升级某一个软件包 ，以升级 tomcat 为例1yum update tomcat 检查可更新的程序1yum check-update]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IOmeter测试指导手册]]></title>
    <url>%2F2018%2F07%2F30%2F%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%2FIOmeter%E6%B5%8B%E8%AF%95%E6%8C%87%E5%AF%BC%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[IOmeter 是一个工作在单系统和集群系统上用来衡量和描述 I/O 子系统的工具。 IOmeter介绍IOmeter 既是工作负载生成器（也就是说，它可以进行输入输出操作，以便增加系统的负荷），它还是一个测量工具（也就是说，它检查并且记录 I/O 操作的性能和对系统的影响）。它可以被配置为模拟任何程序或者基准测试程序的磁盘和网络 I/O 的负载，或者用来产生整个综合的 I/O 负载。它也可以用来产生并测量单系统或者多系统（网络）的负载。 IOmeter 可以被用来测量和描述： 磁盘和网络控制器的性能 总线的带宽和时延容量 对于附带驱动器的网络吞吐量 共享总线的性能 系统级别的硬件驱动的性能 系统级别的网络性能 组成部分IOmeter 包含了两个程序，IOmeter 和 Dynamo. IOmeter 是控制程序。使用图形用户接口（GUI），你可以配置负载，设置操作参数，启动和停止测试。IOmeteter 告诉Dynamo 去做什么，搜集分析数据，将分析数据输出到文件中。在某一时刻，只能有一个IOmeteter副本运行；典型的情况是运行在服务器上。 Dynamo 是负载生成器。它没有用户界面。当接收到IOmeteter 发送过来的命令，Dynamo执行相应的I/O 操作并且记录性能信息，然后将数据返回给IOmeteter。它可以有多个副本同时运行；典型的情况是服务器上运行一个副本，每个客户端运行一个副本。Dynamo 是多线程的；每一个副本都可以模拟多客户程序的工作负载。Dynamo中的每一个运行的副本称为一个管理者（Manager）；副本中的一个线程称为工作者（Worker）。 版本下载IOmeter 是一个开源工具，可以到 http://www.iometer.org/doc/downloads.html 下载，也可以点击此处下载1.1.0版本备份。截至2018年7月30日，最新版本是 1.1.0。因更新内容很少，最常使用的版本仍为 2006.07.27。 下面我们就以 iometer-2006.07.27.win32.i386-setup.exe 为例进行介绍。 安装Windows 下的安装Windows 下的安装文件为 iometer-2006.07.27.win32.i386-setup.exe，安装过程如下： 双击打开安装文件，点击 next 和同意协议-&gt;选择加入 MS Access 导入导航-&gt;选择安装目录-&gt;安装完成。 Linux 下的安装Linux 下的安装文件为 iometer-2006_07_27.linux.i386-bin.gz，解压完成后就可以用了： 解压该文件：tar xzvf iometer-2006_07_27.linux.i386-bin.gz 注： Linux 下只有 dynamo 这个 agent，要想控制还得要在 Windows 的一台主机上安装 IOmeteter的 console。 IOmeter使用 打开 IOmeteter，上面按钮的作用从左到右依次是： 打开已有的测试配置文件 保存现在的测试配置文件 新增一个是负载生成器（Dynamo 支持多线程，一般情况下不需要在同一台主机上运行多个Dynamo 新增一个 Disk Worker（后面会介绍通过修改 OutStanding I/Os 数来增大压力，我在跑 IOmeteter 的时候，每台主机只保留一个 Disk Worker） ； 在选定的主机下，点击该按钮可以增加一个 Network Worker 选中某一个 Disk Worker 或 Network Worker，让后点击该按钮，会复制出一个完全一样的 Worker 开始测试； 停止当前测试并保存结果； 中断所有测试； 将 Workers 复位（reset 掉当前的配置，相当于把 IOmeteter 关了再重新打开）； 删除,可以方便地删除 Topology 窗体下的 Manager、Worker 退出软件； 关于 IOmeteter，显示版本和开源许可信息。 在 Windows 主机打开 IOmeterConsole 如下图所示，默认会在 Topology 里面出现本机以及几个 Disk Worker（软件会根据主机 CPU 个数来决定增加几个 Disk Worker），在这里，点选中 Worker2，然后点击 删除按钮，只保留一个 Worker。 大家还会注意到，运行该工具的同时，任务栏多了一个 Dos 窗口，没错，根据第一章介绍的，这个 Dos 窗口才是真正的负载生成器，而上面的图形窗口只是一个控制台。 可以做一个实验，点击 新增负载生成器按钮增加一个 Manager，我们发现在 Topology 里面多了一台主机，任务栏也多了一个负载生成器的 Dos 窗口。 Disk Target 界面及参数介绍如下图，框 1 中指的是 Disk Worker 1 会在用户盘 D 上进行 IO 测试； 框 2 中“Maximum Disk Size”单位是 Sectors(扇区)，意思是说 Worker 1 会在 D 盘占用多少个扇区进行压力测试。 默认填 0，会将 D 盘的剩余空间全用完。（Windows 系统每个扇区的大小是 512Byte，所以这里填 20,000,000 就是指 Worker 1 会占用 D 盘 10GB 的空间，另外在做 FCSAN/IPSAN 磁盘测试时设置的大小推荐大于 SAN 设备缓存的 2 倍，太小的话 IOmeteter 只跑在缓存上，跑出来的性能会偏高不准确。但是从里一个方面来说，如果我们想让最终的结果看起来好看，那么我们可以将该参数设置的小一点，根据测试目的自己把握）；“Starting Disk Sector”即 Worker1 从哪个扇区开始写它的 iobw.tst 测试文件，这里保持默认的 0即可。 框 3 中“# of Outstanding I/Os”指的是 Worker 1 在 D 盘上同时会开多少个异步的 IO 操作，在主机的CPU、内存能力够强时，并发数越多最终跑出来的结果会越准确，默认的是 1 个。 具体设置为多少比较好，我们可以实际试一下：先跑 1 个，看看 Result 是多少，5 个时 Result 是多少，10 个、20 个、50 个、100 个、120 个…… 在 CPU 内存承受范围内，找到一个最合适的值。 框 4 中“Test Connection Rate”指的是 Worker 1 以什么样的操作频率频率打开、关闭 D 盘。 默认不勾选的意思是，所有的连接都是 open 状态，直到测试停止。这里我们保持默认即可。 Network Targets 界面及参数介绍下图是在用 IOmeteter 跑网络压力时截的图，这里需要两台主机对跑网络，本例中不涉及，先不去管它，后面章节会详细讲到。 Access Specifications 界面及参数介绍Access Specification 是 IOmeteter 工具根据什么样的规格来跑 IO 测试。这里我们每次只添加一个规格，如下图。（如果添加多个规格的话，IOmeteter 会按照从上到下的顺序依次跑每个规格，直到测试停止，用来模拟复杂场景） 如上图，Access Specification，框 1 中是已经定义好的规格。框 2 中可以对 Global Access Specification 做相应的新建、编辑、复制并编辑、删除操作。 框 3 中是已经添加的 Worker1 里面的线程需要在 Target Disk 里面跑的压力的规格。一般我们只跑一个就好了。 当然我们也可以 Add 多个规格来模拟复杂的应用，假设我们 Add 了 4 个规格，IOmeteter 会依次从上到下的顺序来执行相应的读写操作。 规格的具体参数是什么，又有什么意义和影响呢？ 我们用下面的截图说明，新建一个规格，并分析里面的参数： 框 1 “Name”，需要给新建的规格定义一个易懂的名字，比如用参数，或者“Web server workload”等形象的名字； 框 2 “Default Assignment”，我们保持默认的 none 就好了（这一项的意思是，例如你设置某个规格的“Default Assignment”为 Disk Workers 就会发现,每当你新增一个 Disk Worker 时，这个规格会被默认添加到该 Disk Worker 的“Assigned Access Specifications”里面； 同理我们把某个规格的“Default Assignment”设置为 All Workers，那么每当我们新增加一个 Worker 时，这个规格都会被默认加到该 Worker 的“AssignedAccess Specifications”里面。该选项对测试结果没有任何影响，可以不设置，保持默认。） 框 3 可以看到该规格的配置信息，这里我们只跑一个规格，所以不用管它。 框 4 每次 IO 的大小，这个值越小则跑出来的结果里面的 IOPS 就会越大，带宽就会越小。相反，这个值越大，跑出来的结果里面的 IOPS 就会越小，带宽就会越大。只能填写 512Byte 的整数倍，否则跑的时候会报错。 框 5，Percent of Access Specification（该规格在所有规格中占用的连接百分比），由于这里我们只跑一个规格，所以保持默认 100%就可以了。 框 6，读写比例，这个参数对结果影响也很大，因为读的性能是比写的性能好很多的 框 7，随即/顺序读写的百分比，这个参数对结果影响也很大，因为顺序读写，比随机读写的性能好很多。 框 8，可以模拟定量的 IOPS，比如在 Delay 里面填 1000（单位毫秒，1000 毫秒即 1 秒钟），然后在 Burst Length 里面填 10. 意思就是在一秒钟会发生 10 个 IOPS。 框 9、10，保持默认即可。 在测试中，我们通过改变三个参数，即上面黑体部分的块大小、读写百分比、随机顺序百分比来模拟不同场景的应用。 如果我们想得到 IOPS 的极限，那么我们可以用“512Byte; 100%Read; 0%random”规格来跑。 如果我们想得到带宽的极限，那么我们可以用“32KB（可以用 64K、128K、1024K 或者更大，这需要调测几次看哪个值能够得到带宽最大值）; 100%Read; 0%random”的规格来跑。 传输数据块大小在应用服务器类型测试为 4KB，数据库服务器类型测试为 8KB；读写百分比在应用服务器类型测试为读 100%，数据库服务器类型测试根据实际情况来判断，如纯查询的数据库读 100%，一个典型的业务系统的数据库系统，按照默认的 67%读即可；随机/连续存取百分比在应用服务器类型测试为 100%，数据库服务器类型测试为 100%；（测试人员可根据实际情况修改此处数值，典型的 OLAP 环境：选择顺序的大 IO，测试存储所能支持的最大吞吐量以及响应时间；典型的 OLTP 环境：选择随机的小 IO，测试存储所能支持的最大 IOPS 以及响应时间）。 Test Setup 界面可以设置测试跑多长时间，即“Run Time”。 设置“Ramp Time”的目的是保证结果更准确，因为 IO 测试开始的前几秒钟得出的数据与实际数据误差较大，例如我们这里设置“15”的意思是，真正的 IO 测试开始 15 秒钟后，才会在“Result Display”界面中记录测试结果。 其它设置保持默认。 Result Display 界面Update Frequency，指下面的蓝色条的结果更新的频率，默认是无穷大，如果不点击鼠标修改的话会一直看不到更新的结果。当进行一项高负载测试时，不建议设置更新频率过小，否则会影响系统性能。 Results Since，选择上面的 Start of Test 是说从记录结果开始一直到现在的平均结果。 选择下面的 Last Update 意思是，只显示最近 5 秒钟的性能结果。 这个界面可以通过点击鼠标，来选择显示不同的参数，动手试试吧。 Windows 下单机跑 IOmeteter熟悉了上面讲的参数的意思后，Windows 上单机跑 IOmeteter 就很简单了。将参数设置好，点击 start Tests，根据提示选择日志日志保存的位置和文件名 xxx.csv 就 OK 了，现在就在你的 PC 机上练练手吧。（PC 机性能有限，建议将# of Outstandings 设置在 10 以下，以免 PC 机卡死） Windows 下用 IOmeteter 跑网络压力有两台虚拟机，VM54 和 VM58，以这两台 VM 为例介绍如何用 IOmeteter 工具跑网络压力。 在 VM54 打开 IOmeter 界面作为 Console 端，如下图在 VM58 打开 CMD 进入 IOmeteter 的安装目录路径下，然后输入命令 1Dynamo –i vm54(或者 VM54 的 IP 地址，-i 是指定 console 端) –m VM58（或 VM58 的IP，-m 是指定 Worker） 注意，要先打开 Console 端再在 client 端运行该命令，否则 Console 找不到 client 这样，如图在 VM54 的控制台上出现 VM58: 由于这里只演示跑网络压力，所以将 Disk Worker move 掉。 在 VM54 增加 8 个 Network 的 Worker，并添加 VM58 为 Targets，如下图 增加如下规格的连接 开始 run，如下图，网络利用达到了 100% 测试结果 自动化脚本根据帮助文档查看 IOmeter 参数打开 cmd，在 IOmeteter 目录下，输入命令 iometer ？ 得到以下帮助文档 自动化测试脚本如果有多台机器要跑相同的 IOmeteter 测试，就可以先在一台机器上将参数设置好，并保存为配置文件xxx.icf。 我们把这个配置文件拷贝到每台机器的 IOmeteter 安装目录下，同时日志也放到安装目录下,创建如下脚本，并保存为.bat 文件，双击就可以运行了 12cd &quot;\Program Files\IOmeteter.org\IOmeteter 2006.07.27&quot;iometer iometer_config.icf iometer_result.csv 性能指标本章主要讲述在结果显示页签中各个性能指标的含义。 Opretions per second Total I/Os per Second(IOPS)：每秒 I/O 次数，包含读 I/O 和写 I/O。对于磁盘来说，一次磁头的连续读或者连续写就是一次 I/O。 Read I/Os per Second：每秒读 I/O 次数。 Write I/Os per Second：每秒写 I/O 次数。 Transaction per Second：每秒事务处理数。当在存取规则中设置 Reply Size 为”no reply”时，Transaction per Second=IOPS，即事务只包含发送数据块；当在存取规则中设置了 Reply Size 不为 0 时，Transaction per Second 将发送和接收数据块作为一个事务。 Connections per Second：每秒连接数 Megabytes per second Total MBs per Second：每秒数据传输量，也就是常说的吞吐量，包含读取和写入。Total MBs per Second=IOPS*传输数据块大小= Transaction per Second*（传输数据块+接收数据块） Read MBs per Second：每秒读取数据量。 Write MBs per Second：每秒写入数据量。 Avarange latency Average I/O Response Time(ms)：平均 I/O 响应时间。 Avg.Read Response Time(ms)：平均读 I/O 响应时间。 Avg.Write Response Time(ms)：平均写 I/O 响应时间。 Avg. Transaction Time(ms)：平均事务处理时间。 Avg. Connections Time(ms)：平均连接时间。 Maximum latency Maximum I/O Response Time(ms)：最大 I/O 响应时间。 Max.Read Response Time(ms)：最大读 I/O 响应时间。 Max.Write Response Time(ms)：最大写 I/O 响应时间。 Max. Transaction Time(ms)：最大事务处理时间。 Max. Connections Time(ms)：最大连接时间。 CPU %CPU Utilization（total）：CPU 占用率。 %User Time：非内核级应用程序占用时间。 %Privileged Time：CPU 内核程序占用时间，是在特权模式下处理线程执行代码所花时间的百分比 %DPC Time：处理器在网络处理上消耗的时间。%DPC Time 是%Privileged Time 的一部分。 %Interrupt Time：中断时间是在采样间隔期间接收和处理硬件中断处理器花费的时间百分比，如系统时钟，鼠标，磁盘驱动器，数据通信线路，网络接口卡和其它外围设备。 Interrupts per Second：每秒中断数。 CPU Effectiveness：CPU 效率的一个表征：将 IOPS 除以%CPU Utilization 即可得到。 Network Network Packets per Second：每秒网络数据包发送/接收数。 Packets Errors：错误包个数。 TCP Segments Retrans.per Sec：TCP 数据段每秒返回数。 Errors Total Error Count：总错误数。 Read Error Count：读取错误数。 Write Error Count：写入错误数。 参考文档IOmeter测试指导手册]]></content>
  </entry>
  <entry>
    <title><![CDATA[javascript_es6入门]]></title>
    <url>%2F2018%2F07%2F25%2F%E5%89%8D%E7%AB%AF%2Fjavascript-es6%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[async 函数ES2017 标准引入了 async 函数，使得异步操作变得更加方便。async 函数是什么？一句话，它就是 Generator 函数的语法糖 定义一个async函数:123456789101112async function timeout(ms) &#123; await new Promise((resolve) =&gt; &#123; setTimeout(resolve, ms); &#125;);&#125;async function asyncPrint(value, ms) &#123; await timeout(ms); console.log(value);&#125;asyncPrint('hello world', 50); async的返回对象async函数返回一个 Promise 对象。 async函数内部return语句返回的值，会成为then方法回调函数的参数。 123456async function f() &#123; return 'hello world';&#125;// "hello world"]]></content>
  </entry>
  <entry>
    <title><![CDATA[深入理解正则表达式]]></title>
    <url>%2F2018%2F07%2F25%2Fpython%2F2.python%E6%A0%87%E5%87%86%E5%BA%93%2Fpython%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F2%2F</url>
    <content type="text"><![CDATA[概念一：按单字符匹配正则里面的数据都是按照单个字符来进行匹配的，这个通过数值区间的例子最容易体现出来，比如，示例一： 我要匹配0-15的数值区间，用正则来写的话，便是[0-9]|1[0-5]，这里，便是把0-9这种单字符的情况，和10-15这种多字符的情况拆分开了，使用分支|来区分开，表示要么是0-9，要么是10-15。上面是两位数值的情况，现在延伸至1-65535，我个人的处理思想是从大到小，一块块分解： 1234561. 65530-65535 ==&gt; 6553[0-5] 末位区间0-52. 65500-65529 ==&gt; 655[0-2][0-9] 第四位区间0-2，末位区间0-93. 65000-65499 ==&gt; 65[0-4][0-9]&#123;2&#125; 第三位区间0-4，后两位0-94. 60000-64999 ==&gt; 6[0-4][0-9]&#123;3&#125; 第二位区间0-4，后三位0-95. 10000-59999 ==&gt; [1-5][0-9]&#123;4&#125; 第一位区间1-5，后四位0-96. 1-9999 ==&gt; [1-9][0-9]&#123;0,3&#125; 第一位只能是1-9，后三位可有可无 最后组合起来： 1(6553[0-5]|655[0-2][0-9]|65[0-4][0-9]&#123;2&#125;|6[0-4][0-9]&#123;3&#125;|[1-5][0-9]&#123;4&#125;|[1-9][0-9]&#123;0,3&#125;) 便得到1-65535匹配正则。 根据数据处理需求，可以在正则前后加上^和$，以匹配整个数据串，或者前后加入\b，把它当做单词边界处理。没有限定字符的边界往往是js正则判断中常见的错误之一。 概念二：匹配优先和不匹配优先匹配优先和不匹配优先从字面理解也是比较容易的，所谓匹配优先，就是，能匹配我就先匹配；不匹配优先就是，能不匹配我就先不匹配，这段匹配先跳过，先看看后面的匹配能不能通过。 概念三：贪婪模式与非贪婪模式正则的贪婪模式和非贪婪模式是一个比较容易混淆的概念，不过，我们可以这么理解，一个人很贪婪，所以他会能拿多少拿多少，换过来，那就是贪婪模式下的正则表达式，能匹配多少就匹配多少，尽可能最多。而非贪婪模式，则是能不匹配就不匹配，尽可能最少。 下面举个例子，示例二： 需求：匹配1后面跟任意个0源串：10001使用贪婪模式：10 结果：1000 和 1使用非贪婪模式：10? 结果：1 和 1首先，*是匹配0个或多个的意思。 贪婪模式下，它表示，首先匹配一个1，然后匹配1后面的0，最多可以匹配3个0，因此得到1000，然后第二次又匹配到一个1，但是后面没有0，因此得到1；非贪婪模式下，它表示，首先匹配一个1，然后1后面的0，能不匹配就不匹配了，所以，它每次都只是匹配了一个1。看到这里，也许，有些人觉得，哎呀，我懂了！那么，下来我们改改，看看你是不是真懂了。 示例三： 需求：匹配1后面跟任意个0，再跟一个1源串：10001使用贪婪模式：101 结果：10001使用非贪婪模式：10?1 结果：10001为什么这两次的结果一样了？ 因为，正则表达式要判断完这整个正则才算成功，这种情况下， 贪婪模式，首先匹配一个1，然后匹配1后面尽可能多的0，发现3个，再匹配0后面的一个1，正则表达式匹配完，完成匹配，得到10001；非贪婪模式，首先匹配一个1，然后，0*?是非贪婪模式，它不想匹配了（非贪婪模式不匹配优先），看看后面是不是1了？然后发现哎妈呀，后面是个0啊，然后它回头，不能再偷懒了，用0*?匹配一个0吧，然后匹配到10，又不想匹配了，看看后面有没有1了，还是没有，又回去用0*?匹配掉一个0，得到100，继续偷懒，但是发现后面还不是1，然后又用0*?匹配得到1000，最后，发现真不容易啊，终于看到1了，匹配得到10001，正则表达式匹配完，完成匹配。看到这里，是不是懂了？那究竟哪个模式好呢？ 什么时候使用贪婪模式，什么时候使用非贪婪模式，哪个性能好，哪个性能不好，不能一概而论，要根据情况分析。下面我举个例子：源码： 12&lt;a href=&quot;http://www.zjmainstay.cn/my-regexp&quot; target=&quot;_blank&quot; title=&quot;我眼里的正则表达式（入门）&quot;&gt;我眼里的正则表达式（入门）&lt;/a&gt;&lt;a title=&quot;我眼里的正则表达式（入门）&quot; href=&quot;http://www.zjmainstay.cn/my-regexp&quot; target=&quot;_blank&quot;&gt;我眼里的正则表达式（入门）&lt;/a&gt; 正则1：&lt;a [^&gt;]*?href=&quot;([^&quot;]*?)&quot;[^&gt;]*?&gt;([^&lt;]*?)&lt;/a&gt;（238次）正则2：&lt;a [^&gt;]*?href=&quot;([^&quot;]*)&quot;[^&gt;]*&gt;([^&lt;]*)&lt;/a&gt;（65次）正则3：&lt;a [^&gt;]*href=&quot;([^&quot;]*)&quot;[^&gt;]*&gt;([^&lt;]*)&lt;/a&gt;（136次）附：执行次数的获取请下载正则表达式测试工具：RegexBuddy 4.1.0-正则测试工具.rar，使用里面的Debug功能。 正则1是通用写法，正则2是在确定字符不会溢出的情况下消除非贪婪模式，正则3是证明并不是全部消除非贪婪模式就是最优。 因此，关于贪婪模式好还是非贪婪模式好的讨论，只能说根据需求而定，不过，在平时的时候用，一般使用非贪婪模式较多，因为贪婪模式经常会由于元字符范围限制不严谨而导致匹配越界，得到非预期结果。 在确定的数据结构里，可以尝试使用[^&gt;]*&gt;这样的排除字符贪婪模式替换非贪婪模式，提升匹配的效率。注意，贪婪部分（[^&gt;]*）的匹配，最好不要越过其后面的字符（&gt;），否则会导致贪婪模式下的回溯，如正则3，[^&gt;]*的匹配越过了href，一直匹配到&gt;为止，而这时候再匹配href，会匹配不到而导致多次回溯处理，直到回溯到href前的位置，后面才继续了下去。 另外，需要注意一点，无论使用贪婪模式还是非贪婪模式，在不同语言需要注意回溯次数和嵌套次数的限制，比如在PHP中，pcre.backtrack_limit=100000，pcre.recursion_limit=100000。 概念四：环视（断言/零宽断言）环视，在不同的地方又称之为零宽断言，简称断言。用一句通俗的话解释：环视，就是先从全局环顾一遍正则，（然后断定结果，）再做进一步匹配处理。断言，就是先从全局环顾一遍正则，然后断定结果，再做进一步匹配处理。 两个虽然字面不一样，意思却是同一个，都是做全局观望，再做进一步处理。 环视的作用相当于对其所在位置加了一个附加条件，只有满足这个条件，环视子表达式才能匹配成功。 环视主要有以下4个用法：(?&lt;=exp) 匹配前面是exp的数据(?&lt;!exp) 匹配前面不是exp的数据(?=exp) 匹配后面是exp的数据(?!exp) 匹配后面不是exp的数据 示例四：(?&lt;=B)AAA 匹配前面是B的数据，即BAAA匹配，而CAAA不匹配(?&lt;!B)AAA 匹配前面不是B的数据，即CAAA匹配，而BAAA不匹配AAA(?=B) 匹配后面是B的数据，即AAAB匹配，而AAAC不匹配AAA(?!B) 匹配后面不是B的数据，即AAAC能匹配，而AAAB不能匹配 另外，还会看到(?!B)[A-Z]这种写法，其实它是[A-Z]范围里，排除B的意思，前置的(?!B)只是对后面数据的一个限定，从而达到过滤匹配的效果。 因此，环视做排除处理是比较实用的，比如，示例五： 需求：字母、数字组合，不区分大小写，不能纯数字或者纯字母，6-16个字符。通用正则：^[a-z0-9]{6,16}$ 字母数字组合，6-16个字符 排除纯字母：(?!^[a-z]+$)排除纯数字：(?!^[0-9]+$) 组合起来：(?!^[a-z]+$)(?!^[0-9]+$)^[a-z0-9]{6,16}$注意，环视部分是不占宽度的，所以有零宽断言的叫法。所谓不占宽度，可以分成两部分理解： 1、环视的匹配结果不纳入数据结果2、环视它匹配过的地方，下次还能用它继续匹配。 如果不是环视，则匹配过的地方，不能再匹配第二次了。 上面示例四体现了：环视的匹配结果不纳入数据结果，它的结果： 1234(?&lt;=B)AAA 源串：BAAA 结果：AAA(?&lt;!B)AAA 源串：CAAA 结果：AAAAAA(?=B) 源串：AAAB 结果：AAAAAA(?!B) 源串：AAAC 结果：AAA 而示例五体现了：环视它匹配过的地方，下次还能用它继续匹配因为，整个匹配过程中，正则表达式一共走了3次字符串匹配，第一次匹配不全部是字母，第二次匹配不全部是数字，第三次匹配全部是字母数字组合，6-16个字符。 扩展部分：[A-Z](?&lt;=B) [A-Z]范围等于B[A-Z](?&lt;!B) [A-Z]范围排除B(?!B)[A-Z] [A-Z]范围排除B js不支持(?&lt;=exp) 和 (?&lt;!exp) 语法 python中零宽度断言：前向环视括号中的表达式必须是常值 ，但是支持[abcd],[^abcd],否则会报error: look-behind requires fixed-width pattern错误后向环视括号中的表达式可以是正则(.*?) 概念五：平衡组平衡组并不是所有程序语言都支持，而我本人使用的PHP语言就不支持，所以平时接触也是比较少的。 平衡组主要用到下面四个语法： 12345(?&apos;group&apos;) 把捕获的内容命名为group,并压入堆栈(Stack)(?&apos;-group&apos;) 从堆栈上弹出最后压入堆栈的名为group的捕获内容，如果堆栈本来为空，则本分组的匹配失败(?(group)yes|no) 如果堆栈上存在以名为group的捕获内容的话，继续匹配yes部分的表达式，否则继续匹配no部分(?!) 零宽负向先行断言，由于没有后缀表达式，如没有(?!B)的B，试图匹配总是失败在PHP中是支持(?(group)yes|no)语法的，这里的group是分组编号，即子模式编号，如(A)?(?(1)yes|no) ，匹配Ayes 和 no 下面这里引用《正则表达式30分钟入门教程#平衡组》关于&lt;&gt;配对匹配的例子，展示平衡组用法， 12345678910111213141516&lt; #最外层的左括号 [^&lt;&gt;]* #最外层的左括号后面的不是括号的内容 ( ( (?&apos;Open&apos;&lt;) #碰到了左括号，在黑板上写一个&quot;Open&quot; [^&lt;&gt;]* #匹配左括号后面的不是括号的内容 )+ ( (?&apos;-Open&apos;&gt;) #碰到了右括号，擦掉一个&quot;Open&quot; [^&lt;&gt;]* #匹配右括号后面不是括号的内容 )+ )* (?(Open)(?!)) #在遇到最外层的右括号时，判断黑板上还有没有没擦掉的&quot;Open&quot;；如果还有，则匹配失败&gt; #最外层的右括号平衡组的一个最常见的应用就是匹配HTML,下面这个例子可以匹配嵌套的&lt;div&gt;标签：&lt;div[^&gt;]*&gt;[^&lt;&gt;]*(((?&apos;Open&apos;&lt;div[^&gt;]*&gt;)[^&lt;&gt;]*)+((?&apos;-Open&apos;&lt;/div&gt;)[^&lt;&gt;]*)+)*(?(Open)(?!))&lt;/div&gt; 概念六：匹配次数 *:匹配前一个表达式0次或多次。等价于 {0,} +:匹配前面一个表达式1次或者多次。等价于 {1,} ?:匹配前面一个表达式0次或者1次。等价于 {0,1} {n,m}: n 和 m 都是整数。匹配前面的字符至少n次，最多m次 {n}:n是一个正整数，匹配了前面一个字符刚好发生了n次 {n,}: n是一个正整数，匹配前面一个字符至少n次 示例：下例中无论hello之前是否包含空格，都可以匹配到 12&apos; hello world!&apos;.match(/ *hello world!/)&apos;hello world!&apos;.match(/ *hello world!/) 概念七：模式修饰符模式修饰符在许多程序语言中都支持的，比如最常见的是i，不区分大小写，如javascript里的/[a-z0-9]/i，表示匹配字母数字，不区分大小写。 最后留下一句至尊提醒：.是万能字符，大家看着用，遇到换行使用[\s\S]替换.即可。 参考文档深入理解正则表达式高级教程]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[avocado自动化测试框架中文文档]]></title>
    <url>%2F2018%2F07%2F18%2F%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%2Favocado%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95%E6%A1%86%E6%9E%B6%E4%B8%AD%E6%96%87%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[关于AvocadoAvocado是一套帮助自动化测试的工具和库。 人们可以将其称为具有益处的测试框架。 本机测试是用Python编写的,它们遵循unitest模式,但任何可执行文件都可以作为测试。 Avocado由以下组成： 一个允许您执行测试的测试运行器。 这些测试可以用您选择的语言编写,也可以用Python编写并使用可用的库。 在这两种情况下,您都可以获得自动日志和系统信息收集等功能。 帮助您以简洁,富有表现力和强大的方式编写测试的库。 您可以在库和API中找到有关哪些库适用于测试编写者的更多信息。 可以扩展Avocado Framework并为其添加新功能的插件。 Avocado是建立在Autotest积累的经验基础上,同时改善其弱点和缺点。 Avocado尽可能地遵守标准的Python测试技术。 使用Avocado API编写的测试来自unittest类,同时添加了适用于功能和性能测试的其他方法。 测试运行器旨在帮助人们在提供各种系统和日志记录工具的同时运行他们的测试,并且如果您需要更多功能,那么您可以逐步开始使用API功能。 入门那些喜欢视频介绍的人,请看看其他资源。 无论哪种方式,使用Avocado的第一步显然是安装它。 安装AvocadoAvocado主要是用Python编写的,因此标准的Python安装是可行的,而且通常更可取。 如果您正在寻找特定于虚拟化的测试,请在完成Avocado安装后考虑查看Avocado-VT安装说明。 使用标准Python工具进行安装最简单的安装方法是通过pip。 在大多数可用Python 2.7和pip的POSIX系统上,只需一个命令即可执行安装： 译者注:虽然python2.7是可用的,但是已经逐渐被淘汰了,因此建议使用python3.6+以及其相对应的pip进行安装 1pip install --user avocado-framework 这将从PyPI存储库中获取Avocado包(可能还有一些依赖项),并尝试将其安装在用户的主目录中(通常在〜/ .local下)。 如果要执行系统范围的安装,请删除 –user删除。译者:如果希望在命令行启用 avocado 命令的话,安装时不能使用 –user 参数 如果您想要更多隔离,Avocado也可以安装在Python虚拟环境中。 除了创建和激活虚拟环境本身之外没有其他步骤： 123python -m virtualenv /path/to/new/virtual_environment. /path/to/new/virtual_environment/bin/activatepip install avocado-framework 请注意,这将安装Avocado核心功能。 许多Avocado功能都作为非核心插件分发,也可作为PyPI上的附加软件包提供。 你应该能够通过pip search avocado-framework-plugin | grep avocado-framework-plugin找到它们。其中一些列在下面： avocado-framework-plugin-result-html: HTML报告 avocado-framework-plugin-resultsdb: 将job结果传播到Resultsdb avocado-framework-plugin-runner-remote: 用于远程执行的运行器 avocado-framework-plugin-runner-vm: 用于libvirt VM执行的运行器 avocado-framework-plugin-runner-docker: Docker容器上执行的Runner avocado-framework-plugin-loader-yaml: 从YAML文件加载测试 avocado-framework-plugin-robot: 执行Robot Framework测试 avocado-framework-plugin-varianter-yaml-to-mux: 将YAML文件解析为变量 从包安装原文介绍了一些Avocado其它的安装方法,有兴趣可以去原址查看 使用Avocado您应首先使用测试运行器体验Avocado,即命令行工具,它将方便地运行您的测试并收集其结果。 运行测试为此,请使用run子命令运行Avocado,run后面跟随要进行的测试,它可以是文件的路径,也可以是可识别的名称： 1234567$ avocado run /bin/trueJOB ID : 381b849a62784228d2fd208d929cc49f310412dcJOB LOG : $HOME/avocado/job-results/job-2014-08-12T15.39-381b849a/job.log (1/1) /bin/true: PASS (0.01 s)RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0JOB TIME : 0.11 sJOB HTML : $HOME/avocado/job-results/job-2014-08-12T15.39-381b849a/html/results.html 您可能已经注意到我们使用/ bin / true作为测试,并且根据我们的期望,它通过了！ 这些被称为简单测试,但也有另一种类型的测试,我们称之为仪器测试。 在测试类型中查看更多信息或继续阅读。 虽然在大多数情况下运行Avocado运行 $ test1 $ test3 …很好,但它可能导致参数与测试名称冲突。 最安全的执行测试的方法是Avocado运行 - $ argument1 - $ argument2 - $ test1 $ test2。 之后的所有内容 - 将被视为位置参数,即测试名称(在Avocado运行的情况下) 列出测试您有两种方法来检测测试文件。 您可以使用–dry-run参数来模拟执行： 1234567avocado run /bin/true --dry-runJOB ID : 0000000000000000000000000000000000000000JOB LOG : /tmp/avocado-dry-runSeWniM/job-2015-10-16T15.46-0000000/job.log (1/1) /bin/true: SKIPRESULTS : PASS 0 | ERROR 0 | FAIL 0 | SKIP 1 | WARN 0 | INTERRUPT 0JOB TIME : 0.10 sJOB HTML : /tmp/avocado-dry-runSeWniM/job-2015-10-16T15.46-0000000/html/results.html 它支持所有运行参数,模拟运行甚至列出测试参数。 另一种方法是使用list子命令列出发现的测试如果没有提供参数,Avocado会为每个插件列出“默认”测试。 输出可能如下所示： 译者: avocado list . 列出当前目录的avocado测试,直接使用avocado list未返回结果。 1234567891011121314151617181920$ avocado listINSTRUMENTED /usr/share/doc/avocado/tests/abort.pyINSTRUMENTED /usr/share/doc/avocado/tests/datadir.pyINSTRUMENTED /usr/share/doc/avocado/tests/doublefail.pyINSTRUMENTED /usr/share/doc/avocado/tests/doublefree.pyINSTRUMENTED /usr/share/doc/avocado/tests/errortest.pyINSTRUMENTED /usr/share/doc/avocado/tests/failtest.pyINSTRUMENTED /usr/share/doc/avocado/tests/fiotest.pyINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.pyINSTRUMENTED /usr/share/doc/avocado/tests/gendata.pyINSTRUMENTED /usr/share/doc/avocado/tests/linuxbuild.pyINSTRUMENTED /usr/share/doc/avocado/tests/multiplextest.pyINSTRUMENTED /usr/share/doc/avocado/tests/passtest.pyINSTRUMENTED /usr/share/doc/avocado/tests/sleeptenmin.pyINSTRUMENTED /usr/share/doc/avocado/tests/sleeptest.pyINSTRUMENTED /usr/share/doc/avocado/tests/synctest.pyINSTRUMENTED /usr/share/doc/avocado/tests/timeouttest.pyINSTRUMENTED /usr/share/doc/avocado/tests/warntest.pyINSTRUMENTED /usr/share/doc/avocado/tests/whiteboard.py... Avocado认为这些Python文件包含INSTRUMENTED测试。 现在让我们列出可执行的shell脚本： 123456$ avocado list | grep ^SIMPLESIMPLE /usr/share/doc/avocado/tests/env_variables.shSIMPLE /usr/share/doc/avocado/tests/output_check.shSIMPLE /usr/share/doc/avocado/tests/simplewarning.shSIMPLE /usr/share/doc/avocado/tests/failtest.shSIMPLE /usr/share/doc/avocado/tests/passtest.sh 这里,如前所述,SIMPLE意味着这些文件是可执行文件,被视为简单测试。 您还可以使用–verbose或-V标志来显示Avocado找到的文件,但不被视为Avocado测试： 1234567891011$ avocado list examples/gdb-prerun-scripts/ -VType Test Tag(s)NOT_A_TEST examples/gdb-prerun-scripts/READMENOT_A_TEST examples/gdb-prerun-scripts/pass-sigusr1TEST TYPES SUMMARY==================SIMPLE: 0INSTRUMENTED: 0MISSING: 0NOT_A_TEST: 2 请注意,详细标志还会添加摘要信息。 写一个简单的测试这个用shell脚本编写的简单测试的简单例子 123$ echo '#!/bin/bash' &gt; /tmp/simple_test.sh$ echo 'exit 0' &gt;&gt; /tmp/simple_test.sh$ chmod +x /tmp/simple_test.sh 請注意,該文件具有可執行權限,這是Avocado將其視為簡單測試的要求。 另請注意,腳本以狀態代碼0退出,這表示Avocado成功結果。 运行更复杂的测试工作您可以按任意顺序运行任意数量的测试,以及混合和匹配仪器化测试和简单测试： 123456789101112$ avocado run failtest.py sleeptest.py synctest.py failtest.py synctest.py /tmp/simple_test.shJOB ID : 86911e49b5f2c36caeea41307cee4fecdcdfa121JOB LOG : $HOME/avocado/job-results/job-2014-08-12T15.42-86911e49/job.log (1/6) failtest.py:FailTest.test: FAIL (0.00 s) (2/6) sleeptest.py:SleepTest.test: PASS (1.00 s) (3/6) synctest.py:SyncTest.test: PASS (2.43 s) (4/6) failtest.py:FailTest.test: FAIL (0.00 s) (5/6) synctest.py:SyncTest.test: PASS (2.44 s) (6/6) /tmp/simple_test.sh.1: PASS (0.02 s)RESULTS : PASS 4 | ERROR 0 | FAIL 2 | SKIP 0 | WARN 0 | INTERRUPT 0JOB TIME : 5.98 sJOB HTML : $HOME/avocado/job-results/job-2014-08-12T15.42-86911e49/html/results.html 在第一次失败的测试中中断job(failfast)Avocado运行命令具有选项--failfast on以在遇到第一次失败的测试时退出测试,后面的用例不再继续执行： 123456789$ avocado run --failfast on /bin/true /bin/false /bin/true /bin/trueJOB ID : eaf51b8c7d6be966bdf5562c9611b1ec2db3f68aJOB LOG : $HOME/avocado/job-results/job-2016-07-19T09.43-eaf51b8/job.log (1/4) /bin/true: PASS (0.01 s) (2/4) /bin/false: FAIL (0.01 s)Interrupting job (failfast).RESULTS : PASS 1 | ERROR 0 | FAIL 1 | SKIP 2 | WARN 0 | INTERRUPT 0JOB TIME : 0.12 sJOB HTML : /home/apahim/avocado/job-results/job-2016-07-19T09.43-eaf51b8/html/results.html 在重新运行--failfast on执行的job时,也可以使用--failfast off强制禁用failfast模式。 忽略缺少的测试引用当您提供测试参考列表时,Avocado将尝试将所有测试参考解析为测试。如果无法将一个或多个测试引用解析为测试,则不会创建job。例： 12$ avocado run passtest.py badtest.pyUnable to resolve reference(s) &apos;badtest.py&apos; with plugins(s) &apos;file&apos;, &apos;robot&apos;, &apos;external&apos;, try running &apos;avocado list -V badtest.py&apos; to see the details. 但是如果你无论如何都想要执行这项测试,使用可以解决的测试,你可以使用--ignore-missing-references on。 UI中将显示相同的消息,但将执行这个测试： 12345678$ avocado run passtest.py badtest.py --ignore-missing-references onUnable to resolve reference(s) &apos;badtest.py&apos; with plugins(s) &apos;file&apos;, &apos;robot&apos;, &apos;external&apos;, try running &apos;avocado list -V badtest.py&apos; to see the details.JOB ID : 85927c113074b9defd64ea595d6d1c3fdfc1f58fJOB LOG : $HOME/avocado/job-results/job-2017-05-17T10.54-85927c1/job.log (1/1) passtest.py:PassTest.test: PASS (0.02 s)RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0 | CANCEL 0JOB TIME : 0.11 sJOB HTML : $HOME/avocado/job-results/job-2017-05-17T10.54-85927c1/html/results.html 使用外部运行器运行测试在大多数软件项目中使用逐渐增加的测试套件是很常见的。这些通常包括一个定制的,非常具体的测试运行器,它知道如何查找和运行自己的测试 尽管如此,由于各种原因,在Avocado中运行这些测试可能是一个好主意,包括能够以不同的人机和机器可读格式获得结果,收集系统信息以及这些测试(Avocado的sysinfo功能)等等 Avocado 通过其”external runner”功能实现了这一目标。最基本的使用方法是： 1$ avocado run --external-runner=/path/to/external_runner foo bar baz 在此示例中,Avocado将报告测试foo,bar和baz的各个测试结果。实际结果将基于/path/to/external_runner foo,/path/to/external_runner bar和/path/to/external_runner baz的单独执行的返回代码。其中/path/to/external_runner是你的外部解释器的路径。 作为另一种解释该功能如何工作的方法,可以将”external runner”视为某种解释器,并将个体测试视为此解释器识别并能够执行的任何内容。一个UNIX shell,比如/bin/sh可以被认为是一个外部运行器,带有shell代码的文件可以被认为是测试： 12345678910$ echo "exit 0" &gt; /tmp/pass$ echo "exit 1" &gt; /tmp/fail$ avocado run --external-runner=/bin/sh /tmp/pass /tmp/failJOB ID : 4a2a1d259690cc7b226e33facdde4f628ab30741JOB LOG : /home/&lt;user&gt;/avocado/job-results/job-&lt;date&gt;-&lt;shortid&gt;/job.log(1/2) /tmp/pass: PASS (0.01 s)(2/2) /tmp/fail: FAIL (0.01 s)RESULTS : PASS 1 | ERROR 0 | FAIL 1 | SKIP 0 | WARN 0 | INTERRUPT 0JOB TIME : 0.11 sJOB HTML : /home/&lt;user&gt;/avocado/job-results/job-&lt;date&gt;-&lt;shortid&gt;/html/results.html 这个例子非常明显,可以通过给/tmp/pass和/tmp/fail “shebangs”(#!/bin/sh)来实现,使它们可执行(chmod+x /tmp/pass /tmp/fail并将它们作为”SIMPLE”测试运行。 但现在考虑以下示例：123456789$ avocado run --external-runner=/bin/curl http://local-avocado-server:9405/jobs/ \ http://remote-avocado-server:9405/jobs/JOB ID : 56016a1ffffaba02492fdbd5662ac0b958f51e11JOB LOG : /home/&lt;user&gt;/avocado/job-results/job-&lt;date&gt;-&lt;shortid&gt;/job.log(1/2) http://local-avocado-server:9405/jobs/: PASS (0.02 s)(2/2) http://remote-avocado-server:9405/jobs/: FAIL (3.02 s)RESULTS : PASS 1 | ERROR 0 | FAIL 1 | SKIP 0 | WARN 0 | INTERRUPT 0JOB TIME : 3.14 sJOB HTML : /home/&lt;user&gt;/avocado/job-results/job-&lt;date&gt;-&lt;shortid&gt;/html/results.html 这有效地使/bin/curl成为”外部测试运行器”,负责尝试获取这些URL,并为每个URL报告PASS或FAIL。 调试测试显示测试输出在开发新测试时,您经常希望直接查看job日志,而无需切换屏幕或不必“拖尾”job日志。 为了实现它,你可以使用avocado --show test run ... 或者 avocado run --show-job-log选项 123456789101112$ avocado --show test run examples/tests/sleeptest.py...Job ID: f9ea1742134e5352dec82335af584d1f151d4b85START 1-sleeptest.py:SleepTest.testPARAMS (key=timeout, path=*, default=None) =&gt; NonePARAMS (key=sleep_length, path=*, default=1) =&gt; 1Sleeping for 1.00 secondsPASS 1-sleeptest.py:SleepTest.testTest results available in $HOME/avocado/job-results/job-2015-06-02T10.45-f9ea174 如您所见,UI输出被抑制,只显示job日志,这使其成为测试开发和调试的有用功能。 中断测试执行要中断job执行,用户可以按ctrl + c,在单次按下后将SIGTERM发送到主测试的进程并等待它完成。如果这没有帮助,用户可以再次按ctrl + c(2s宽限期后),这会非常有效地破坏测试过程并安全地完成job执行,始终提供测试结果。 要暂停测试执行,用户可以使用ctrl + z将SIGSTOP发送到从测试的PID继承的所有进程。我们尽力停止所有进程,但操作不是原子操作,可能无法停止某些新进程。再次按下ctrl + z将SIGCONT发送到测试的PID继承执行的所有进程。请注意,测试执行时间(关于测试超时)仍然在测试进程停止时运行。 Avocado功能也可以中断测试。一个例子是使用GDB调试GDB调试功能。 对于自定义交互,还可以使用其他方法,如pdb或pydevd Avocado开发提示断点。请注意,不能在测试中使用STDIN(除非使用黑暗魔法)。 书写Avocado测试我们将用Python编写Avocado测试,我们将继承avocado.Test。 这使得该测试成为所谓的仪器测试。 基本示例12345678910import timefrom avocado import Testclass SleepTest(Test): def test(self): sleep_length = self.params.get('sleep_length', default=1) self.log.debug("Sleeping for %.2f seconds", sleep_length) time.sleep(sleep_length) 这是您可以为Avocado编写的最简单的测试,同时仍然可以利用其API功能。 什么是Avocado测试从上面的示例中可以看出,Avocado测试是一种从继承自avocado.Test的类开始的测试方法。 多个测试和命名约定您可以在一个类中进行多个测试。 为此,只需给出以test开头的方法名称,比如test_foo,test_bar等等。 我们建议您遵循此命名样式,如PEP8函数名称部分中所定义。 对于类名,您可以选择任何您喜欢的名称,但我们也建议它遵循CamelCase约定,也称为CapWords,在类名称下的PEP 8文档中定义。 便利属性 可以通过self.log访问测试的即用型日志机制。 它允许您记录调试,信息,错误和警告消息。 可以通过self.params访问的参数传递系统(和提取系统)。 这与Varianter有关,您可以在Testary参数中找到更多信息。 还有更多(参见avocado.core.test.Test) 为了最大限度地减少意外冲突,我们将公共冲突定义为属性,因此如果您看到类似AttributeError: can&#39;t set attribute就不要覆盖这些属性。 测试状态Avocado支持最常见的退出状态: PASS - 测试通过,没有未经处理的例外情况 WARN - PASS的一种变量,用于跟踪最终不会影响测试结果的值得注意的事件。 一个例子可能是dmesg输出中存在的软锁定。 它与测试结果无关,除非测试失败,否则意味着该功能可能按预期工作,但有一些条件可能很好审查。 (某些结果插件不支持此功能并报告PASS) SKIP - 测试的先决条件不满足且测试的主体未被执行(也没有执行setUp()和tearDown)。 CANCEL - 在setUp(),测试方法或tearDown()期间某处取消了测试。 执行setUp()和tearDown方法。 FAIL - 测试未达到预期结果。 失败指向测试对象中的(可能的)错误,而不是测试脚本本身。 当测试(及其)执行中断时,报告ERROR而不是FAIL。 ERROR - 这可能(可能)指向测试本身的一个错误,而不是在被测试的对象中。它通常是由未捕获的异常引起的,这种失败需要彻底探索并且应该导致测试修改以避免这种失败或者 使用self.fail以及描述测试中的对象如何无法执行它的任务。 INTERRUPTED - 此结果无法由测试编写者设置,只有在超时或用户在执行此测试时按下CTRL + C时才会出现。 other - 还有其他一些内部测试状态,但你应该不会遇到它们。 正如您所看到的那样,如果正确开发了测试,则FAIL是一个整洁的状态。在编写测试时,总要考虑它的setUp应该是什么,test body是什么,并且在测试中预计会出错。为了支持您,Avocado支持以下几种方法： 测试方法设置状态的最简单方法是直接从test中使用self.fail,self.error或self.cancel。 要记录警告,只需写入self.log.warning日志即可。这不会中断测试执行,但会记住条件,如果没有失败,则会将测试报告为WARN。 将错误转化为失败Python代码上的错误通常以抛出异常的形式发出信号。当Avocado运行测试时,任何未处理的异常都将被视为测试错误,而不是失败。 尽管如此,依赖库通常会引发自定义(或内置)异常。这些异常通常会导致错误,但如果您确定这是测试对象的奇怪行为,您应该捕获异常并解释self.fail方法中的失败： 1234try: process.run("stress_my_feature")except process.CmdError as details: self.fail("The stress comamnd failed: %s" % details) 如果你的测试组件有很多执行而你无法在其他情况下得到这个异常然后预期失败,你可以使用fail_on装饰器来简化代码： 12345@avocado.fail_on(process.CmdError)def test(self): process.run("first cmd") process.run("second cmd") process.run("third cmd") 再次,让您的测试脚本保持最新并区分FAIL和ERROR的区别,将在查看测试结果时节省大量时间。 保存测试生成的(自定义)数据每个测试实例都提供一个所谓的whiteboard。它可以通过self.whiteboard访问。这个whiteboard只是一个字符串,在测试结束后会自动保存到测试结果中(在执行过程中没有同步,所以当机器或python严重崩溃时可能不存在,并且应该使用direct io直接输出到关键数据的输出)。如果您选择将二进制数据保存到whiteboard,则您有责任首先对其进行编码(base64是显而易见的选择)。 在之前演示的sleeptest测试的基础上,假设您想要保存sleep length以供其他一些脚本或数据分析工具使用： 12345def test(self): sleep_length = self.params.get('sleep_length', default=1) self.log.debug("Sleeping for %.2f seconds", sleep_length) time.sleep(sleep_length) self.whiteboard = "%.2f" % sleep_length whiteboard可以并且应该由可用的测试结果插件生成的文件公开。 results.json文件已包含每个测试的whiteboard。此外,为方便起见,我们将whiteboard内容的原始副本保存在名为whiteboard的文件中,与result.json文件位于同一级别(也许您希望直接使用基准测试结果与自定义脚本分析特定的基准测试结果)。 如果需要附加多个输出文件,还可以使用self.outputdir,它指向$RESULTS/test-results/$ TEST_ID/data位置,并保留用于任意测试结果数据。 访问测试数据文件某些测试可能依赖于测试文件本身外部的数据文件。 Avocado提供了一个测试API,可以很容易地访问这些文件：get_data() 。 对于Avocado测试(即INSTRUMENTED测试),get_data()允许从最多三个源访问测试数据文件： 文件级数据目录：以测试文件命名但以.data结尾的目录。对于测试文件/home/user/test.py,文件级数据目录是/home/user/test.py.data/。 测试级别数据目录：以测试文件和特定测试名称命名的目录。当同一文件的不同测试部分需要不同的数据文件(具有相同或不同名称)时,这些功能非常有用。考虑到之前的/home/user/test.py示例,并假设它包含两个测试,MyTest.test_foo和MyTest.test_bar,测试级数据目录将是/home/user/test.py.data/MyTest.test_foo/和home/user/test.py.data/MyTest.test_bar/ 变量级数据目录：如果在测试期间使用变量执行时,也会考虑以变量命名的目录寻找测试数据文件。对于测试文件/home/user/test.py,并测试MyTest.test_foo,带有变量debug-ffff,数据目录路径将是/home/user/test.py.data/MyTest.test_foo/debug-ffff/。 与INSTRUMENTED测试不同,SIMPLE测试仅定义file和variant 数据目录,因此是最具体的数据目录可能看起来像/bin/echo.data/debug-ffff /。 Avocado按照定义的顺序查找数据文件DATA_SOURCES,这是从最具体的一个到最通用的一个。这意味着,如果是变量正在使用,首先使用variant目录。然后测试尝试测试级别目录,最后是文件级目录。 另外,你可以使用get_data(filename,must_exist = False)来获取可能不存在的文件的预期位置,这在当你打算创建它的情况下很有用。 运行测试时,您可以使用--log-test-data-directories命令行选项记录将使用的测试数据目录对于特定的测试和执行条件(例如使用或没有变种)。在测试日志中查找“测试数据目录”。 以前存在的APIavocado.core.test.Test.datadir,用于允许基于测试文件访问数据目录仅限位置。此API已被删除。无论出于何种原因,您仍然只需要根据测试文件位置访问数据目录,可以使用get_data(filename =&#39;&#39;,source =&#39;file&#39;,must_exist = False)。 访问测试参数每个测试都有一组可以访问的参数self.params.get($ name,$ path = None,$ default = None)其中： name - 参数名称(键) path - 查找此参数的位置(未指定时使用mux-path) default - 未找到param时返回的内容 路径是有点棘手。 Avocado使用树来表示参数。 在简单的场景中,您不必担心,您将在默认情况下找到所有值的路径,但最终你可能想要查询Test parameters来理解细节。 假设您的测试收到以下参数(您将在下一节中学习如何执行它们)： 1234567$ avocado variants -m examples/tests/sleeptenmin.py.data/sleeptenmin.yaml --variants 2...Variant 1: /run/sleeptenmin/builtin, /run/variants/one_cycle /run/sleeptenmin/builtin:sleep_method =&gt; builtin /run/variants/one_cycle:sleep_cycles =&gt; 1 /run/variants/one_cycle:sleep_length =&gt; 600... 在测试中你可以通过以下方式访问这些参数： 123self.params.get("sleep_method") # returns "builtin"self.params.get("sleep_cycles", '*', 10) # returns 1self.params.get("sleep_length", "/*/variants/*") # returns 600 在可能发生冲突的复杂场景中,该路径很重要,因为当存在多个具有相同键匹配值的值时,Avocado会引发异常。如上所述,您可以通过使用特定路径或通过定义允许指定解析层次结构的自定义mux-path来避免这些路径。 更多细节可以在测试参数中找到。 运行多个测试变量在上一节中,我们描述了如何处理参数。 现在,让我们看看如何生成它们并使用不同的参数执行测试。 变量子系统允许创建多个参数变量,并使用这些参数变量执行测试。此子系统是可插入的,因此您可以使用自定义插件来生成变量。为了简单起见,让我们使用Avocado的初步实施,称为yaml_to_mux。 yaml_to_mux插件接受YAML文件。 这些将创建树状结构,将变量存储为参数并使用自定义标记将位置标记为multiplex域。 让我们使用examples/tests/sleeptenmin.py.data/sleeptenmin.yaml文件作为例子： 123456789101112131415161718sleeptenmin: !mux builtin: sleep_method: builtin shell: sleep_method: shellvariants: !mux one_cycle: sleep_cycles: 1 sleep_length: 600 six_cycles: sleep_cycles: 6 sleep_length: 100 one_hundred_cycles: sleep_cycles: 100 sleep_length: 6 six_hundred_cycles: sleep_cycles: 600 sleep_length: 1 其中产生以下结构和参数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263$ avocado variants -m examples/tests/sleeptenmin.py.data/sleeptenmin.yaml --summary 2 --variants 2Multiplex tree representation: ┗━━ run ┣━━ sleeptenmin ┃ ╠══ builtin ┃ ║ → sleep_method: builtin ┃ ╚══ shell ┃ → sleep_method: shell ┗━━ variants ╠══ one_cycle ║ → sleep_length: 600 ║ → sleep_cycles: 1 ╠══ six_cycles ║ → sleep_length: 100 ║ → sleep_cycles: 6 ╠══ one_hundred_cycles ║ → sleep_length: 6 ║ → sleep_cycles: 100 ╚══ six_hundred_cycles → sleep_length: 1 → sleep_cycles: 600Multiplex variants (8):Variant builtin-one_cycle-f659: /run/sleeptenmin/builtin, /run/variants/one_cycle /run/sleeptenmin/builtin:sleep_method =&gt; builtin /run/variants/one_cycle:sleep_cycles =&gt; 1 /run/variants/one_cycle:sleep_length =&gt; 600Variant builtin-six_cycles-723b: /run/sleeptenmin/builtin, /run/variants/six_cycles /run/sleeptenmin/builtin:sleep_method =&gt; builtin /run/variants/six_cycles:sleep_cycles =&gt; 6 /run/variants/six_cycles:sleep_length =&gt; 100Variant builtin-one_hundred_cycles-633a: /run/sleeptenmin/builtin, /run/variants/one_hundred_cycles /run/sleeptenmin/builtin:sleep_method =&gt; builtin /run/variants/one_hundred_cycles:sleep_cycles =&gt; 100 /run/variants/one_hundred_cycles:sleep_length =&gt; 6Variant builtin-six_hundred_cycles-a570: /run/sleeptenmin/builtin, /run/variants/six_hundred_cycles /run/sleeptenmin/builtin:sleep_method =&gt; builtin /run/variants/six_hundred_cycles:sleep_cycles =&gt; 600 /run/variants/six_hundred_cycles:sleep_length =&gt; 1Variant shell-one_cycle-55f5: /run/sleeptenmin/shell, /run/variants/one_cycle /run/sleeptenmin/shell:sleep_method =&gt; shell /run/variants/one_cycle:sleep_cycles =&gt; 1 /run/variants/one_cycle:sleep_length =&gt; 600Variant shell-six_cycles-9e23: /run/sleeptenmin/shell, /run/variants/six_cycles /run/sleeptenmin/shell:sleep_method =&gt; shell /run/variants/six_cycles:sleep_cycles =&gt; 6 /run/variants/six_cycles:sleep_length =&gt; 100Variant shell-one_hundred_cycles-586f: /run/sleeptenmin/shell, /run/variants/one_hundred_cycles /run/sleeptenmin/shell:sleep_method =&gt; shell /run/variants/one_hundred_cycles:sleep_cycles =&gt; 100 /run/variants/one_hundred_cycles:sleep_length =&gt; 6Variant shell-six_hundred_cycles-1e84: /run/sleeptenmin/shell, /run/variants/six_hundred_cycles /run/sleeptenmin/shell:sleep_method =&gt; shell /run/variants/six_hundred_cycles:sleep_cycles =&gt; 600 /run/variants/six_hundred_cycles:sleep_length =&gt; 1 您可以看到它创建了每个Multiplex域的所有可能变量,这些变量由YAML文件中的！mux标记定义,并在树视图中显示为单行(与具有值的单个节点的双行比较)。 总共它会产生每种测试的8种变量： 12345678910111213$ avocado run --mux-yaml examples/tests/sleeptenmin.py.data/sleeptenmin.yaml -- passtest.pyJOB ID : cc7ef22654c683b73174af6f97bc385da5a0f02fJOB LOG : /home/medic/avocado/job-results/job-2017-01-22T11.26-cc7ef22/job.log (1/8) passtest.py:PassTest.test;builtin-one_cycle-f659: PASS (0.01 s) (2/8) passtest.py:PassTest.test;builtin-six_cycles-723b: PASS (0.01 s) (3/8) passtest.py:PassTest.test;builtin-one_hundred_cycles-633a: PASS (0.01 s) (4/8) passtest.py:PassTest.test;builtin-six_hundred_cycles-a570: PASS (0.01 s) (5/8) passtest.py:PassTest.test;shell-one_cycle-55f5: PASS (0.01 s) (6/8) passtest.py:PassTest.test;shell-six_cycles-9e23: PASS (0.01 s) (7/8) passtest.py:PassTest.test;shell-one_hundred_cycles-586f: PASS (0.01 s) (8/8) passtest.py:PassTest.test;shell-six_hundred_cycles-1e84: PASS (0.01 s)RESULTS : PASS 8 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0JOB TIME : 0.16 s 高级日志记录功能Avocado在测试运行时提供高级日志记录功能。 这些可以在测试中与标准Python库API结合使用。 一个常见的例子是需要在更长或更复杂的测试中遵循特定的进展。 让我们看一个非常简单的测试示例,但在单个测试中有一个多个明确的阶段： 123456789101112131415161718192021222324252627282930313233343536import loggingimport timefrom avocado import Testprogress_log = logging.getLogger("progress")class Plant(Test): def test_plant_organic(self): rows = self.params.get("rows", default=3) # Preparing soil for row in range(rows): progress_log.info("%s: preparing soil on row %s", self.name, row) # Letting soil rest progress_log.info("%s: letting soil rest before throwing seeds", self.name) time.sleep(2) # Throwing seeds for row in range(rows): progress_log.info("%s: throwing seeds on row %s", self.name, row) # Let them grow progress_log.info("%s: waiting for Avocados to grow", self.name) time.sleep(5) # Harvest them for row in range(rows): progress_log.info("%s: harvesting organic avocados on row %s", self.name, row) 从现在开始,您可以要求Avocado显示您的日志记录流,无论是独占还是其他内置流： 1$ avocado --show app,progress run plant.py 结果应类似于： 1234567891011121314151617JOB ID : af786f86db530bff26cd6a92c36e99bedcdca95bJOB LOG : /home/cleber/avocado/job-results/job-2016-03-18T10.29-af786f8/job.log (1/1) plant.py:Plant.test_plant_organic: progress: 1-plant.py:Plant.test_plant_organic: preparing soil on row 0progress: 1-plant.py:Plant.test_plant_organic: preparing soil on row 1progress: 1-plant.py:Plant.test_plant_organic: preparing soil on row 2progress: 1-plant.py:Plant.test_plant_organic: letting soil rest before throwing seeds-progress: 1-plant.py:Plant.test_plant_organic: throwing seeds on row 0progress: 1-plant.py:Plant.test_plant_organic: throwing seeds on row 1progress: 1-plant.py:Plant.test_plant_organic: throwing seeds on row 2progress: 1-plant.py:Plant.test_plant_organic: waiting for Avocados to grow\progress: 1-plant.py:Plant.test_plant_organic: harvesting organic avocados on row 0progress: 1-plant.py:Plant.test_plant_organic: harvesting organic avocados on row 1progress: 1-plant.py:Plant.test_plant_organic: harvesting organic avocados on row 2PASS (7.01 s)RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0JOB TIME : 7.11 sJOB HTML : /home/cleber/avocado/job-results/job-2016-03-18T10.29-af786f8/html/results.html 自定义progress流与应用程序输出结合在一起,可能适合或可能不适合您的需要或喜好。 如果你为了清楚和持久性,想把progress流将发送到一个单独的文件,你可以像这样运行Avocado： 1$ avocado run plant.py --store-logging-stream progress 结果是,除了通常生成的所有其他日志文件之外,还会在job结果目录中有另一个名为progress.INFO的日志文件。 在测试运行期间,可以通过以下方式观察进度： 123456789101112$ tail -f ~/avocado/job-results/latest/progress.INFO10:36:59 INFO | 1-plant.py:Plant.test_plant_organic: preparing soil on row 010:36:59 INFO | 1-plant.py:Plant.test_plant_organic: preparing soil on row 110:36:59 INFO | 1-plant.py:Plant.test_plant_organic: preparing soil on row 210:36:59 INFO | 1-plant.py:Plant.test_plant_organic: letting soil rest before throwing seeds10:37:01 INFO | 1-plant.py:Plant.test_plant_organic: throwing seeds on row 010:37:01 INFO | 1-plant.py:Plant.test_plant_organic: throwing seeds on row 110:37:01 INFO | 1-plant.py:Plant.test_plant_organic: throwing seeds on row 210:37:01 INFO | 1-plant.py:Plant.test_plant_organic: waiting for Avocados to grow10:37:06 INFO | 1-plant.py:Plant.test_plant_organic: harvesting organic avocados on row 010:37:06 INFO | 1-plant.py:Plant.test_plant_organic: harvesting organic avocados on row 110:37:06 INFO | 1-plant.py:Plant.test_plant_organic: harvesting organic avocados on row 2 这个非常相似的progress logger,可以跨多个测试方法和多个测试模块使用。在给出的示例中,测试名称用于提供额外的上下文。 unittest.TestCase继承由于Avocado测试继承了unittest.TestCase,所以可以使用其父级的所有断言方法。代码示例使用 assertEqual, assertTrue 和 assertIsInstace: 123456789101112131415from avocado import Testclass RandomExamples(Test): def test(self): self.log.debug("Verifying some random math...") four = 2 * 2 four_ = 2 + 2 self.assertEqual(four, four_, "something is very wrong here!") self.log.debug("Verifying if a variable is set to True...") variable = True self.assertTrue(variable) self.log.debug("Verifying if this test is an instance of test.Test") self.assertIsInstance(self, test.Test) 在其它单元测试下运行测试脚本nose是另一个Python测试框架,它也与unittest兼容。因此,您可以使用nosetest应用程序运行Avocado测试： 123456$ nosetests examples/tests/sleeptest.py.----------------------------------------------------------------------Ran 1 test in 1.004sOK 相反,您也可以使用标准unittest.main()入口点运行Avocado测试。检查下面的代码,以保存为dummy.py： 123456789from avocado import Testfrom unittest import mainclass Dummy(Test): def test(self): self.assertTrue(True)if __name__ == '__main__': main() 使用: 123456$ python dummy.py.----------------------------------------------------------------------Ran 1 test in 0.000sOK Setup和cleanup方法在测试之前或之后执行setUp操作,您可以使用setUp和tearDown方法,tearDown方法总是在安装失败时执行,所以不要忘记在setUp过程中初始化变量。使用示例在下一节运行第三方测试套件中。 运行第三方测试套件在测试自动化工作负载中使用第三方开发的测试套件非常常见。通过在Avocado测试模块中封装执行代码,您可以访问框架提供的设施和API。假设你想用C写一个测试套件,它在一个tarball中,解压缩它,编译套件代码,然后执行测试。下面是一个例子： 1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/usr/bin/env pythonimport osfrom avocado import Testfrom avocado import mainfrom avocado.utils import archivefrom avocado.utils import buildfrom avocado.utils import processclass SyncTest(Test): """ Execute the synctest test suite. """ def setUp(self): """ Set default params and build the synctest suite. """ sync_tarball = self.params.get('sync_tarball', default='synctest.tar.bz2') self.sync_length = self.params.get('sync_length', default=100) self.sync_loop = self.params.get('sync_loop', default=10) # Build the synctest suite self.cwd = os.getcwd() tarball_path = self.get_data(sync_tarball) archive.extract(tarball_path, self.workdir) self.workdir = os.path.join(self.workdir, 'synctest') build.make(self.workdir) def test(self): """ Execute synctest with the appropriate params. """ os.chdir(self.workdir) cmd = ('./synctest %s %s' % (self.sync_length, self.sync_loop)) process.system(cmd) os.chdir(self.cwd)if __name__ == "__main__": main() 这里我们有一个setup方法的例子：这里我们通过avocado.Test.get_data() 得到测试套件代码(tarball)的位置。然后通过avocado.utils.archive.extract()解压缩一个API会解压缩tarball套件,avocado.utils.build.make()会建立则个套件。 在这个例子中,测试方法刚刚进入编译的套件的基本目录,并使用avocado.utils.process.system()和适当的参数执行./synctest命令。 获取资产文件测试输出检查和输出记录模式在很多情况下,你想变得简单：只需检查给定测试的输出是否匹配预期输出。为了帮助这个常见的用例,Avocado提供了--output-check-record选项 如果启用这个选项,Avocado将会将测试生成的内容保存到标准(POSIX)流,即STDOUT 和 STDERR.根据所选的选项,您可能会记录不同的文件(我们称之为“参考文件”)： stdout将生成一个名为stdout.expected的文件,该文件包含来自测试过程标准输出流(文件描述符1)的内容。 stderr将生成一个名为stderr.expected的文件,该文件包含来自测试过程标准错误流(文件描述符2)的内容。 both将生成一个名为stdout.expected和一个名为stderr.expected的文件 combined将生成一个名为output.expected的文件,其中包含测试过程标准输出和错误流(文件描述符1和2)的内容。 none将显式禁用测试生成的输出和生成内容的生成参考文件的所有记录 参考文件将被记录在第一个(最特定的)测试数据文件夹(访问测试数据文件)中。让我们以测试synctest.py为例。检查Avocado源代码,您可以找到以下参考文件： 12examples/tests/synctest.py.data/stderr.expectedexamples/tests/synctest.py.data/stdout.expected 在这两个文件中,只有stdout.expected有些内容 123$ cat examples/tests/synctest.py.data/stdout.expectedPAR : waitingPASS : sync interrupted 这意味着,在之前的测试执行期间,用--output-check-record both进行输出记录,并且仅在stdout流上生成内容： 123456$ avocado run --output-check-record both synctest.pyJOB ID : b6306504351b037fa304885c0baa923710f34f4aJOB LOG : $JOB_RESULTS_DIR/job-2017-11-26T16.42-b630650/job.log (1/1) examples/tests/synctest.py:SyncTest.test: PASS (2.03 s)RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0 | CANCEL 0JOB TIME : 2.26 s 在添加参考文件之后,检查过程是透明的,从某种意义上说,您不需要向test runner提供特殊标志。从这一点开始,在测试(一个带有参考文件记录的一个)完成运行之后,Avocado将检查输出是否与参考文件内容匹配。如果它们不匹配,则测试将以失败状态结束。 当引用文件存在时,你也可以对此测试运行程序禁用自动检查--output-check=off对此测试运行程序。 这个过程还可以也可以在简单测试,也就是返回0 (PASSed) or != 0 (FAILed)的程序或或shell脚本工作的很好。让我们考虑例子： 123$ cat output_record.sh#!/bin/bashecho &quot;Hello, world!&quot; 让我们记录下这个的输出： 123456$ scripts/avocado run output_record.sh --output-check-record allJOB ID : 25c4244dda71d0570b7f849319cd71fe1722be8bJOB LOG : $HOME/avocado/job-results/job-2014-09-25T20.49-25c4244/job.log (1/1) output_record.sh: PASS (0.01 s)RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0JOB TIME : 0.11 s 完成此操作后,您会注意到测试数据目录出现在我们的shell脚本的同一个级别,包含2个文件： 12$ ls output_record.sh.data/stderr.expected stdout.expected 让我们看看它们中的内容： 1234$ cat output_record.sh.data/stdout.expectedHello, world!$ cat output_record.sh.data/stderr.expected$ 现在,每次测试运行时,程序都会自动对比记录的预期文件,我们不需要做任何其他操作。让我们看看如果把STDUT.期望的文件内容改为Hello,avocado 会发生什么呢？： 123456$ scripts/avocado run output_record.shJOB ID : f0521e524face93019d7cb99c5765aedd933cb2eJOB LOG : $HOME/avocado/job-results/job-2014-09-25T20.52-f0521e5/job.log (1/1) output_record.sh: FAIL (0.02 s)RESULTS : PASS 0 | ERROR 0 | FAIL 1 | SKIP 0 | WARN 0 | INTERRUPT 0JOB TIME : 0.12 s 确认失败的原因： 123456789101112131415161718192021222324252627$ cat $HOME/avocado/job-results/latest/job.log 2017-10-16 14:23:02,567 test L0381 INFO | START 1-output_record.sh 2017-10-16 14:23:02,568 test L0402 DEBUG| Test metadata: 2017-10-16 14:23:02,568 test L0403 DEBUG| filename: $HOME/output_record.sh 2017-10-16 14:23:02,596 process L0389 INFO | Running &apos;$HOME/output_record.sh&apos; 2017-10-16 14:23:02,603 process L0499 INFO | Command &apos;$HOME/output_record.sh&apos; finished with 0 after 0.00131011009216s 2017-10-16 14:23:02,602 process L0479 DEBUG| [stdout] Hello, world! 2017-10-16 14:23:02,603 test L1084 INFO | Exit status: 0 2017-10-16 14:23:02,604 test L1085 INFO | Duration: 0.00131011009216 2017-10-16 14:23:02,604 test L0274 DEBUG| DATA (filename=stdout.expected) =&gt; $HOME/output_record.sh.data/stdout.expected (found at file source dir) 2017-10-16 14:23:02,605 test L0740 DEBUG| Stdout Diff: 2017-10-16 14:23:02,605 test L0742 DEBUG| --- $HOME/output_record.sh.data/stdout.expected 2017-10-16 14:23:02,605 test L0742 DEBUG| +++ $HOME/avocado/job-results/job-2017-10-16T14.23-8cba866/test-results/1-output_record.sh/stdout 2017-10-16 14:23:02,605 test L0742 DEBUG| @@ -1 +1 @@ 2017-10-16 14:23:02,605 test L0742 DEBUG| -Hello, Avocado! 2017-10-16 14:23:02,605 test L0742 DEBUG| +Hello, world! 2017-10-16 14:23:02,606 stacktrace L0041 ERROR| 2017-10-16 14:23:02,606 stacktrace L0044 ERROR| Reproduced traceback from: $HOME/git/avocado/avocado/core/test.py:872 2017-10-16 14:23:02,606 stacktrace L0047 ERROR| Traceback (most recent call last): 2017-10-16 14:23:02,606 stacktrace L0047 ERROR| File &quot;$HOME/git/avocado/avocado/core/test.py&quot;, line 743, in _check_reference_stdout 2017-10-16 14:23:02,606 stacktrace L0047 ERROR| self.fail(&apos;Actual test sdtout differs from expected one&apos;) 2017-10-16 14:23:02,606 stacktrace L0047 ERROR| File &quot;$HOME//git/avocado/avocado/core/test.py&quot;, line 983, in fail 2017-10-16 14:23:02,607 stacktrace L0047 ERROR| raise exceptions.TestFail(message) 2017-10-16 14:23:02,607 stacktrace L0047 ERROR| TestFail: Actual test sdtout differs from expected one 2017-10-16 14:23:02,607 stacktrace L0048 ERROR| 2017-10-16 14:23:02,607 test L0274 DEBUG| DATA (filename=stderr.expected) =&gt; $HOME//output_record.sh.data/stderr.expected (found at file source dir) 2017-10-16 14:23:02,608 test L0965 ERROR| FAIL 1-output_record.sh -&gt; TestFail: Actual test sdtout differs from expected one 正如预期的那样,测试失败了,因为我们改变了它的期望,因此记录了一个统一的差异。统一的差异也存在于文件stdout.diff 和 stderr.diff中,存在于测试结果目录中： 123456$ cat $HOME/avocado/job-results/latest/test-results/1-output_record.sh/stdout.diff--- $HOME/output_record.sh.data/stdout.expected+++ $HOME/avocado/job-results/job-2017-10-16T14.23-8cba866/test-results/1-output_record.sh/stdout@@ -1 +1 @@-Hello, Avocado!+Hello, world! 目前,stdout和stder都以文本方式存储。根据当前区域设置无法解码的数据将根据 https://docs.python.org/3/library/codecs.html#codecs.replace_errors 替换 在本机Avocado模块中测试日志,stdout和stderr如果需要,可以直接从原生测试范围写入预期的stdout和stderr文件。区分以下实体是很重要的： The test logs The test expected stdout 期待的标准输出 The test expected stderr 期待的标准错误 第一个是用于调试和输出信息的目的。另外,写入self.log.warning会导致测试被标记为dirty,当一切顺利时,测试以警告结束。这意味着测试通过了,但是在警告日志中描述了非相关的意外情况。 您可以使用avocado.test.log类属性中的方法将一些日志记录到测试日志中。考虑这个例子： 12345678910class output_test(Test): def test(self): self.log.info('This goes to the log and it is only informational') self.log.warn('Oh, something unexpected, non-critical happened, ' 'but we can continue.') self.log.error('Describe the error here and don\'t forget to raise ' 'an exception yourself. Writing to self.log.error ' 'won\'t do that for you.') self.log.debug('Everybody look, I had a good lunch today...') 如果您需要直接写入测试stdout和stderr流,Avocado使两个预先配置的日志记录器可用于此目的,名为avocado.test.stdout和avocado.test.stderr。可以使用Python的标准日志API来对它们进行写入。例子： 12345678910import loggingclass output_test(Test): def test(self): stdout = logging.getLogger('avocado.test.stdout') stdout.info('Informational line that will go to stdout') ... stderr = logging.getLogger('avocado.test.stderr') stderr.info('Informational line that will go to stderr') Avocado将自动保存测试在STDUT中生成的任何东西到stdout文件中,在测试结果目录中找到。这同样适用于测试在STDRR上生成的任何东西,也就是说,它将被保存到同一个位置的stderr文件中。 此外,当使用runner的输出记录特性,即--output-check-record参数值stdout, stderr或者all时,所有给这些记录器的所有内容都将保存到文stdout.expected和stderr.expected在测试数据目录中(与job/test results不同)。 设置测试超时有时您的测试套件/测试可能会被永久卡住,这可能会影响测试网格。您可以解释这种可能性,并为测试设置超时参数。测试超时可以通过测试参数来设置,如下所示。 12sleep_length: 5timeout: 3 1234567$ avocado run sleeptest.py --mux-yaml /tmp/sleeptest-example.yamlJOB ID : c78464bde9072a0b5601157989a99f0ba32a288eJOB LOG : $HOME/avocado/job-results/job-2016-11-02T11.13-c78464b/job.log (1/1) sleeptest.py:SleepTest.test: INTERRUPTED (3.04 s)RESULTS : PASS 0 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 1JOB TIME : 3.14 sJOB HTML : $HOME/avocado/job-results/job-2016-11-02T11.13-c78464b/html/results.html 123456789101112131415161718192021222324252627282930313233343536$ cat $HOME/avocado/job-results/job-2016-11-02T11.13-c78464b/job.log2016-11-02 11:13:01,133 job L0384 INFO | Multiplex tree representation:2016-11-02 11:13:01,133 job L0386 INFO | \-- run2016-11-02 11:13:01,133 job L0386 INFO | -&gt; sleep_length: 52016-11-02 11:13:01,133 job L0386 INFO | -&gt; timeout: 32016-11-02 11:13:01,133 job L0387 INFO |2016-11-02 11:13:01,134 job L0391 INFO | Temporary dir: /var/tmp/avocado_PqDEyC2016-11-02 11:13:01,134 job L0392 INFO |2016-11-02 11:13:01,134 job L0399 INFO | Variant 1: /run2016-11-02 11:13:01,134 job L0402 INFO |2016-11-02 11:13:01,134 job L0311 INFO | Job ID: c78464bde9072a0b5601157989a99f0ba32a288e2016-11-02 11:13:01,134 job L0314 INFO |2016-11-02 11:13:01,345 sysinfo L0107 DEBUG| Not logging /proc/pci (file does not exist)2016-11-02 11:13:01,351 sysinfo L0105 DEBUG| Not logging /proc/slabinfo (lack of permissions)2016-11-02 11:13:01,355 sysinfo L0107 DEBUG| Not logging /sys/kernel/debug/sched_features (file does not exist)2016-11-02 11:13:01,388 sysinfo L0388 INFO | Commands configured by file: /etc/avocado/sysinfo/commands2016-11-02 11:13:01,388 sysinfo L0399 INFO | Files configured by file: /etc/avocado/sysinfo/files2016-11-02 11:13:01,388 sysinfo L0419 INFO | Profilers configured by file: /etc/avocado/sysinfo/profilers2016-11-02 11:13:01,388 sysinfo L0427 INFO | Profiler disabled2016-11-02 11:13:01,394 multiplexer L0166 DEBUG| PARAMS (key=timeout, path=*, default=None) =&gt; 32016-11-02 11:13:01,395 test L0216 INFO | START 1-sleeptest.py:SleepTest.test2016-11-02 11:13:01,396 multiplexer L0166 DEBUG| PARAMS (key=sleep_length, path=*, default=1) =&gt; 52016-11-02 11:13:01,396 sleeptest L0022 DEBUG| Sleeping for 5.00 seconds2016-11-02 11:13:04,411 stacktrace L0038 ERROR|2016-11-02 11:13:04,412 stacktrace L0041 ERROR| Reproduced traceback from: $HOME/src/avocado/avocado/core/test.py:4542016-11-02 11:13:04,412 stacktrace L0044 ERROR| Traceback (most recent call last):2016-11-02 11:13:04,413 stacktrace L0044 ERROR| File &quot;/usr/share/doc/avocado/tests/sleeptest.py&quot;, line 23, in test2016-11-02 11:13:04,413 stacktrace L0044 ERROR| time.sleep(sleep_length)2016-11-02 11:13:04,413 stacktrace L0044 ERROR| File &quot;$HOME/src/avocado/avocado/core/runner.py&quot;, line 293, in sigterm_handler2016-11-02 11:13:04,413 stacktrace L0044 ERROR| raise SystemExit(&quot;Test interrupted by SIGTERM&quot;)2016-11-02 11:13:04,414 stacktrace L0044 ERROR| SystemExit: Test interrupted by SIGTERM2016-11-02 11:13:04,414 stacktrace L0045 ERROR|2016-11-02 11:13:04,414 test L0459 DEBUG| Local variables:2016-11-02 11:13:04,440 test L0462 DEBUG| -&gt; self &lt;class &apos;sleeptest.SleepTest&apos;&gt;: 1-sleeptest.py:SleepTest.test2016-11-02 11:13:04,440 test L0462 DEBUG| -&gt; sleep_length &lt;type &apos;int&apos;&gt;: 52016-11-02 11:13:04,440 test L0592 ERROR| ERROR 1-sleeptest.py:SleepTest.test -&gt; TestError: SystemExit(&apos;Test interrupted by SIGTERM&apos;,): Test interrupted by SIGTERM YAML文件定义了一个测试参数超时,它在运行程序结束之前,通过发送一个类:signal.SIGTERM到测试,raise错误avocado.core.exceptions.TestTimeoutError,从而提高了测试速度。 跳过测试要在Avocado中跳过测试,必须使用Avocado跳过装饰器中的一种： `@avocado.skip(reason)`: 跳过测试. `@avocado.skipIf(condition, reason): 跳过测试如果条件为True`. `@avocado.skipUnless(condition, reason): 跳过测试如果条件为False` 这些装饰器可以同时使用setup方法和Test*()方法中使用。测试如下： 123456789101112131415import avocadoclass MyTest(avocado.Test): @avocado.skipIf(1 == 1, 'Skipping on True condition.') def test1(self): pass @avocado.skip("Don't want this test now.") def test2(self): pass @avocado.skipUnless(1 == 1, 'Skipping on False condition.') def test3(self): pass 将产生以下结果： 123456789$ avocado run test_skip_decorators.pyJOB ID : 59c815f6a42269daeaf1e5b93e52269fb8a78119JOB LOG : $HOME/avocado/job-results/job-2017-02-03T17.41-59c815f/job.log (1/3) test_skip_decorators.py:MyTest.test1: SKIP (2/3) test_skip_decorators.py:MyTest.test2: SKIP (3/3) test_skip_decorators.py:MyTest.test3: PASS (0.02 s)RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 2 | WARN 0 | INTERRUPT 0JOB TIME : 0.13 sJOB HTML : $HOME/avocado/job-results/job-2017-02-03T17.41-59c815f/html/results.html 注意,由于提供的条件不是false,所以没有跳过Test3。 使用跳过装饰器,实际上没有执行任何操作。我们将跳过setup方法、测试方法和teardown方法。 任何skip装饰器都不能在teardown方法上使用,否则会出现错误,状态吗为ERROR 取消测试您可以在测试的任何阶段(setup()、测试方法或teardown)中调用self.cancel()取消测试。测试将以取消状态结束,并且不会使job以非0状态退出。例子： 12345678910111213141516171819202122232425262728293031from avocado import Testfrom avocado import mainfrom avocado.utils.process import runfrom avocado.utils.software_manager import SoftwareManagerclass CancelTest(Test): """ Example tests that cancel the current test from inside the test. """ def setUp(self): sm = SoftwareManager() self.pkgs = sm.list_all(software_components=False) def test_iperf(self): if 'iperf-2.0.8-6.fc25.x86_64' not in self.pkgs: self.cancel('iperf is not installed or wrong version') self.assertIn('pthreads', run('iperf -v', ignore_status=True).stderr) def test_gcc(self): if 'gcc-6.3.1-1.fc25.x86_64' not in self.pkgs: self.cancel('gcc is not installed or wrong version') self.assertIn('enable-gnu-indirect-function', run('gcc -v', ignore_status=True).stderr)if __name__ == "__main__": main() 在缺少IPRF包但系统安装在正确版本中的系统中,结果将是： 1234567JOB ID : 39c1f120830b9769b42f5f70b6b7bad0b1b1f09fJOB LOG : $HOME/avocado/job-results/job-2017-03-10T16.22-39c1f12/job.log (1/2) /home/apahim/avocado/tests/test_cancel.py:CancelTest.test_iperf: CANCEL (1.15 s) (2/2) /home/apahim/avocado/tests/test_cancel.py:CancelTest.test_gcc: PASS (1.13 s)RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0 | CANCEL 1JOB TIME : 2.38 sJOB HTML : $HOME/avocado/job-results/job-2017-03-10T16.22-39c1f12/html/results.html 注意,使用self.cancel()将从该点取消其余的测试,但是teardown()仍将被执行。 根据您所提到的结果格式,取消状态被映射到相应格式的有效状态。见下表： Format Corresponding Status json cancel xunit skipped tap ok html CANCEL (warning) Docstring指令一些Avododo特性,通常只适用于仪器化测试,依赖于在测试类的DoScord中设置指令。DOXString指令由标记(:avocado:)组成,接着是自定义内容本身,如:avocado: directive。 这与DoScript指令类似,例如:param my_param: description对于大多数Python开发人员来说,这应该不是一个意外。 Avocado使用这些DOSCRON指令(而不是真正的Python代码)的原因是,在寻找测试时进行的检查不涉及代码的任何执行。 有关DoScript格式的有效性的详细解释,请参阅我们关于DOSCSHIPE指令规则的章节。 现在让我们继续使用一些DoScript指令示例。 显式启用或禁用测试如果您的测试是直接从avocado.Test继承的类中的一个方法,那么avocado会像预期的那样找到它。 现在,可能需要更复杂的测试,使用更先进的Python特性,例如继承。对于那些不直接从avocado.Test中继承的测试,Avocado可能需要你的帮助,因为Avocado只使用静态分析来检查文件。 例如,假设您定义了一个新的测试类,该类继承了avocado基础测试类,即avocado.Test,并将其放入mylibrary.py： 12345678910from avocado import Testclass MyOwnDerivedTest(Test): def __init__(self, methodName='test', name=None, params=None, base_logdir=None, job=None, runner_queue=None): super(MyOwnDerivedTest, self).__init__(methodName, name, params, base_logdir, job, runner_queue) self.log('Derived class example') 然后在mytest.py中使用该派生类实现实际测试： 12345678910import mylibraryclass MyTest(mylibrary.MyOwnDerivedTest): def test1(self): self.log('Testing something important') def test2(self): self.log('Testing something even more important') 如果您试图列出该文件中的测试,这将是您将得到的： 123456789101112131415scripts/avocado list mytest.py -VType Test Tag(s)NOT_A_TEST mytest.pyTEST TYPES SUMMARY==================ACCESS_DENIED: 0BROKEN_SYMLINK: 0EXTERNAL: 0FILTERED: 0INSTRUMENTED: 0MISSING: 0NOT_A_TEST: 1SIMPLE: 0VT: 0 你需要通过添加一个docstring指令来给Avocado一点帮助。docstring指令是：:avocado: enable。它告诉Avocado安全测试检测代码,将其视为Avocado试验,而不管检测代码对它的看法如何。让我们看看效果如何。添加docstring,如下所示： 1234567891011import mylibraryclass MyTest(mylibrary.MyOwnDerivedTest): """ :avocado: enable """ def test1(self): self.log('Testing something important') def test2(self): self.log('Testing something even more important') 再次尝试列出该文件中的测试： 12345678910111213141516scripts/avocado list mytest.py -VType Test Tag(s)INSTRUMENTED mytest.py:MyTest.test1INSTRUMENTED mytest.py:MyTest.test2TEST TYPES SUMMARY==================ACCESS_DENIED: 0BROKEN_SYMLINK: 0EXTERNAL: 0FILTERED: 0INSTRUMENTED: 2MISSING: 0NOT_A_TEST: 0SIMPLE: 0VT: 0 您还可以使用：avocado:disable的docstring指令,相反的工作方式：将被一个Avocado测试强制视为非avocado测试。 :avocado: disable指令首先被Avocado评估,这意味着,如果:avocado: disable 和 :avocado: enable同时出现的话,测试将不会被列出。 递归发现测试除了:avocado: disable 和 :avocado: enable指令,Avocado还支持:avocado: recursive 分类测试Python unittest兼容性限制和警告测试的环境变量Avocado将一些信息(包括测试参数)作为环境变量导出到正在运行的测试中。 虽然这些变量可用于所有测试,但它们通常对SIMPLE测试更有意义。 原因是SIMPLE测试无法直接使用Avocado API。 INSTRUMENTED测试通常会有更强大的方法来访问相同的信息。 AVOCADO_VERSION: Avocado测试运行器的版本 VOCADO_TEST_BASEDIR:Avocado测试的基本目录 AVOCADO_TEST_WORKDIR:测试的工作目录 AVOCADO_TESTS_COMMON_TMPDIR:teststmpdir插件创建的临时目录。该目录在同一个Job中的整个测试中是持久的 AVOCADO_TEST_LOGDIR：日志目录 AVOCADO_TEST_LOGFILE: 测试的日志文件 AVOCADO_TEST_OUTPUTDIR:测试的输出目录 AVOCADO_TEST_SYSINFODIR：系统信息目录 ***: 来自-mux-yaml的所有变量 AVOCADO_TEST_SRCDIR存在于早期版本中,但在版本60.0上已弃用,在版本62.0上已删除。请改用AVOCADO_TEST_WORKDIR。 AVOCADO_TEST_DATADIR存在于早期版本中,但在版本60.0上已弃用,在版本62.0上已删除。现在,测试数据文件(和目录)已动态评估,不可用作环境变量 SIMPLE测试BASH扩展用shell编写的SIMPLE测试可以使用一些Avocado实用程序。 在shell代码中,检查库是否可用,例如： 1AVOCADO_SHELL_EXTENSIONS_DIR=$(avocado exec-path 2&gt;/dev/null) 如果可用,将包含这些实用程序的目录注入shell使用的PATH,使这些实用程序易于访问： 123if [ $? == 0 ]; then PATH=$AVOCADO_SHELL_EXTENSIONS_DIR:$PATHfi 有关实用程序的完整列表,请查看目录返回通过avocado exec-path(如果有的话)。 另外,示例测试examples/tests/ simplewarning.sh可以提供进一步的灵感。 这些扩展可以作为单独的包提供。 对于RPM包,请查找bash子包。 简单的测试状态通过SIMPLE测试,Avocado会检查测试的退出代码,以确定测试是否已通过或已失败。 如果您的测试以退出代码0退出,但您仍希望在某些条件下设置不同的测试状态,则Avocado可以在测试输出中搜索给定的正则表达式,并在此基础上将状态设置为WARN或SKIP。 要使用该功能,您必须在配置文件中设置正确的密钥。 例如,当测试输出类似：’11：08：24 Test Skipped’：所示的行时,将测试状态设置为SKIP 12[simpletests.output]skip_regex = ^\d\d:\d\d:\d\d Test Skipped$ 该配置将使Avocado在stdout和stderr上搜索Python正则表达式。 如果您只想限制其中一个搜索,那么该配置还有另一个键： 123[simpletests.output]skip_regex = ^\d\d:\d\d:\d\d Test Skipped$skip_location = stderr WARN状态存在相同的设置。 例如,如果要在测试输出以字符串WARNING开头的行时将测试状态设置为WARN,则配置文件将如下所示： 12345[simpletests.output]skip_regex = ^\d\d:\d\d:\d\d Test Skipped$skip_location = stderrwarn_regex = ^WARNING:warn_location = all 本文小节我们建议您查看一下中的示例测试examples/tests目录,这个目录包含一些样本以从中获取灵感。。 除了包含示例之外,该目录也被使用Avocado自测套件可对Avocado进行功能测试。也可以查看https://github.com/avocado-framework-tests,它允许人们分享他们的基本系统测试,以从中获取灵感。 结果格式化测试脚本必须提供各种方法来清晰地将结果传达给相关方,无论是人还是机器。 有几个可选的结果插件,你可以在Result Plugins中找到它们。 人类可读结果Avocado有两种不同的结果格式,供人类使用： 默认UI,它在命令行上显示基于文本UI的实况测试执行结果。 HTML报告,它是在测试任务完成后生成的。 Avocado的命令行界面 定期运行Avocado将以生动的方式呈现测试结果,也就是说,工作和测试结果不断更新： 123456789$ avocado run sleeptest.py failtest.py synctest.pyJOB ID : 5ffe479262ea9025f2e4e84c4e92055b5c79bdc9JOB LOG : $HOME/avocado/job-results/job-2014-08-12T15.57-5ffe4792/job.log (1/3) sleeptest.py:SleepTest.test: PASS (1.01 s) (2/3) failtest.py:FailTest.test: FAIL (0.00 s) (3/3) synctest.py:SyncTest.test: PASS (1.98 s)RESULTS : PASS 1 | ERROR 1 | FAIL 1 | SKIP 0 | WARN 0 | INTERRUPT 0JOB TIME : 3.27 sJOB HTML : $HOME/avocado/job-results/job-2014-08-12T15.57-5ffe4792/html/results.html 最重要的是要记住,程序不需要分析人的输出来确定测试工作运行的情况。 机器可读结果另一种类型的结果是那些被其他应用程序解析的结果。测试社区中存在若干标准,Avocado在理论上可以支持几乎所有的结果标准。 非常好,Avocado支持一些机器可读的结果。它们总是生成并存储在结果目录中。$Type文件,但是您也可以要求不同的位置。 xunitAvocado默认的机器可读输出是xunit xunit是以结构化形式包含测试结果的XML格式,并由其他测试自动化项目(Jenkins)使用。如果你想让Avocado在runner的标准输出中生成xunit作为输出,可以简单地使用： 12345678910111213141516171819202122232425$ avocado run sleeptest.py failtest.py synctest.py --xunit -&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;testsuite name=&quot;avocado&quot; tests=&quot;3&quot; errors=&quot;0&quot; failures=&quot;1&quot; skipped=&quot;0&quot; time=&quot;3.5769162178&quot; timestamp=&quot;2016-05-04 14:46:52.803365&quot;&gt; &lt;testcase classname=&quot;SleepTest&quot; name=&quot;1-sleeptest.py:SleepTest.test&quot; time=&quot;1.00204920769&quot;/&gt; &lt;testcase classname=&quot;FailTest&quot; name=&quot;2-failtest.py:FailTest.test&quot; time=&quot;0.00120401382446&quot;&gt; &lt;failure type=&quot;TestFail&quot; message=&quot;This test is supposed to fail&quot;&gt;&lt;![CDATA[Traceback (most recent call last): File &quot;/home/medic/Work/Projekty/avocado/avocado/avocado/core/test.py&quot;, line 490, in _run_avocado raise test_exceptionTestFail: This test is supposed to fail]]&gt;&lt;/failure&gt; &lt;system-out&gt;&lt;![CDATA[14:46:53 ERROR|14:46:53 ERROR| Reproduced traceback from: /home/medic/Work/Projekty/avocado/avocado/avocado/core/test.py:43514:46:53 ERROR| Traceback (most recent call last):14:46:53 ERROR| File &quot;/home/medic/Work/Projekty/avocado/avocado/examples/tests/failtest.py&quot;, line 17, in test14:46:53 ERROR| self.fail(&apos;This test is supposed to fail&apos;)14:46:53 ERROR| File &quot;/home/medic/Work/Projekty/avocado/avocado/avocado/core/test.py&quot;, line 585, in fail14:46:53 ERROR| raise exceptions.TestFail(message)14:46:53 ERROR| TestFail: This test is supposed to fail14:46:53 ERROR|14:46:53 ERROR| FAIL 2-failtest.py:FailTest.test -&gt; TestFail: This test is supposed to fail14:46:53 INFO |]]&gt;&lt;/system-out&gt; &lt;/testcase&gt; &lt;testcase classname=&quot;SyncTest&quot; name=&quot;3-synctest.py:SyncTest.test&quot; time=&quot;2.57366299629&quot;/&gt;&lt;/testsuite&gt; 最后的-是xunit的选项,表示xunit应该转到标准输出 如果你的测试产生了很长的输出,你可以用 –xunit-max-test-log-chars 来限制嵌入字符的数量。如果日志文件中的输出较长,则只从最大值开始附加到最大测试日志字符,另一半从内容的结尾开始。 JSONJSON是一种广泛使用的数据交换格式,JSON avocado插件使用类似于xunit使用方式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647$ avocado run sleeptest.py failtest.py synctest.py --json -&#123; &quot;cancel&quot;: 0, &quot;debuglog&quot;: &quot;/home/cleber/avocado/job-results/job-2016-08-09T13.53-10715c4/job.log&quot;, &quot;errors&quot;: 0, &quot;failures&quot;: 1, &quot;job_id&quot;: &quot;10715c4645d2d2b57889d7a4317fcd01451b600e&quot;, &quot;pass&quot;: 2, &quot;skip&quot;: 0, &quot;tests&quot;: [ &#123; &quot;end&quot;: 1470761623.176954, &quot;fail_reason&quot;: &quot;None&quot;, &quot;logdir&quot;: &quot;/home/cleber/avocado/job-results/job-2016-08-09T13.53-10715c4/test-results/1-sleeptest.py:SleepTest.test&quot;, &quot;logfile&quot;: &quot;/home/cleber/avocado/job-results/job-2016-08-09T13.53-10715c4/test-results/1-sleeptest.py:SleepTest.test/debug.log&quot;, &quot;start&quot;: 1470761622.174918, &quot;status&quot;: &quot;PASS&quot;, &quot;id&quot;: &quot;1-sleeptest.py:SleepTest.test&quot;, &quot;time&quot;: 1.0020360946655273, &quot;whiteboard&quot;: &quot;&quot; &#125;, &#123; &quot;end&quot;: 1470761623.193472, &quot;fail_reason&quot;: &quot;This test is supposed to fail&quot;, &quot;logdir&quot;: &quot;/home/cleber/avocado/job-results/job-2016-08-09T13.53-10715c4/test-results/2-failtest.py:FailTest.test&quot;, &quot;logfile&quot;: &quot;/home/cleber/avocado/job-results/job-2016-08-09T13.53-10715c4/test-results/2-failtest.py:FailTest.test/debug.log&quot;, &quot;start&quot;: 1470761623.192334, &quot;status&quot;: &quot;FAIL&quot;, &quot;id&quot;: &quot;2-failtest.py:FailTest.test&quot;, &quot;time&quot;: 0.0011379718780517578, &quot;whiteboard&quot;: &quot;&quot; &#125;, &#123; &quot;end&quot;: 1470761625.656061, &quot;fail_reason&quot;: &quot;None&quot;, &quot;logdir&quot;: &quot;/home/cleber/avocado/job-results/job-2016-08-09T13.53-10715c4/test-results/3-synctest.py:SyncTest.test&quot;, &quot;logfile&quot;: &quot;/home/cleber/avocado/job-results/job-2016-08-09T13.53-10715c4/test-results/3-synctest.py:SyncTest.test/debug.log&quot;, &quot;start&quot;: 1470761623.208165, &quot;status&quot;: &quot;PASS&quot;, &quot;id&quot;: &quot;3-synctest.py:SyncTest.test&quot;, &quot;time&quot;: 2.4478960037231445, &quot;whiteboard&quot;: &quot;&quot; &#125; ], &quot;time&quot;: 3.4510700702667236, &quot;total&quot;: 3&#125; 请记住,没有AvocadoJSON结果格式的文档标准。这意味着它可能会逐渐增加,以适应更新的Avocado特性。适当的工作解析JSON构成的结果将不会破坏与应用程序的向后兼容性。 TAP提供当前在V12版本的基本TAP(测试任何协议)结果。不像大多数现有Avocado机器可读的输出,这一个是流线型(每个测试结果)： 12345678$ avocado run sleeptest.py --tap -1..1# debug.log of sleeptest.py:SleepTest.test:# 12:04:38 DEBUG| PARAMS (key=sleep_length, path=*, default=1) =&gt; 1# 12:04:38 DEBUG| Sleeping for 1.00 seconds# 12:04:39 INFO | PASS 1-sleeptest.py:SleepTest.test# 12:04:39 INFO |ok 1 sleeptest.py:SleepTest.test 译者 debug.log不会显示在控制台,需要到debug.log中去查看cat ~/avocado/job-results/latest/test-results/1-sleeptest.py_SleepTest.test/debug.log Silent result此结果禁用所有stdout日志记录(同时将错误消息打印到stderr)。然后,可以使用返回代码来了解结果： 123$ avocado --silent run failtest.py$ echo $?1 在实践中,这通常会被脚本用来运行avocado,并检查其结果： 1234567#!/bin/bash...$ avocado --silent run /path/to/my/test.pyif [ $? == 0 ]; then echo &quot;great success!&quot;elif ... 关于退出代码中的退出代码的更多细节部分。 一次获得多个结果只要只有一个使用标准输出,就可以同时拥有多个结果格式。例如,使用xunit结果输出到stdout并将json结果输出到文件： 12345678910111213$ avocado run sleeptest.py synctest.py --xunit - --json /tmp/result.json&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;testsuite name=&quot;avocado&quot; tests=&quot;2&quot; errors=&quot;0&quot; failures=&quot;0&quot; skipped=&quot;0&quot; time=&quot;3.64848303795&quot; timestamp=&quot;2016-05-04 17:26:05.645665&quot;&gt; &lt;testcase classname=&quot;SleepTest&quot; name=&quot;1-sleeptest.py:SleepTest.test&quot; time=&quot;1.00270605087&quot;/&gt; &lt;testcase classname=&quot;SyncTest&quot; name=&quot;2-synctest.py:SyncTest.test&quot; time=&quot;2.64577698708&quot;/&gt;&lt;/testsuite&gt;$ cat /tmp/result.json&#123; &quot;debuglog&quot;: &quot;/home/cleber/avocado/job-results/job-2016-08-09T13.55-1a94ad6/job.log&quot;, &quot;errors&quot;: 0, ...&#125; 但是如果没有传递给程序的 -json 项,您将无法做到这一点： 123$ avocado run sleeptest.py synctest.py --xunit - --json -Options --json --xunit are trying to use stdout simultaneouslyPlease set at least one of them to a file to avoid conflicts 这基本上是你需要遵循唯一,理智的的规则。 退出码Avocado退出代码试图代表在执行过程中可能发生的不同事情。这意味着退出代码作为一个单独的退出代码可以是和代码组合在一起。最后的退出代码可以取消绑定,这样用户就可以对工作发生的事情有一个很好的了解。 示例如下: AVOCADO_ALL_OK (0) AVOCADO_TESTS_FAIL (1) AVOCADO_JOB_FAIL (2) AVOCADO_FAIL (4) AVOCADO_JOB_INTERRUPTED (8) 举个例子,如果一个job以退出代码9结束,它意味着我们至少有一个测试失败了,而且我们在某个时候有一个job中断,这可能是由于job超时或CTRL+C造成的。 实现其他结果格式如果你想实现一种新的机器或人类可读的输出格式,你可以参考avocado.plugins.xunit并使用它作为起点。 f你的结果是一次生成的,基于完整的工作结果,你应该创建一个继承avocado.core.plugin_interfaces.Result接口的新类。并实现avocado.core.plugin_interfaces.Result.render()方法。 但是,如果您的结果实现是在测试之前/期间/测试之后输出信息,就要看一看avocado.core.plugin_interfaces.ResultEvents. 它将要求您实现对job和测试执行中的每个定义的事件执行操作(写入文件/流)的方法。 你可以看看插件系统,了解更多关于如何编写插件的信息,这些插件将激活和执行新的结果格式。 配置关于如何使用户喜欢使用他们的系统,Avocado有一些基于训练,合理的(我们希望)猜测的默认行为。 当然,不同的人会有不同的需求和/或不喜欢我们的默认值,这就是为什么可以用配置系统来帮助这些案例的原因。 Avocado配置文件格式是基于(非正式)INI文件的“规范”,它由Python的配置分析器实现。分段组成,格式简单明了,包含多个键和值。以一个基本的Avocado配置文件为例： 12345[datadir.paths]base_dir = /var/lib/avocadotest_dir = /usr/share/doc/avocado/testsdata_dir = /var/lib/avocado/datalogs_dir = ~/avocado/job-results datadir.paths部分包含多个键,它们都与测试工具使用的目录相关。base_dir是其他重要Avocado目录的基础目录,如日志、数据和测试目录。您还可以通过变量test_dir、data_dir和logs_dir选择设置其他重要目录。您可以通过简单编辑可用的配置文件来实现这一点。 配置文件解析顺序Avocado开始解析它所称的系统范围配置文件/etc/avocado/avocado.conf,该文件被传输到全系统目录中的所有Avocado用户.然后,它将验证是否存在一个本地用户配置文件,该文件通常位于~/.config/avocado/avocado.conf.解析的顺序很重要,所以首先系统范围的文件被解析,然后用户配置文件最后被解析,这样用户可以随意重写值。还有另一个目录将被额外配置文件扫描,/etc/avocado/conf.d.该目录可能包含插件配置文件,以及系统管理员/Avocado开发人员可能判断需要放置的额外附加配置文件。 请注意,对于基本目录,如果您选择了不能被Avocado正确使用的目录(一些目录需要读访问、其他读取和写入访问),Avocado将回落到一些默认值。因此,如果您的普通权限用户想将日志写入到/root/avocado/logs,Avocado将不能使用该目录,因为它无法将文件写入该位置。默认情况下,将选择一个新的位置~/avocado/job-results。 本节中描述的文件顺序仅在Avocado安装在系统中才有效。对于使用Git仓库(通常是Avocado开发人员)的Avocado来说,并没有安装在系统中,请记住Avocado将读取Git仓库中存在的配置文件,并且将忽略系统范围配置文件。执行avocado config会让你之道哪些config文件正在实际使用。 插件配置文件插件也可以通过配置文件来配置。为了不干扰主Avocado配置文件,如果希望的话,这些插件可以安装附加配置文件/etc/avocado/conf.d/[pluginname].conf这将在系统范围配置文件之后进行解析。用户也可以在本地配置文件级别上重写这些值。思考假想插件salad的配置： 123[salad.core]base = ceasardressing = ceasar 如果需要,可以通过在本地配置文件中简单地添加[salad.core]新部分来更改配置文件中的dressing,并在其中设置不同的值。 解析顺序重述因此,文件解析顺序为： /etc/avocado/avocado.conf /etc/avocado/conf.d/*.conf ~/.config/avocado/avocado.conf 按此顺序,意味着您在本地配置文件上设置的内容可以覆盖系统范围文件中定义的内容。 请注意,如果Avocado从Git 仓库中运行,这些文件将被忽略,并被配置树文件取代。这通常只会影响开发Avocado的人,如果你有疑问,avocado config会告诉你确切的文件在任何特定的情况下都被使用。 测试中使用的值的优先顺序由于可以使用配置系统来改变测试中使用的行为和值(例如,对测试程序的思考路径),所以我们建立了以下变量优先级顺序(从最小优先级到大多数)： 缺省值(来自库或测试代码) 全局配置文件 本地(用户)配置文件 命令行开关 试验参数 因此,最不重要的值来自库或测试代码默认值,一直到测试参数系统。 配置插件一个配置插件被提供给希望快速查看在Avocado配置的所有部分中定义的用户,在所有文件以正确的解析顺序解析之后。例子： 12345678910$ avocado configConfig files read (in order): /etc/avocado/avocado.conf $HOME/.config/avocado/avocado.conf Section.Key Value runner.base_dir /var/lib/avocado runner.test_dir /usr/share/doc/avocado/tests runner.data_dir /var/lib/avocado/data runner.logs_dir ~/avocado/job-results 该命令还显示了解析配置文件的顺序,让您更好地了解正在发生的事情。关键术语在git config --list output得到启发。 Avocado数据目录当运行测试时,我们往往希望： 定位测试 将日志写入给定位置 抓取对测试有用的文件,例如ISO文件或VM磁盘映像 Avocado拥有一个专门用于寻找这些路径的模块,以避免人们在以前的测试框架中不得不做的繁琐的路径操作。 如果要列出所有有关测试的目录,可以使用Avocadoavocado config --datadir命令列出这些目录。执行它会给你一个类似于下面看到的输出： 1234567891011121314$ avocado config --datadirConfig files read (in order): /etc/avocado/avocado.conf $HOME/.config/avocado/avocado.confAvocado replaces config dirs that can&apos;t be accessedwith sensible defaults. Please edit your local configfile to customize valuesAvocado Data Directories: base $HOME/avocado tests $HOME/Code/avocado/examples/tests data $HOME/avocado/data logs $HOME/avocado/job-results 注意,虽然Avocado将尽最大努力使用配置文件中提供的配置值,如果它不能将值写入所提供的位置,它将回落到(我们希望)合理的默认值,并且我们在命令的输出中通知用户。相关的API文档和每个数据目录的含义都是在avocado.core.data_dir中,所以强烈建议您查看一下。 您可以通过将它们设置在Avocado配置文件中来设置首选数据文件夹。这里的重要数据文件夹的唯一例外是AvocadoTMP DIR,用来放置测试所使用的临时文件。该目录将在正常情况下/var/tmp/avocado_XXXXX,(XXXXX实际上是一个随机字符串)安全地创建在/var/tmp/上,除非用户有$TMPDIR环境变量集,因为这是UNIX程序中惯用的。 文档的下一部分说明了如何查看和设置修改Avocado实用工具和插件的行为的配置值。 测试发现在本节中,您可以了解测试是如何被发现的以及如何影响这个过程。 测试loader的顺序Avocado支持不同类型的测试.从简单的测试开始,它是简单的可执行文件,然后是unitest-like测试,称为INSTRUMENTED,如avocado-vt的,它使用复杂的矩阵测试配置文件不直接映射到现有的文件。给定装载器的数量,从命令行上的测试名称到执行的测试的映射可能并不总是唯一的。另外,有些人可能总是(或给定的运行)希望只执行单个类型的测试。 为了调整这种行为,你可以在avocado设置(/etc/avocado/)中调整plugins.loaders,或暂时使用--loaders(Avocado运行选项)选项。 此选项允许您指定可用的测试加载器的顺序和一些参数。您可以指定loader_name(file)、 loader_name + TEST_TYPE (file.SIMPLE),并且对于某些loaders,甚至附加的参数也会通过: (external:/bin/echo -e,您也可以提供@DEFAULT,它将所有剩余的未使用的loaders注入到该位置。 --loaders 如何影响生成的测试(手动收集,因为其中一些会导致错误)： 123456789101112131415$ avocado run passtest.py boot this_does_not_exist /bin/echo &gt; INSTRUMENTED passtest.py:PassTest.test &gt; VT io-github-autotest-qemu.boot &gt; MISSING this_does_not_exist &gt; SIMPLE /bin/echo$ avocado run passtest.py boot this_does_not_exist /bin/echo --loaders @DEFAULT &quot;external:/bin/echo -e&quot; &gt; INSTRUMENTED passtest.py:PassTest.test &gt; VT io-github-autotest-qemu.boot &gt; EXTERNAL this_does_not_exist &gt; SIMPLE /bin/echo$ avocado run passtest.py boot this_does_not_exist /bin/echo --loaders file.SIMPLE file.INSTRUMENTED @DEFAULT external.EXTERNAL:/bin/echo &gt; INSTRUMENTED passtest.py:PassTest.test &gt; VT io-github-autotest-qemu.boot &gt; EXTERNAL this_does_not_exist &gt; SIMPLE /bin/echo 使用参数运行简单测试这个过去通过运行avocado run &quot;test arg1 arg2&quot;提供了现有的支持,但是它的确是很混乱并且已经被删除。但它仍然可以通过使用shell来实现,甚至可以将正常测试和参数化的测试结合起来： 1avocado run --loaders file external:/bin/sh -- existing_file.py &quot;&apos;/bin/echo something&apos;&quot; nonexisting-file 这将运行3个测试,第一个测试是由existing_file.py定义的正常测试(很可能是一个仪器化测试)。然后我们将通过/bin/sh -c &#39;/bin/echo something&#39;执行/bin/echo someting。最后一个将是nonexisting-file,它将执行/bin/sh -c nonexisting-file,这很可能会失败的。 请注意,您负责引用测试ID(请参阅”‘/bin/echo something’”示例)。 通过标签过滤测试Avocado允许测试提供标签,可以用来创建测试类别。使用标签集,用户可以选择由测试解析器(也称为测试加载器)找到的测试的子集。有关测试标记的更多信息,请访问 WritingTests.html#categorizing-tests。 测试引用 Test References测试引用是一个字符串,可以通过Avocado测试解析器解析为(解释为)一个或多个测试。 每个解析器(a.k.a加载器)可以不同地处理测试引用。例如,外部加载器将使用测试引用作为外部命令的参数,而文件加载器将期望文件路径。 如果不指定要使用的加载器,则所有可用的加载器将用于解析所提供的测试引用。一个接一个地,测试引用将由第一个加载程序解决,该第一个加载程序能够从该引用中创建测试列表。 下面你可以找到一些具体的内置Avocado装载机的细节。对于通过插件(VT, Robot…)引入Avocado的装载机,请参阅相应的加载 loader/plugin 文档。 文件加载器对于文件加载器,加载器负责发现 INSTRUMENTED, PyUNITTEST(经典Python UNITTEST)和SIMPLE测试。 如果文件对应于 INSTRUMENTED 或 PyUNITTEST,可以通过在测试引用后面添加：,来筛选测试ID,：后面是正则表达式。 例如,如果您想列出所有在gdbtest.py文件中存在的测试,可以使用下面的列表命令： 12345678910111213141516171819202122$ avocado list /usr/share/doc/avocado/tests/gdbtest.pyINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_start_exitINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_existing_commands_rawINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_existing_commandsINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_load_set_breakpoint_run_exit_rawINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_load_set_breakpoint_run_exitINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_generate_coreINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_set_multiple_breakINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_disconnect_rawINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_disconnectINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_remote_execINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_stream_messagesINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_connect_multiple_clientsINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_server_exitINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_multiple_serversINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_interactiveINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_interactive_argsINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_exit_statusINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_server_stderrINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_server_stdoutINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_interactive_stdoutINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_remote 若要筛选结果,只列出在测试方法名称中具有test_interactive的测试,则可以执行： 1234$ avocado list /usr/share/doc/avocado/tests/gdbtest.py:test_interactiveINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_interactiveINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_interactive_argsINSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_interactive_stdout ：之后的字符串是正则表达式,三个测试被过滤进去。您可以操作正则表达式,使其具有确切名称的测试： 12$ avocado list /usr/share/doc/avocado/tests/gdbtest.py:test_interactive$INSTRUMENTED /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_interactive 一旦测试引用提供了预期的结果,您就可以用Run子命令替换列表子命令来执行测试： 12345678$ avocado run /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_[le].*rawJOB ID : 333912fb02698ed5339a400b832795a80757b8afJOB LOG : $HOME/avocado/job-results/job-2017-06-14T14.54-333912f/job.log (1/2) /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_existing_commands_raw: PASS (0.59 s) (2/2) /usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_load_set_breakpoint_run_exit_raw: PASS (0.42 s)RESULTS : PASS 2 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0 | CANCEL 0JOB TIME : 1.15 sJOB HTML : $HOME/avocado/job-results/job-2017-06-14T14.54-333912f/html/results.html 特别是在使用正则表达式时,建议单独用引号包裹测试引用,以避免损坏它们。在这种情况下,上述示例的命令将是： 1avocado run &quot;/usr/share/doc/avocado/tests/gdbtest.py:GdbTest.test_[le].*raw&quot; 外部加载器使用External Loader, Avocado 会考虑它,ExternalRunner 将会就位,所以Avocado不会真的去解析这个引用. 相反,Avocaddo将把引用作为参数传递给这个External Runner. 示例: 123$ avocado run 20Unable to resolve reference(s) &apos;20&apos; with plugins(s) &apos;file&apos;, &apos;robot&apos;,&apos;vt&apos;, &apos;external&apos;, try running &apos;avocado list -V 20&apos; to see the details. 在上面的命令中,没有加载程序可以将20解析为一个测试.但是在上面的命令中台添加一个 External Runner /bin/sleep 将会使 Avocado 执行 /bin/sleep 20 并且检查它返回的代码: 1234567$ avocado run 20 --loaders external:/bin/sleepJOB ID : 42215ece2894134fb9379ee564aa00f1d1d6cb91JOB LOG : $HOME/avocado/job-results/job-2017-06-19T11.17-42215ec/job.log (1/1) 20: PASS (20.03 s)RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0 | CANCEL 0JOB TIME : 20.13 sJOB HTML : $HOME/avocado/job-results/job-2017-06-19T11.17-42215ec/html/results.html 将测试引用放在命令行的末尾, 在一个 – 之后更安全. 这将避免争论与测试引用冲突。在这种情况下,之后的所有内容都将被视为位置参数。考虑以上语法, 上个例子的命令是: 1avocado run --loaders external:/bin/sleep -- 20 日志系统本节介绍日志系统在Avocado和Avocado试验中的应用。 调整UIAvocado使用Python的日志系统来生成UI并存储测试输出。该系统非常灵活,允许您通过内置流集或直接使用流名称来调整输出到您的需求。要调整它们,可以使用 avocado -–show STREAM[:LEVEL][,STREAM[:LEVEL],…] run ...。具有描述的内置流(随后是相关联的Python流列表)： app: 基于文本的UI (avocado.app)test: 执行测试的输出 (avocado.test, “”)debug: 用于调试avocado的附加消息 (avocado.app.debug)remote: Fabric/paramiko 调试消息,用于分析远程执行 (avocado.fabric, paramiko)early: 在日志记录系统设置之前进行早期日志记录。它包括测试输出和由使用的库产生的大量输出 (“”, avocado.test) 另外,您可以指定“全部”或“否”来启用/禁用所有预定义流,还可以提供自定义Python日志流,并将它们传递到标准输出。 存储自定义日志当您运行测试时,您还可以通过avocado run –store-logging-stream [STREAM[:LEVEL] [STREAM[:LEVEL] …]]将自定义日志流存储到结果目录中,它将在测试结果目录中生成每一个(唯一的)条目的$STREAM.$LEVEL文件。 必须指定分离的日志流。在这个函数中不能使用内置流。 目前,自定义流仅存储在每个job中,而不是针对每个单独的测试。 分页器一些子命令(列表,插件,…)支持”paginator”,在兼容的终端上,基本上将有色输出管道设置为较少,以简化生成的输出的浏览。可以通过–paginator {on|off}.禁用它。 sysinfo收集Avocado附带一个sysinfo插件,它自动收集每个系统的系统信息,甚至在测试之间。当我们想知道是什么导致测试失败是这是非常有用的。这个系统是可配置的,但是我们为您提供了一组明智的默认值。 在 Avocado 默认配置 /etc/avocado/avocado.conf有一节sysinfo.collect您可以在其中启用/禁用sysinfo集合以及配置基本环境。在sysinfo.collectibles节中,您可以定义在何处寻找sysinfo 集合之前/期间执行哪些命令/任务的基本路径。Avocado支持三种类型的任务： command: 用新行分隔的命令列表,在job/test之前和之后执行命令(单执行命令)。可以通过在[sysinfo.collect]中设置commands_timeout为正数来为每个执行命令所执行的超时。 file: 使用新行分隔的文件列表,表示要复制的文件 profilers: 文件具有新的行分隔的命令列表,在job/test之前执行并在job/test结束时被杀死(类似命令) 此外,这个插件试图通过journalctl跟踪系统日志,如果可用的话。 默认情况下,每个job都会收集这些数据,但也可以通过在sysinfo.collect节中设置per_test = True在每个test中运行它们。 如果需要的话,也可以在命令行上通过--sysinfo on|off来启用/禁用sysinfo。 job执行后,您可以在$RESULTS/test-results/$TEST/sysinfo的$RESULTS/sysinfo找到所收集的信息,它被分类为前、后和概要文件夹,文件名是安全地执行命令或文件名。当您启用HTML结果插件时,还可以在HTML结果中看到sysinfo。 如果使用源代码的Avocado,则需要手动放置commands/files/profilers到/etc/avocado/sysinfo或者调整$AVOCADO_SRC/etc/avocado/avocado.conf的路径 译者: 使用pip 安装的配置文件在诸如/usr/lib/python3.6/site-packages/avocado/etc/avocado/avocado.conf路径中,真实路径可以使用avocado config命令进行查询 测试参数本节详细介绍了哪些测试参数以及整个变体机制在Avocado中的工作原理。 如果您对基础知识感兴趣,请参阅Yaml_to_mux插件中的示例访问测试参数或实际视图。 Avocado允许将参数传递给测试,这有效地导致每个测试的几种不同变体。 这些参数在(test)的self.params中可用,并且是avocado.core.varianter.AvocadoParams类型。 self.params的数据由avocado.core.varianter.Varianter提供,它会询问所有已注册的插件的变体,或者在没有定义变体时使用默认值。 params处理如何工作的总体情况是： 1234567891011121314151617181920212223242526272829303132333435363738394041 +-----------+ | | // Test使用variant来生成AvocadoParams | Test | | | +-----^-----+ | // 将单个变量传递给测试 | +-----------+ | Runner | // 迭代测试和变量来运行所有 +-----^-----+ // 由“--execution-order”指定的所需组合 | |+-------------------+ 提供变量 +-----------------------+| |&lt;-----------------| || Varianter API | | Varianter plugins API || |-----------------&gt;| |+-------------------+ 更新默认值 +-----------------------+ ^ ^ | | | // 默认参数注入 | // 调用所有插件+--------------------------------------+ | // 轮流| +--------------+ +-----------------+ | || | avocado-virt | | other providers | | || +--------------+ +-----------------+ | |+--------------------------------------+ | | +----------------------------+-----+ | | | | v v +--------------------+ +-------------------------+ | yaml_to_mux plugin | | Other variant plugin(s) | +-----^--------------+ +-------------------------+ | | // yaml 被解析为 MuxTree, | // multiplexed and yields variants +---------------------------------+ | +------------+ +--------------+ | | | --mux-yaml | | --mux-inject | | | +------------+ +--------------+ | +---------------------------------+ 我们来介绍一下基本的关键词。 TreeNodeavocado.core.tree.TreeNode 节点对象是否允许使用parent-&gt; multiple_children关系创建树状结构并存储参数。 它还可以报告它的环境,这是从根到此节点收集的一组参数。 这用于测试,而不是传递完整树,只传递叶节点,它们的环境代表树的所有值 AvocadoParamsavocado.core.varianter.AvocadoParams 在每个(instrumented)Avocado测试中存在params的“数据库”。 它是在avocado.core.test.Test的init期间,从变量中生成的。 它接受TreeNode对象列表; 测试名称avocado.core.test.TestID(用于记录目的)和默认路径列表(参数路径)。 在测试中,它允许使用以下方法查询数据： 1self.params.get($name, $path=None, $default=None) name - 参数名称(键) path - 查找此参数的位置(未指定时使用mux-path) default - 找不到param时返回的内容(默认值) 每个变量都定义了一个层次结构,该层次结构会被保留,因此AvocadoParams跟随它以返回最合适的值或在出错时引发异常。 参数路径 Parameter Paths由于测试参数在树中组织,因此可以在多个位置具有相同的变量。 当它们从同一个TreeNode生成时,它不是问题,但是当它们是不同的值时,无法区分应报告的内容。 一种方法是在询问参数时使用特定路径,但有时候,通常在组合上游和下游变体时,我们希望首先得到我们的值,然后在找不到它们时回退到上游值。 例如,假设我们在/upstream/sleeptest中有上游值,并且在/downstream/sleeptest中也有值。 如果我们使用路径”“询问值,则会引发异常,因为程序无法区分是否需要来自”/downstream”或”/upstream”的值。 我们可以将参数路径设置为[“/downstream/“,”/upstream/“]以使所有相对调用(以开头的路径)首先查看/downstream中的节点,如果未找到则查看/ upstream。 VariantVariant是由Varianter_s生成的一组参数,并由测试运行员作为“params”参数传递给测试。 最简单的变体是None,它仍然会生成一个空的AvocadoParams。 此外,变量也可以是元组(列表,路径)或只是带有参数的avocado.core.tree.TreeNode列表。 Dumping/Loading Variants根据参数的数量,生成变量可能非常耗费计算量。 由于变量是作为作业执行的一部分生成的,因此计算密集型任务将由被测系统执行,从而导致这些系统上可能不需要的CPU负载。 为了避免这种情况,您可以获取由变量计算生成的生成的JSON序列化变体文件,并将该文件加载到将执行作业的系统上。 有两种方法可以获取JSON序列化变体文件： 使用avocado variants命令的–json-variants-dump选项： 12345$ avocado variants --mux-yaml examples/yaml_to_mux/hw/hw.yaml --json-variants-dump variants.json...$ file variants.jsonvariants.json: ASCII text, with very long lines, with no line terminators 执行Avocado作业后获取自动生成的JSON序列化变体文件： 12345 avocado run passtest.py --mux-yaml examples/yaml_to_mux/hw/hw.yaml...$ file $HOME/avocado/job-results/latest/jobdata/variants.json$HOME/avocado/job-results/latest/jobdata/variants.json: ASCII text, with very long lines, with no line terminators 获得variants.json文件后,可以将其加载到将要执行作业的系统上： 123456789101112$ avocado run passtest.py --json-variants-load variants.jsonJOB ID : f2022736b5b89d7f4cf62353d3fb4d7e3a06f075JOB LOG : $HOME/avocado/job-results/job-2018-02-09T14.39-f202273/job.log (1/6) passtest.py:PassTest.test;intel-scsi-56d0: PASS (0.04 s) (2/6) passtest.py:PassTest.test;intel-virtio-3d4e: PASS (0.02 s) (3/6) passtest.py:PassTest.test;amd-scsi-fa43: PASS (0.02 s) (4/6) passtest.py:PassTest.test;amd-virtio-a59a: PASS (0.02 s) (5/6) passtest.py:PassTest.test;arm-scsi-1c14: PASS (0.03 s) (6/6) passtest.py:PassTest.test;arm-virtio-5ce1: PASS (0.04 s)RESULTS : PASS 6 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0 | CANCEL 0JOB TIME : 0.51 sJOB HTML : $HOME/avocado/job-results/job-2018-02-09T14.39-f202273/results.html Varianteravocado.core.varianter.Varianter是一个内部对象,用于与Avocado中的变体机制进行交互。 它的生命周期是两个阶段的复合。 首先,它允许核心/插件注入默认值,然后对其进行解析,只允许查询值,变体数量等。 avocado run passtest.py -m example.yaml的示例工作流程是： 123456789101112131415161718avocado run passtest.py -m example.yaml | + parser.finish -&gt; Varianter.__init__ // dispatcher initializes all plugins | + $PLUGIN -&gt; args.default_avocado_params.add_default_param // could be used to insert default values | + job.run_tests -&gt; Varianter.is_parsed | + job.run_tests -&gt; Varianter.parse | // processes default params | // initializes the plugins | // updates the default values | + job._log_variants -&gt; Varianter.to_str // prints the human readable representation to log | + runner.run_suite -&gt; Varianter.get_number_of_tests | + runner._iter_variants -&gt; Varianter.itertests // Yields variants 为了允许强制更新Varianter,它支持ignore_new_data,可用于忽略新数据。 Job Replay使用它将当前运行的Varianter替换为从重放作业加载的Varianter。 ignore_new_data的工作流程可能如下所示： 123456789101112131415161718avocado run --replay latest -m example.yaml | + $PLUGIN -&gt; args.default_avocado_params.add_default_param // could be used to insert default values | + replay.run -&gt; Varianter.is_parsed | + replay.run // Varianter object is replaced with the replay job&apos;s one | // Varianter.ignore_new_data is set | + $PLUGIN -&gt; args.default_avocado_params.add_default_param // is ignored as new data are not accepted | + job.run_tests -&gt; Varianter.is_parsed | + job._log_variants -&gt; Varianter.to_str | + runner.run_suite -&gt; Varianter.get_number_of_tests | + runner._iter_variants -&gt; Varianter.itertests Varianter本身只能使用Default params生成一个空变量,但是它会调用所有Varianter插件,如果它们中的任何一个报告变量,它会生成它们而不是默认变量。 Default paramsDefault params是一种在Varianter或Varianter插件中指定默认值的机制。 它们的目的通常是定义依赖于系统的值,这些值不应影响测试的结果。 一个例子是qemu二进制位置,它可能因主机而异,但最终它们应该导致qemu在测试中可执行。 因此,Default params不会影响测试的variant-id(至少不会影响官方的Varianter插件)。 可以通过从args获取default_avocado_params并使用以下命令从plugin/core设置这些参数： 1default_avocado_params.add_default_parma(self, name, key, value, path=None) name - 注入数据的插件的名称(尚未用于任何内容,但我们计划允许白/黑列表)key - 参数的名称value - 参数的值path - 此参数的位置。 当路径尚不存在时,它是由TreeNode创建的。 Varianter pluginsavocado.core.plugin_interfaces.Varianter 一个插件接口,可用于构建自定义插件,Varianter使用它来获取测试变体。 有关灵感,请参阅avocado_varianter_yaml_to_mux.YamlToMux这是一个可选的varianter插件。 有关此插件的详细信息,请访问Yaml_to_mux插件。 Multiplexeravocado.core.mux Multiplexer或简称Mux是一个抽象概念,它是树状参数结构背后的基本思想,支持产生所有可能的变体。 在创建自定义插件时可以使用基本构建块的核心实现。 在avocado_varianter_yaml_to_mux中有一个使用此概念的插件的演示版本,它添加了一个解析器,然后使用此多路复用器概念来定义一个avocado插件,以便从yaml(或json)文件生成变体。 Multiplexer concept如前所述,这是构建块的内核实现,旨在基于定义了Multiplex域的树来编写Varianter插件。 可用的块是： MuxTree - 表示树的一部分并处理多路复用的对象,这意味着从树状对象生成所有可能的变体。 MuxPlugin - 构建Varianter插件的基类 MuxTreeNode - 从TreeNode继承并添加对控制标志(MuxTreeNode.ctrl)和Multiplex域(MuxTreeNode.multiplex)的支持。 Multiplex domains带变量的默认AvocadoParams树可能如下所示： 123456Multiplex tree representation: ┣━━ paths ┃ → tmp: /var/tmp ┃ → qemu: /usr/libexec/qemu-kvm ┗━━ environ → debug: False 多路复用器想要产生类似的结构,但也能够定义不仅一个变体,而是定义所有可能的组合,然后将切片报告为变体。 我们使用术语Multiplex域来定义此节点的子节点不仅仅是不同的路径,但它们是不同的值,我们一次只需要一个。 在表示中,我们使用双线来可视地区分正常关系和多路关系。 让我们稍微修改一下我们的例子： 123456789Multiplex tree representation: ┣━━ paths ┃ → tmp: /var/tmp ┃ → qemu: /usr/libexec/qemu-kvm ┗━━ environ ╠══ production ║ → debug: False ╚══ debug → debug: True 不同之处在于environ现在是一个多重节点,它的子节点将一次产生一个,产生两个变体： 1234567891011121314Variant 1: ┣━━ paths ┃ → tmp: /var/tmp ┃ → qemu: /usr/libexec/qemu-kvm ┗━━ environ ┗━━ production → debug: FalseVariant 2: ┣━━ paths ┃ → tmp: /var/tmp ┃ → qemu: /usr/libexec/qemu-kvm ┗━━ environ ┗━━ debug → debug: False 请注意,Multiplex仅与直接子项有关,因此变体中的叶数可能不同： 123456789101112Multiplex tree representation: ┣━━ paths ┃ → tmp: /var/tmp ┃ → qemu: /usr/libexec/qemu-kvm ┗━━ environ ╠══ production ║ → debug: False ╚══ debug ┣━━ system ┃ → debug: False ┗━━ program → debug: True 使用/ paths和/ paths,/ environ / debug / system和/ environ / debug / program生成一个带/ paths和/ environ / production的变体和其他变体。 如前所述,权力不是产生一种变体,而是定义具有所有可能变体的巨大情景。 通过使用具有多重域的树结构,您可以避免从Jenkin的稀疏矩阵作业中可能知道的大多数丑陋过滤器。 为了比较,我们来看看Avocado中的相同例子： 1234567891011121314151617Multiplex tree representation: ┗━━ os ┣━━ distro ┃ ┗━━ redhat ┃ ╠══ fedora ┃ ║ ┣━━ version ┃ ║ ┃ ╠══ 20 ┃ ║ ┃ ╚══ 21 ┃ ║ ┗━━ flavor ┃ ║ ╠══ workstation ┃ ║ ╚══ cloud ┃ ╚══ rhel ┃ ╠══ 5 ┃ ╚══ 6 ┗━━ arch ╠══ i386 ╚══ x86_64 123456789101112Variant 1: /os/distro/redhat/fedora/version/20, /os/distro/redhat/fedora/flavor/workstation, /os/arch/i386Variant 2: /os/distro/redhat/fedora/version/20, /os/distro/redhat/fedora/flavor/workstation, /os/arch/x86_64Variant 3: /os/distro/redhat/fedora/version/20, /os/distro/redhat/fedora/flavor/cloud, /os/arch/i386Variant 4: /os/distro/redhat/fedora/version/20, /os/distro/redhat/fedora/flavor/cloud, /os/arch/x86_64Variant 5: /os/distro/redhat/fedora/version/21, /os/distro/redhat/fedora/flavor/workstation, /os/arch/i386Variant 6: /os/distro/redhat/fedora/version/21, /os/distro/redhat/fedora/flavor/workstation, /os/arch/x86_64Variant 7: /os/distro/redhat/fedora/version/21, /os/distro/redhat/fedora/flavor/cloud, /os/arch/i386Variant 8: /os/distro/redhat/fedora/version/21, /os/distro/redhat/fedora/flavor/cloud, /os/arch/x86_64Variant 9: /os/distro/redhat/rhel/5, /os/arch/i386Variant 10: /os/distro/redhat/rhel/5, /os/arch/x86_64Variant 11: /os/distro/redhat/rhel/6, /os/arch/i386Variant 12: /os/distro/redhat/rhel/6, /os/arch/x86_64 与Jenkin的稀疏矩阵对比： 12345678os_version = fedora20 fedora21 rhel5 rhel6os_flavor = none workstation cloudarch = i386 x86_64filter = ((os_version == &quot;rhel5&quot;).implies(os_flavor == &quot;none&quot;) &amp;&amp; (os_version == &quot;rhel6&quot;).implies(os_flavor == &quot;none&quot;)) &amp;&amp; !(os_version == &quot;fedora20&quot; &amp;&amp; os_flavor == &quot;none&quot;) &amp;&amp; !(os_version == &quot;fedora21&quot; &amp;&amp; os_flavor == &quot;none&quot;) 这仍然是一个相对简单的例子,但它随着内部依赖性而急剧增长。 MuxPlugin定义avocado.core.plugin_interfaces.Varianter所需的完整接口。 插件编写者应该从这个MuxPlugin继承,然后从Varianter继承并调用： self.initialize_mux(root, paths, debug) root - 是params树的根(类似TreeNode节点的复合体) paths - 是测试中使用的所有变体的参数路径 debug - 是否使用调试模式(要求传递的树是TreeNodeDebug类节点的复合,它存储变量/值/环境的来源作为列表用途的值,并且NOT用于测试执行。 MuxTree这是努力工作的核心功能。 当在搜索叶节点时到达另一个multiplex时,它遍历树并记住所有叶节点或使用MuxTree列表。 当它被要求报告变体时,它组合了每个记忆项目的一个变体(叶子节点始终保持不变,但MuxTree圈出它的值),递归地产生不同多重域的所有可能变体。 工作重演为了使用相同的数据再现给定的job,我们可以使用--replay选项执行run命令,从原始job中得知hash id以实现重演.hash id可以只是一部分,只要所提供的部分对应于原始job id,并且它也足够独特。或者,代替job id,您可以使用最新的字符串,Avocado将重演最新执行的job。 让我们来看一个例子。首先,用两个测试引用运行一个简单的job： 12345678$ avocado run /bin/true /bin/falseJOB ID : 825b860b0c2f6ec48953c638432e3e323f8d7cadJOB LOG : $HOME/avocado/job-results/job-2016-01-11T16.14-825b860/job.log (1/2) /bin/true: PASS (0.01 s) (2/2) /bin/false: FAIL (0.01 s)RESULTS : PASS 1 | ERROR 0 | FAIL 1 | SKIP 0 | WARN 0 | INTERRUPT 0JOB TIME : 0.12 sJOB HTML : $HOME/avocado/job-results/job-2016-01-11T16.14-825b860/html/results.html 现在我们可以重新运行这个job： 123456789$ avocado run --replay 825b86JOB ID : 55a0d10132c02b8cc87deb2b480bfd8abbd956c3SRC JOB ID : 825b860b0c2f6ec48953c638432e3e323f8d7cadJOB LOG : $HOME/avocado/job-results/job-2016-01-11T16.18-55a0d10/job.log (1/2) /bin/true: PASS (0.01 s) (2/2) /bin/false: FAIL (0.01 s)RESULTS : PASS 1 | ERROR 0 | FAIL 1 | SKIP 0 | WARN 0 | INTERRUPT 0JOB TIME : 0.11 sJOB HTML : $HOME/avocado/job-results/job-2016-01-11T16.18-55a0d10/html/results.html 回放功能将检索原始测试引用、变量和配置。让我们看看另一个例子,现在使用mux YAML文件： 12345678910$ avocado run /bin/true /bin/false --mux-yaml mux-environment.yamlJOB ID : bd6aa3b852d4290637b5e771b371537541043d1dJOB LOG : $HOME/avocado/job-results/job-2016-01-11T21.56-bd6aa3b/job.log (1/4) /bin/true;first-c49a: PASS (0.01 s) (2/4) /bin/true;second-f05f: PASS (0.01 s) (3/4) /bin/false;first-c49a: FAIL (0.04 s) (4/4) /bin/false;second-f05f: FAIL (0.04 s)RESULTS : PASS 2 | ERROR 0 | FAIL 2 | SKIP 0 | WARN 0 | INTERRUPT 0JOB TIME : 0.19 sJOB HTML : $HOME/avocado/job-results/job-2016-01-11T21.56-bd6aa3b/html/results.html 我们可以使用$ avocado run --replay latest重新运行job,或者忽略变量运行job 12345678910$ avocado run --replay bd6aa3b --replay-ignore variantsIgnoring variants from source job with --replay-ignore.JOB ID : d5a46186ee0fb4645e3f7758814003d76c980bf9SRC JOB ID : bd6aa3b852d4290637b5e771b371537541043d1dJOB LOG : $HOME/avocado/job-results/job-2016-01-11T22.01-d5a4618/job.log (1/2) /bin/true: PASS (0.01 s) (2/2) /bin/false: FAIL (0.01 s)RESULTS : PASS 1 | ERROR 0 | FAIL 1 | SKIP 0 | WARN 0 | INTERRUPT 0JOB TIME : 0.12 sJOB HTML : $HOME/avocado/job-results/job-2016-01-11T22.01-d5a4618/html/results.html 此外,可以只重演给定结果的变体,使用--replay-test-status选项,查看以下示例: 1234567891011$ avocado run --replay bd6aa3b --replay-test-status FAILJOB ID : 2e1dc41af6ed64895f3bb45e3820c5cc62a9b6ebSRC JOB ID : bd6aa3b852d4290637b5e771b371537541043d1dJOB LOG : $HOME/avocado/job-results/job-2016-01-12T00.38-2e1dc41/job.log (1/4) /bin/true;first-c49a: SKIP (2/4) /bin/true;second-f05f: SKIP (3/4) /bin/false;first-c49a: FAIL (0.03 s) (4/4) /bin/false;second-f05f: FAIL (0.04 s)RESULTS : PASS 0 | ERROR 0 | FAIL 24 | SKIP 24 | WARN 0 | INTERRUPT 0JOB TIME : 0.29 sJOB HTML : $HOME/avocado/job-results/job-2016-01-12T00.38-2e1dc41/html/results.html 其中一个特殊的例子是--replay-test-status INTERRUPTED或--replay-resume,它跳过所执行的测试,只执行取消测试后取消或未执行的测试。这个特性即使在系统崩溃等严重中断时也可以工作。 在重演用--failfast on选项执行的job时,可以使用--failfast off禁用failfast 选项重演job。 为了能够重演job,Avocado将job数据记录在同一个job结果目录中,在一个名为replay的子目录内。如果给定的job有一个非默认路径来记录日志,当重播时间到来时,我们需要通知日志在何处。见下面的例子： 1234567$ avocado run /bin/true --job-results-dir /tmp/avocado_results/JOB ID : f1b1c870ad892eac6064a5332f1bbe38cda0aaf3JOB LOG : /tmp/avocado_results/job-2016-01-11T22.10-f1b1c87/job.log (1/1) /bin/true: PASS (0.01 s)RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0JOB TIME : 0.11 sJOB HTML : /tmp/avocado_results/job-2016-01-11T22.10-f1b1c87/html/results.html 试图重演这项job,但失败了: 12$ avocado run --replay f1b1can&apos;t find job results directory in &apos;$HOME/avocado/job-results&apos; 在这种情况下,我们必须通知工作结果目录位于何处： 12345678$ avocado run --replay f1b1 --replay-data-dir /tmp/avocado_resultsJOB ID : 19c76abb29f29fe410a9a3f4f4b66387570edffaSRC JOB ID : f1b1c870ad892eac6064a5332f1bbe38cda0aaf3JOB LOG : $HOME/avocado/job-results/job-2016-01-11T22.15-19c76ab/job.log (1/1) /bin/true: PASS (0.01 s)RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0JOB TIME : 0.11 sJOB HTML : $HOME/avocado/job-results/job-2016-01-11T22.15-19c76ab/html/results.html 工作差异Avocadodiff插件允许用户轻松地比较两个给定的job的几个方面。基本用法是： 123456789101112131415161718$ avocado diff 7025aaba 384b949c--- 7025aaba9c2ab8b4bba2e33b64db3824810bb5df+++ 384b949c991b8ab324ce67c9d9ba761fd07672ff@@ -1,15 +1,15 @@ COMMAND LINE-/usr/bin/avocado run sleeptest.py+/usr/bin/avocado run passtest.py TOTAL TIME-1.00 s+0.00 s TEST RESULTS-1-sleeptest.py:SleepTest.test: PASS+1-passtest.py:PassTest.test: PASS ... Avocado Diff可以比较和创建一个统一的差异： 命令行 工作时间 变量和参数 测试结果 配置 sysinfo前后 结果中只包含不同内容的部分。还可以使用--diff-filter启用/禁用这些部分。请参阅avocado diff --help更多信息。 可以通过jobs ID、结果目录或latest来标识job。示例： 12345678910111213141516$ avocado diff ~/avocado/job-results/job-2016-08-03T15.56-4b3cb5b/ latest--- 4b3cb5bbbb2435c91c7b557eebc09997d4a0f544+++ 57e5bbb3991718b216d787848171b446f60b3262@@ -1,9 +1,9 @@ COMMAND LINE-/usr/bin/avocado run perfmon.py+/usr/bin/avocado run passtest.py TOTAL TIME-11.91 s+0.00 s TEST RESULTS-1-test.py:Perfmon.test: FAIL+1-examples/tests/passtest.py:PassTest.test: PASS 与统一的差异,你也可以生成HTML(选项 --html)差异文件,并可选地,打开它在您的首选浏览器(选项 --open browser)： 12$ avocado diff 7025aaba 384b949c --html /tmp/myjobdiff.html/tmp/myjobdiff.html 如果在没有--html的情况下使用 --open browser,我们将创建一个临时HTML文件。 对于那些希望使用自定义DIFF工具而不是Avocado DIFF工具的人,我们提供了--create-reports选项,因此我们创建了两个具有相关内容的临时文件。打印文件名,用户可以复制/粘贴到自定义DIFF工具命令行： 12345678910111213141516171819$ avocado diff 7025aaba 384b949c --create-reports/var/tmp/avocado_diff_7025aab_zQJjJh.txt /var/tmp/avocado_diff_384b949_AcWq02.txt$ diff -u /var/tmp/avocado_diff_7025aab_zQJjJh.txt /var/tmp/avocado_diff_384b949_AcWq02.txt--- /var/tmp/avocado_diff_7025aab_zQJjJh.txt 2016-08-10 21:48:43.547776715 +0200+++ /var/tmp/avocado_diff_384b949_AcWq02.txt 2016-08-10 21:48:43.547776715 +0200@@ -1,250 +1,19 @@ COMMAND LINE ============-/usr/bin/avocado run sleeptest.py+/usr/bin/avocado run passtest.py TOTAL TIME ==========-1.00 s+0.00 s... 远程运行测试Avocado子类使用子类来扩展Avocado测试类的特性是非常直接的,它可能构成了一个非常有用的办法,在项目存储库中托管一些共享/递归代码。 在本文档中,我们提出了一个项目组织,允许您创建和安装所谓的子框架。 让我们举个例子,一个叫做Apricot Framework的项目。这里是提议的文件系统结构： 12345678910~/git/apricot (master)$ tree.├── apricot│ ├── __init__.py│ └── test.py├── README.rst├── setup.py├── tests│ └── test_example.py└── VERSION 在SETUP.PY中,将Avocado框架包指定为依赖项是很重要的： 1234567891011from setuptools import setup, find_packagessetup(name=&apos;apricot&apos;, description=&apos;Apricot - Avocado SubFramwork&apos;, version=open(&quot;VERSION&quot;, &quot;r&quot;).read().strip(), author=&apos;Apricot Developers&apos;, author_email=&apos;apricot-devel@example.com&apos;, packages=[&apos;apricot&apos;], include_package_data=True, install_requires=[&apos;avocado-framework&apos;] ) VERSION: 你希望的文件版本 11.0 apricot/__init__.py:使您的新测试类在您的模块根目录中可用： 123__all__ = [&apos;ApricotTest&apos;]from apricot.test import ApricotTest apricot/test.py,在这里,您将基本上扩展Avocado测试类与您自己的方法和程序： 12345678from avocado import Testclass ApricotTest(Test): def setUp(self): self.log.info("setUp() executed from Apricot") def some_useful_method(self): return True tests/test_example.py:这就是测试的样子。这里最重要的一项是使用:avocado: recursive递归,所以Avocado测试加载器将能够识别您的测试类作为Avocado测试类： 12345678from apricot import ApricotTestclass MyTest(ApricotTest): """ :avocado: recursive """ def test(self): self.assertTrue(self.some_useful_method()) 非侵入的安装你的模块: 12345678910111213141516171819202122232425262728293031323334353637~/git/apricot (master)$ python setup.py develop --userrunning developrunning egg_infowriting requirements to apricot.egg-info/requires.txtwriting apricot.egg-info/PKG-INFOwriting top-level names to apricot.egg-info/top_level.txtwriting dependency_links to apricot.egg-info/dependency_links.txtreading manifest file &apos;apricot.egg-info/SOURCES.txt&apos;writing manifest file &apos;apricot.egg-info/SOURCES.txt&apos;running build_extCreating /home/apahim/.local/lib/python2.7/site-packages/apricot.egg-link (link to .)apricot 1.0 is already the active version in easy-install.pthInstalled /home/apahim/git/apricotProcessing dependencies for apricot==1.0Searching for avocado-framework==55.0Best match: avocado-framework 55.0avocado-framework 55.0 is already the active version in easy-install.pthUsing /home/apahim/git/avocadoSearching for stevedore==1.25.0Best match: stevedore 1.25.0Adding stevedore 1.25.0 to easy-install.pth fileUsing /usr/lib/python2.7/site-packagesSearching for six==1.10.0Best match: six 1.10.0Adding six 1.10.0 to easy-install.pth fileUsing /usr/lib/python2.7/site-packagesSearching for pbr==3.1.1Best match: pbr 3.1.1Adding pbr 3.1.1 to easy-install.pth fileInstalling pbr script to /home/apahim/.local/binUsing /usr/lib/python2.7/site-packagesFinished processing dependencies for apricot==1.0 然后运行你的测试 1234567~/git/apricot$ avocado run tests/test_example.pyJOB ID : 02c663eb77e0ae6ce67462a398da6972791793bfJOB LOG : $HOME/avocado/job-results/job-2017-11-16T12.44-02c663e/job.log (1/1) tests/test_example.py:MyTest.test: PASS (0.03 s)RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0 | CANCEL 0JOB TIME : 0.95 sJOB HTML : $HOME/avocado/job-results/job-2017-11-16T12.44-02c663e/results.html 使用GDB Debugging通过测试运行可执行包Avocado允许可执行文件的测试以易懂的方式运行。用户指定一个脚本(“包装器”),用于运行由测试调用的实际程序。 如果测试脚本被正确实现,它不应该干扰测试行为。也就是说,包装器应该避免改变原始可执行文件的返回状态、标准输出和标准错误消息。 用户可以指定要封装哪个程序(具有类似shell的Glob),或者如果省略了,将应用于测试调用的所有程序的全局包装器。 这个特性是作为插件实现的,它将–wraper添加到Avocado运行命令中。 作为包装器以易懂的方式运行strace的示例： 12#!/bin/shexec strace -ff -o $AVOCADO_TEST_LOGDIR/strace.log -- $@ 让所有程序由test.py开始,用~/bin/my-wrapper.sh包装： 1$ scripts/avocado run --wrapper ~/bin/my-wrapper.sh tests/test.py 只有我的my-binary文件用~/bin/my-wrapper.sh包装 1$ scripts/avocado run --wrapper ~/bin/my-wrapper.sh:*my-binary tests/test.py 警示 不可能用GDB(–gdb-run-bin)进行调试,同时使用包装器(-包装器)。这两个选项是互斥的。 只能设置一个(全局)包装器。如果需要两个包装器中存在的功能,则必须将这些组合成单个包装器脚本。 只有使用avocado.utils.process(以及使用它的其他API模块:avocado.utils.build)运行的可执行文件)才会受到此特性的影响。 插件系统Utilities可选插件参考指南贡献与社区指南Avocado发展要点释放avocado其它资源测试API文档这是用户在编写测试时应该使用并且可以依赖的最小API集。 avocado.mainavocado.core.job.TestProgram的别名 class avocado.Test(methodName=&#39;test&#39;, name=None, params=None, base_logdir=None, job=None, runner_queue=None)测试类的基本实现。 你将继承自己编写自己的测试。 通常,您需要在自己的测试中实现setUp(),test*()和tearDown()方法。 初始化测试。 参数： methodName - 要运行的主方法的名称。 为了与原始unittest类兼容,您不应该设置它。 name(avocado.core.test.TestID) - 测试名称的漂亮名称。 对于使用AvocadoAPI编写的常规测试,不应设置此项。 这保留给内部Avocado使用,例如将随机可执行文件作为测试运行时。 base_logdir - 测试日志应该到达的目录。 如果提供None,则使用avocado.data_dir.create_job_logs_dir()。 job - 此测试所属的工作。 basedir此测试(由文件支持)所在的目录 cache_dirs返回配置文件中设置的缓存目录列表。 cancel(message=None)取消测试。 期望从测试方法调用此方法,而不是其他任何地方,因为根据定义,我们只能取消当前正在执行的测试。 如果在测试方法之外调用此方法,avocado会将测试状态标记为ERROR,并指示您在错误消息中修复测试。 参数：message(str) - 将记录在日志中的可选消息 error(message=None)使当前正在运行的测试状态为错误。 调用此方法后,将终止测试并将其状态设置为ERROR。 参数：message(str) - 将记录在日志中的可选消息 fail(message=None)fail_classfail_reasonfetch_asset(name,asset_hash = None,algorithm = None,locations = None,expire = None)方法o调用utils.asset以获取和支持散列检查,缓存和多个位置的资产文件。 参数： name - 资产文件名或URL asset_hash - 资产哈希(可选) algorithm - 哈希算法(可选,默认为avocado.utils.asset.DEFAULT_HASH_ALGORITHM) locations - 可从中获取资产的URL列表(可选) expire - 资产到期的时间raise： EnvironmentError - 无法获取资产时EnvironmentError - 无法获取资产时返回：资产文件本地路径 filename返回包含当前测试的文件(路径)的名称 get_state()序列化表示测试状态的选定属性 返回：包含相关测试状态数据的字典返回类型：字典 job这项测试与之相关的工作 log增强的测试日志 logdir此测试的日志目录的路径 logfile此测试的主要debug.log文件的路径 name返回测试ID,其中包含测试名称 返回类型：TestID outputdir可用于测试编写者将文件附加到结果的目录 params此测试的参数(AvocadoParam实例) report_state()将当前测试状态发送到测试运行程序进程 run_avocado()包装run方法,用于在Avocado跑步者内执行。 结果：未使用的参数,与unittest.TestCase的兼容性。 runner_queue测试和测试运行器之间的通信通道 running此测试目前是否正在执行 set_runner_queue(runner_queue)覆盖runner_queue status此测试的结果状态 teststmpdir返回临时目录的路径,该路径对于给定作业中的所有测试保持不变。 time_elapsed = -1测试执行的持续时间(总是从time_end - time_start重新计算 time_end = -1(unix)测试完成的时间(可能会被迫测试) time_start = -1(unix)测试开始的时间(可以强制进行测试) timeout = None测试超时(params的超时优先) tracebackwhiteboard=测试结束时将存储在$logdir/whiteboard位置的任意字符串。 workdir此属性返回在整个测试执行期间存在的可写目录,但在测试完成后将清除该目录。 它可以用于解压缩源代码压缩包,构建软件等任务。 avocado.fail_on(exceptions=None)当装饰函数产生指定类型的异常时,测试失败。 (例如,我们的方法可能会在测试软件失败时引发IndexError。我们可以尝试/捕获它或使用此装饰器代替) 参数：exceptions - 假定为测试失败的元组或单个异常[Exception]注意：self.error和self.cancel行为保持不变注意：为了允许简单使用,参数“exception”不能是可调用的 avocado.skip(message=None)跳过测试装饰器 avocado.skipIf(condition, message=None)如果条件为True,装饰器将跳过测试。 avocado.skipUnless(condition, message=None)如果条件为False,装饰器将跳过测试。 exception avocado.TestError基础：avocado.core.exceptions.TestBaseException 表示测试未完全执行且发生错误。 如果测试部分执行并且由于设置,配置或其他致命情况而无法完成,则会出现这种异常。 status =&#39;ERROR&#39; exception avocado.TestFail基础：avocado.core.exceptions.TestBaseException,exceptions.AssertionError 表示测试失败。 TestFail继承自AssertionError,以保持与vanilla python单元测试的兼容性(它们只考虑从AssertionError派生的失败)。 status =&#39;FAIL&#39; exception avocado.TestCancel基础：avocado.core.exceptions.TestBaseException 表示测试已取消。 使用cancel()测试方法时应该抛出。 status = &#39;CANCEL&#39; Utilities APIs这是一组实用程序API,Avocado为测试编写者提供了附加值。 它假设是通用的,没有任何Avocado知识,可以在不同的项目中重复使用。 在当前版本中,存在Avocado日志记录流的隐藏知识。 有关此问题的更多信息,请访问https://trello.com/c/4QyUgWsW/720-get-rid-of-avocado-test-loggers-from-avocado-utils 译者：此章节为翻译,详情请查看原文档 SubpackagesSubmodulesavocado.utils.archive moduleavocado.utils.asset moduleavocado.utils.astring moduleavocado.utils.aurl moduleavocado.utils.build moduleavocado.utils.cpu moduleavocado.utils.crypto moduleavocado.utils.data_factory moduleavocado.utils.data_structures moduleavocado.utils.debug moduleavocado.utils.disk moduleavocado.utils.download moduleavocado.utils.filelock moduleavocado.utils.gdb moduleavocado.utils.genio moduleavocado.utils.git moduleavocado.utils.disk moduleavocado.utils.distro moduleavocado.utils.download moduleavocado.utils.filelock moduleavocado.utils.gdb moduleavocado.utils.genio moduleavocado.utils.git moduleavocado.utils.iso9660 moduleavocado.utils.kernel moduleavocado.utils.linux_modules moduleavocado.utils.lv_utils moduleavocado.utils.memory moduleavocado.utils.multipath moduleavocado.utils.network moduleavocado.utils.output moduleavocado.utils.partition moduleavocado.utils.path moduleavocado.utils.pci moduleavocado.utils.process moduleavocado.utils.runtime moduleavocado.utils.script moduleavocado.utils.service moduleavocado.utils.software_manager moduleavocado.utils.stacktrace moduleavocado.utils.vmimage moduleavocado.utils.wait moduleModule contents内部(核心)API可能是Avocado 骇客感兴趣的内部API. 译者:此节未翻译,更多内容请查看原文档 扩展(插件)API扩展API可能是插件编写者兴趣所在. 译者:此节未翻译,更多内容请查看原文档 可选插件API下面的页面记录了可选的Avocado插件的私有API。 avocado_glib package avocado_runner_docker package avocado_resultsdb package avocado_varianter_yaml_to_mux package avocado_result_upload package avocado_runner_remote package avocado_robot package avocado_loader_yaml package avocado_varianter_pict package avocado_golang package 发行说明 译者:此节未翻译,更多内容请查看原文档 征求意见稿(RFCS) 译者:此节未翻译,更多内容请查看原文档 原文档原文档]]></content>
      <tags>
        <tag>测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[chrome从任何页面运行脚本snippets]]></title>
    <url>%2F2018%2F07%2F12%2F%E5%89%8D%E7%AB%AF%2Fchrome%E4%BB%8E%E4%BB%BB%E4%BD%95%E9%A1%B5%E9%9D%A2%E8%BF%90%E8%A1%8C%E8%84%9A%E6%9C%ACsnippets%2F</url>
    <content type="text"><![CDATA[常用的javascript脚本如何便捷的保存，以便于快速的在chrome中调用方法呢? chrome控制台中的snippets可以实现此功能。 打开控制台，进入source标签，点击snipets 在右侧编辑好脚本后,使用ctrl + S保存 右键点击左侧snipets可以运行或者重命名脚本,下次就可以方便的调用脚本了。 console控制台]]></content>
  </entry>
  <entry>
    <title><![CDATA[如何将普通字符串转化为dom节点插入页面]]></title>
    <url>%2F2018%2F07%2F11%2F%E5%89%8D%E7%AB%AF%2F%E5%A6%82%E4%BD%95%E5%B0%86%E6%99%AE%E9%80%9A%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E5%8C%96%E4%B8%BAdom%E8%8A%82%E7%82%B9%E6%8F%92%E5%85%A5%E9%A1%B5%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[innerHTML在elem内使用elem.innerHTML = html_str是最简单的方法 12elem = document.getElementsById('simple')elem.innerHTML = '&lt;h1&gt;simple&lt;/h1&gt;']]></content>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dom节点操作常用方法]]></title>
    <url>%2F2018%2F07%2F06%2F%E5%89%8D%E7%AB%AF%2Fdom%E8%8A%82%E7%82%B9%E6%93%8D%E4%BD%9C%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[操作常用方法访问/获取节点1234567document.getElementById(id); //返回对拥有指定id的第一个对象进行访问document.getElementsByName(name); //返回带有指定名称的节点集合 注意拼写:Elementsdocument.getElementsByTagName(tagname); //返回带有指定标签名的对象集合 注意拼写：Elementsdocument.getElementsByClassName(classname); //返回带有指定class名称的对象集合 注意拼写：Elements 创建节点/属性12345document.createElement(tagName); //创建一个节点document.createAttribute(attrName); //对某个节点创建属性document.createTextNode(text); //创建文本节点 添加节点123document.insertBefore(newNode,referenceNode); //在某个节点前插入节点parentNode.appendChild(newNode); //给某个节点添加子节点 复制节点1cloneNode(true | false); //复制某个节点 参数：是否复制原节点的所有属性 删除节点1parentNode.removeChild(node); //删除某个节点的子节点 node是要删除的节点 注意：为了保证兼容性，要判断元素节点的节点类型(nodeType)，若nodeType==1，再执行删除操作。通过这个方法，就可以在 IE和 Mozilla 完成正确的操作。 nodeType 属性可返回节点的类型.最重要的节点类型是： 元素类型 节点类型元素element 1属性attr 2文本text 3注释comments 8文档document 9 修改文本节点 方法 作用 appendData(data); 将data加到文本节点后面 deleteData(start,length); 将从start处删除length个字符 insertData(start,data); 在start处插入字符,start的开始值是0; replaceData(start,length,data); 在start处用data替换length个字符 splitData(offset); 在offset处分割文本节点 substringData(start,length); 从start处提取length个字符 属性操作12345getAttribute(name) //通过属性名称获取某个节点属性的值setAttribute(name,value); //修改某个节点属性的值removeAttribute(name); //删除某个属性 查找节点12345parentObj.firstChild; //如果节点为已知节点的第一个子节点就可以使用这个方法。此方法可以递归进行使用 parentObj.firstChild.firstChild.....parentObj.lastChild; //获得一个节点的最后一个节点，与firstChild一样也可以进行递归使用 parentObj.lastChild.lastChild.....parentObj.childNodes; //获得节点的所有子节点，然后通过循环和索引找到目标节点 获取相邻的节点123curtNode.previousSibling; //获取已知节点的相邻的上一个节点curtNode.nextSlbling; // 获取已知节点的下一个节点 获取父节点1childNode.parentNode; //得到已知节点的父节点 替换节点1node.replaceChild(newNode,oldNode); 获取文本内容innerHTMLinnerHTML可以作为获取文本的方法也可以作为修改文本内容的方法 element.innerHTML 会直接返回element节点下所有的HTML化的文本内容 文本文本document.body.innerHTML //返回”文本文本“; 同样逆向的： document.body.innerHTM=”文本“会生成文本文本！注意 innerHTML方法只能作用于元素节点调用；文本节点并不能使用这个方法返回undefined！ nodeValuenodeValue是一个HTML DOM的对象属性； 同样的 可以通过 nodeValue设置节点的文本内容也可以直接返回文本内容 直接用节点对象调用就都可以： 如上例 document.getElementsByTagName(div)[0].childNodes[0].nodeValue //返回“文本” 另外 nodeValue 属性并不只存在于文本节点下 元素节点和属性节点对象也都具有nodeValue属性 属性节点的 nodeValue属性返回属性值元素节点的 nodeValue属性返回null textContenttextContent与innerHTML方法类似会返回对象节点下所有的文本内容 但是区别为 textContent返回的内容只有去HTML化的文本节点的内容 如上例： document.body.textContent //返回”文本文本” ！注意在DOM中标签换行产生的空白字符会计入DOM中作为文本节点 另外IE8以前不支持textContent属性 innerTextinnerText方法与textContent方法类似 并且和innerHTML一样也是作用于元素节点上 但是浏览器对于这两种方法解析空白字符的机制不一样；不是很常用 类似的还有outText outHTML等类似操作文本相关的方法，不是很常用不介绍了； 最后要提醒一点：文本与文本节点一定要区分，有些方法是依靠元素节点返回子文本内容，有些方法是文本节点返回自身文本内容，文本节点是对象而文本只是字符串； Dom节点操作常用方法和获取文本内容]]></content>
      <tags>
        <tag>web前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[复制一个dom的所有样式到另一个dom]]></title>
    <url>%2F2018%2F07%2F05%2F%E5%89%8D%E7%AB%AF%2F%E5%A4%8D%E5%88%B6%E4%B8%80%E4%B8%AAdom%E7%9A%84%E6%89%80%E6%9C%89%E6%A0%B7%E5%BC%8F%E5%88%B0%E5%8F%A6%E4%B8%80%E4%B8%AAdom%2F</url>
    <content type="text"><![CDATA[如果不要求支持IE, 那么只要一行。 1dom.style.cssText = window.getComputedStyle(srcDom, null).cssText; 如果要兼容各个浏览器，那么要原始一点： 12345678910111213/** * IE8不支持window.getComputedStyle * IE9~11中，window.getComputedStyle().cssText返回的总为空字符串 * 默认的window.getComputedStyle || dom.currentStyle, 返回的css键值对中，键是驼峰命名的。 */var oStyle = (window.getComputedStyle &amp;&amp; window.getComputedStyle(srcDom, null)) || srcDom.currentStyle, cssText = '';for (var key in oStyle) &#123; var v = oStyle[key]; if (/^[a-z]/i.test(key) &amp;&amp; [null, '', undefined].indexOf(v) === -1) &#123; dom.style[key] = v; &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[sqlite3_python使用教程]]></title>
    <url>%2F2018%2F07%2F04%2Fpython%2F2.python%E6%A0%87%E5%87%86%E5%BA%93%2Fsqlite3-python%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[sqlite3安装Python 2.5.x 以上版本默认自带了sqlite3 模块模块,无需单独安装。在python中使用时，直接引入即可。 连接数据库下面的 Python 代码显示了如何连接到一个现有的数据库。如果数据库不存在，那么它就会被创建，最后将返回一个数据库对象。 12import sqlite3conn = sqlite3.connect('test.db') 创建表下面的 Python 代码段将用于在先前创建的数据库中创建一个表： 1234567891011121314import sqlite3conn = sqlite3.connect('test.db')c = conn.cursor()c.execute('''CREATE TABLE COMPANY (ID INT PRIMARY KEY NOT NULL, NAME TEXT NOT NULL, AGE INT NOT NULL, ADDRESS CHAR(50), SALARY REAL);''')conn.commit()conn.close() 上述程序执行时，它会在 test.db 中创建 COMPANY 表 INSERT 操作下面的 Python 程序显示了如何在上面创建的 COMPANY 表中创建记录： 12345678910111213141516171819import sqlite3conn = sqlite3.connect('test.db')c = conn.cursor()c.execute("INSERT INTO COMPANY (ID,NAME,AGE,ADDRESS,SALARY) \ VALUES (1, 'Paul', 32, 'California', 20000.00 )");c.execute("INSERT INTO COMPANY (ID,NAME,AGE,ADDRESS,SALARY) \ VALUES (2, 'Allen', 25, 'Texas', 15000.00 )");c.execute("INSERT INTO COMPANY (ID,NAME,AGE,ADDRESS,SALARY) \ VALUES (3, 'Teddy', 23, 'Norway', 20000.00 )");c.execute("INSERT INTO COMPANY (ID,NAME,AGE,ADDRESS,SALARY) \ VALUES (4, 'Mark', 25, 'Rich-Mond ', 65000.00 )");conn.commit()conn.close() 上述程序执行时，它会在 COMPANY 表中创建给定记录 SELECT 操作下面的 Python 程序显示了如何从前面创建的 COMPANY 表中获取并显示记录： 123456789101112131415import sqlite3conn = sqlite3.connect('test.db')c = conn.cursor()print "Opened database successfully";cursor = c.execute("SELECT id, name, address, salary from COMPANY")for row in cursor: print "ID = ", row[0] print "NAME = ", row[1] print "ADDRESS = ", row[2] print "SALARY = ", row[3], "\n"print "Operation done successfully";conn.close() UPDATE 操作下面的 Python 代码显示了如何使用 UPDATE 语句来更新任何记录，然后从 COMPANY 表中获取并显示更新的记录： 1234567891011121314151617import sqlite3conn = sqlite3.connect('test.db')c = conn.cursor()c.execute("UPDATE COMPANY set SALARY = 25000.00 where ID=1")conn.commit()print("Total number of rows updated :", conn.total_changes)cursor = conn.execute("SELECT id, name, address, salary from COMPANY")for row in cursor: print "ID = ", row[0] print "NAME = ", row[1] print "ADDRESS = ", row[2] print "SALARY = ", row[3], "\n"conn.close() DELETE 操作下面的 Python 代码显示了如何使用 DELETE 语句删除任何记录，然后从 COMPANY 表中获取并显示剩余的记录： 12345678910111213141516171819import sqlite3conn = sqlite3.connect('test.db')c = conn.cursor()print "Opened database successfully";c.execute("DELETE from COMPANY where ID=2;")conn.commit()print "Total number of rows deleted :", conn.total_changescursor = conn.execute("SELECT id, name, address, salary from COMPANY")for row in cursor: print "ID = ", row[0] print "NAME = ", row[1] print "ADDRESS = ", row[2] print "SALARY = ", row[3], "\n"print "Operation done successfully";conn.close()]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python代码库]]></title>
    <url>%2F2018%2F07%2F02%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E4%BB%A3%E7%A0%81%E5%BA%93%2F</url>
    <content type="text"><![CDATA[文件IO列出一个文件夹内的文件的名称和文件路径1234567891011121314151617181920212223242526import osfrom collections import namedtupledef get_file_list(dir_path = '.',file_types = '.*'): """ dir_path 参数接受一个文件路径字符串 file_types 接受诸如'.py'的字符串或者[.py,.md]的数组 return 返回一个包含nametuple的列表 nametuple包含文件名,文件绝对路径和文件类型nametuple """ File_data = namedtuple('File_data',['file_name','file_path','file_type']) # result_list = [(filename,filepath)] result_list = [] print(dir_path) # 得到绝对路径 dir_path = os.path.abspath(dir_path) print(dir_path) # 得到文件的文件名，绝对路径，文件类型 file_data_list = [File_data(file_name,os.path.join(dir_path,file_name),os.path.splitext(file_name)[1]) for file_name in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path,file_name))] # 如果没有传入file_type,默认传入 if file_types == '.*': return file_data_list # 筛选出符合条件的文件夹 file_data = (file_name,file_path,file_type) # '.gitignore 的filetype为'',特殊处理'' file_data_list = [File_data(file_name,file_path,file_type) for file_name,file_path,file_type in file_data_list if file_type and file_type in file_types] return file_data_list 数据库封装sqlite3常用方法,使其调用简单1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import sqlite3from functools import wrapsdef cursor(func): @wraps(func) def wrapper(self,*args, **kwargs): self.conn = sqlite3.connect(self.tablebase) self.cursor = self.conn.cursor() result = func(self,*args, **kwargs) self.cursor.close() self.conn.commit() self.conn.close() return result return wrapperclass SQlite(): def __init__(self,tablebase): self.tablebase = tablebase @cursor def create_table(self,tablename,**kw): header = ','.join(['&#123;0&#125; &#123;1&#125;'.format(key,kw[key]) for key in kw.keys()]) sql = 'CREATE TABLE &#123;tablename&#125;(&#123;header&#125;)'.format(tablename = tablename,header = header) print(sql) self.cursor.execute(sql) @cursor def insert(self,tablename,**kw): header = ','.join(kw.keys()) value = ','.join(["'&#123;0&#125;'".format(key) if isinstance(key,str) else str(key) for key in kw.values()]) sql = "INSERT INTO &#123;tablename&#125; (&#123;header&#125;) VALUES (&#123;value&#125;)".format(tablename = tablename,header = header,value = value) self.cursor.execute(sql) print(sql) def _select(self,tablename,*keys,where = ''): headers = ','.join(keys) sql = "SELECT &#123;0&#125; from &#123;1&#125; &#123;2&#125;".format(headers,tablename,where) result = self.cursor.execute(sql) return result @cursor def update(self): self.cursor.execute("UPDATE COMPANY set SALARY = 25000.00 where ID=1") @cursor def delete(self): self.cursor.execute("DELETE from COMPANY where ID=2;") @cursor def fetchall(self,tablename,*keys,where = ''): result = self._select(tablename,*keys,where = where) return result.fetchall() @cursor def fetchone(self,tablename,*keys,where = ''): result = self._select(tablename,*keys,where = where) return result.fetchone()]]></content>
  </entry>
  <entry>
    <title><![CDATA[win10系统使用iphone手机热点连接互联网]]></title>
    <url>%2F2018%2F06%2F29%2Fwindows%2Fwin10%E7%B3%BB%E7%BB%9F%E4%BD%BF%E7%94%A8iphone%E6%89%8B%E6%9C%BA%E7%83%AD%E7%82%B9%E8%BF%9E%E6%8E%A5%E4%BA%92%E8%81%94%E7%BD%91%2F</url>
    <content type="text"><![CDATA[背景:公司网络比较卡,而win10系统的wifi发现列表上无法显示iphone的热点,下面方法可以解决win10pc使用iphone热点连接互联网的问题 前提是你的iphone与windows蓝牙已经配对。 操作步骤 打开控制面板,选择点击查看网络状态和任务 点击更改适配器设置 右键点击蓝牙网络连接,选择查看蓝牙网络设备 稍等片刻,会显示出你的iphone,右键点击,选择连接时使用 &gt; 接入点 图例]]></content>
  </entry>
  <entry>
    <title><![CDATA[python中的exec和eval详解]]></title>
    <url>%2F2018%2F06%2F27%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E4%B8%AD%E7%9A%84exec%E5%92%8Ceval%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[简介python 动态执行字符串代码片段（也可以是文件）， 一般会用到exec,eval。 execexec(function[, globals[, locals]]) 如果指定了global_dic和local_dic，exec便不会访问全局作用域中的变量,如果global_dic没有包含全部的所需的变量则会报错。 1234567a=10b=20c=20global_dic = &#123;'a':6,'b':8&#125;local_dic = &#123;'b':9,'c':10&#125;exec ("global a;print(a,b,c);",global_dic,local_dic)# 6 9 10 local_dic=&gt; global_dic evaleval通常用来执行一个字符串表达式，并返回表达式的值。注意的是，eval的参数不能是函数字符串等，否则会报错。而exec可以接受一个函数字符串作为参数并运行它 eval(expression[, globals[, locals]]) 有三个参数，表达式字符串，globals变量作用域，locals变量作用域。 其中第二个和第三个参数是可选的。 如果忽略后面两个参数，则eval在当前作用域执行。 123&gt;&gt;&gt; a=1&gt;&gt;&gt; eval("a+1")2 注意由于有时exec和eval可能会改变全局变量的值，所以在遇到时应该首先考虑其它使用方式。]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解python装饰器]]></title>
    <url>%2F2018%2F06%2F26%2Fpython%2F2.python%E6%A0%87%E5%87%86%E5%BA%93%2F%E7%90%86%E8%A7%A3python%E8%A3%85%E9%A5%B0%E5%99%A8%2F</url>
    <content type="text"><![CDATA[讲 Python 装饰器前，我想先举个例子，跟装饰器这个话题可能很贴切。 每个人都有的内裤主要功能是用来遮羞，但是到了冬天它没法为我们防风御寒，咋办？我们想到的一个办法就是把内裤改造一下，让它变得更厚更长，这样一来，它不仅有遮羞功能，还能提供保暖，不过有个问题，这个内裤被我们改造成了长裤后，虽然还有遮羞功能，但本质上它不再是一条真正的内裤了。于是聪明的人们发明长裤，在不影响内裤的前提下，直接把长裤套在了内裤外面，这样内裤还是内裤，有了长裤后宝宝再也不冷了。装饰器就像我们这里说的长裤，在不影响内裤作用的前提下，给我们的身子提供了保暖的功效。 谈装饰器前，还要先要明白一件事，Python 中的函数和 Java、C++不太一样，Python 中的函数可以像普通变量一样当做参数传递给另外一个函数，例如： 1234567def foo(): print("foo")def bar(func): func()bar(foo) 装饰器的用途正式回到我们的主题。装饰器本质上是一个 Python 函数或类，它可以让其他函数或类在不需要做任何代码修改的前提下增加额外功能，装饰器的返回值也是一个函数/类对象。它经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景，装饰器是解决这类问题的绝佳设计。有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码到装饰器中并继续重用。概括的讲，装饰器的作用就是为已经存在的对象添加额外的功能。 先来看一个简单例子，虽然实际代码可能比这复杂很多： 12def foo(): print('i am foo') 现在有一个新的需求，希望可以记录下函数的执行日志，于是在代码中添加日志代码： 123def foo(): print('i am foo') logging.info("foo is running") 如果函数 bar()、bar2() 也有类似的需求，怎么做？再写一个 logging 在 bar 函数里？这样就造成大量雷同的代码，为了减少重复写代码，我们可以这样做，重新定义一个新的函数：专门处理日志 ，日志处理完之后再执行真正的业务代码 12345678def use_logging(func): logging.warn("%s is running" % func.__name__) func()def foo(): print('i am foo')use_logging(foo) 这样做逻辑上是没问题的，功能是实现了，但是我们调用的时候不再是调用真正的业务逻辑 foo 函数，而是换成了 use_logging 函数，这就破坏了原有的代码结构， 现在我们不得不每次都要把原来的那个 foo 函数作为参数传递给 use_logging 函数，那么有没有更好的方式的呢？当然有，答案就是装饰器。 123456789101112def use_logging(func): def wrapper(): logging.warn("%s is running" % func.__name__) return func() # 把 foo 当做参数传递进来时，执行func()就相当于执行foo() return wrapperdef foo(): print('i am foo')foo = use_logging(foo) # 因为装饰器 use_logging(foo) 返回的时函数对象 wrapper，这条语句相当于 foo = wrapperfoo() # 执行foo()就相当于执行 wrapper() use_logging 就是一个装饰器，它一个普通的函数，它把执行真正业务逻辑的函数 func 包裹在其中，看起来像 foo 被 use_logging 装饰了一样，use_logging 返回的也是一个函数，这个函数的名字叫 wrapper。在这个例子中，函数进入和退出时 ，被称为一个横切面，这种编程方式被称为面向切面的编程。 @ 语法糖如果你接触 Python 有一段时间了的话，想必你对 @ 符号一定不陌生了，没错 @ 符号就是装饰器的语法糖，它放在函数开始定义的地方，这样就可以省略最后一步再次赋值的操作。 123456789101112def use_logging(func): def wrapper(): logging.warn("%s is running" % func.__name__) return func() return wrapper@use_loggingdef foo(): print("i am foo")foo() 如上所示，有了 @ ，我们就可以省去foo = use_logging(foo)这一句了，直接调用 foo() 即可得到想要的结果。你们看到了没有，foo() 函数不需要做任何修改，只需在定义的地方加上装饰器，调用的时候还是和以前一样，如果我们有其他的类似函数，我们可以继续调用装饰器来修饰函数，而不用重复修改函数或者增加新的封装。这样，我们就提高了程序的可重复利用性，并增加了程序的可读性。 装饰器在 Python 使用如此方便都要归因于 Python 的函数能像普通的对象一样能作为参数传递给其他函数，可以被赋值给其他变量，可以作为返回值，可以被定义在另外一个函数内。 参考文档理解 Python 装饰器就看这一篇]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[js实现通过xpath选择元素]]></title>
    <url>%2F2018%2F06%2F25%2F%E5%89%8D%E7%AB%AF%2Fjs%E5%AE%9E%E7%8E%B0%E9%80%9A%E8%BF%87xpath%E9%80%89%E6%8B%A9%E5%85%83%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[使用js实现xpath选择元素的功能12345678910function query_by_xpath(STR_XPATH) &#123; var xresult = document.evaluate(STR_XPATH, document, null, XPathResult.ANY_TYPE, null); var xnodes = []; var xres; while (xres = xresult.iterateNext()) &#123; xnodes.push(xres); &#125; return xnodes;&#125; 通过chrome快速复制元素xpath使用chrome的开发者模式可以快速复制元素的xpath，方便开发时调用。如图：]]></content>
      <tags>
        <tag>web前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[format格式化字符串方法]]></title>
    <url>%2F2018%2F06%2F22%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fformat%E6%A0%BC%E5%BC%8F%E5%8C%96%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[使用说明base_str.format(new_word) 要比 %字符串操作符的功能更为强大。 在字符串中使用{NUM}进行占位,0表示第一个参数,1表示第二个参数。使用:, 指定代表元素需要的操作, 如{0:.3}小数点三位, {1:8}占8个字符空间等; :后可以添加特定的字母, 表示特定的输出格式: 操作 说明 {0:b} 二进制. 将数字以2为基数进行输出. {0:c} 字符. 在打印之前将整数转换成对应的Unicode字符串. {0:d} 十进制整数. 将数字以10为基数进行输出. {0:o} 八进制. 将数字以8为基数进行输出. {0:x} 十六进制. 将数字以16为基数进行输出, 9以上的位数用小写字母. {0:e} 幂符号. 用科学计数法打印数字, 用e表示幂. {0:g} 一般格式. 将数值以fixed-point格式输出. 当数值特别大的时候, 用幂形式打印. {0:n} 数字. 当值为整数时和d相同, 值为浮点数时和g相同. 不同的是它会根据区域设置插入数字分隔符. {0:%} 百分数. 将数值乘以100然后以fixed-point(‘f’)格式打印, 值后面会有一个百分号. {0:0&gt;10} 字符串位数为10,靠右对齐，左侧空位以0补全 {0:0&lt;10} 字符串位数为10,靠左对齐，左侧空位以0补全 {0:0^10} 字符串位数为10,居中对齐，左侧空位以0补全 format函数可以使用别名的方式替换{NUM}，使代码更易于阅读 12&gt;&gt;&gt; '&#123;first&#125; is as &#123;second&#125;.'.format(first='Caroline', second='Wendy') #别名替换'Caroline is as Wendy.' ‘’ 一些示例：1234567891011age = 25name = 'Caroline'print('&#123;0&#125; is &#123;1&#125; years old. '.format(name, age)) #输出参数 Caroline is 25 years old.print('&#123;0&#125; is a girl. '.format(name)) # Caroline is a girl.print('&#123;0:.3&#125; is a decimal. '.format(1/3)) #小数点后三位 0.333 is a decimal.print('&#123;0:_^11&#125; is a 11 length.'.format(name)) #使用_补齐空位 _Caroline__ is a 11 length.print('&#123;0:.2%&#125; is a percent.'.format(0.5)) # '50.00% is a percent.'print('&#123;first&#125; is as &#123;second&#125;. '.format(first=name, second='Wendy')) #别名替换print('My name is &#123;0.name&#125;'.format(open('out.txt', 'w'))) #调用方法 My name is out.txtprint('My name is &#123;0:8&#125;.'.format('Fred')) #指定宽度 My name is Fred . 转义可以使用{}对{}进行转义 12&gt;&gt;&gt;'&#123;&#125; world&#123;&#123;!&#125;&#125;'.format('hello')'hello world&#123;!&#125;' 然而，在大量存在{}时，比如js函数库的字符替换，这种方法比较繁琐，还要回退到%字符操作符的方式。]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不可不知的python模块:collections]]></title>
    <url>%2F2018%2F06%2F21%2Fpython%2F%E9%A2%9D%E5%A4%96%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8Bcollections%2F</url>
    <content type="text"><![CDATA[基本介绍我们都知道，Python拥有一些内置的数据类型，比如str, int, list, tuple, dict等， collections模块在这些内置数据类型的基础上，提供了几个额外的数据类型： namedtuple(): 生成可以使用名字来访问元素内容的tuple子类 deque: 双端队列，可以快速的从另外一侧追加和推出对象 Counter: 计数器，主要用来计数 OrderedDict: 有序字典 defaultdict: 带有默认值的字典 namedtuple()namedtuple主要用来产生可以使用名称来访问元素的数据对象，通常用来增强代码的可读性， 在访问一些tuple类型的数据时尤其好用。 123456789101112from collections import namedtuplewebsite_info_tuple = ('Sohu', 'http://www.google.com/', '张朝阳')# 生成namedtuple实例Website = namedtuple('Website', ['name', 'url', 'founder'])# 将tuple转化为符合Website定义的namedtuplewebsite = Website._make(website_info_tuple)&gt;&gt;&gt; websiteWebsite(name='Sohu', url='http://www.google.com/', founder='张朝阳')&gt;&gt;&gt; website.name'Sohu' deque()构造函数deque其实是 double-ended queue 的缩写，翻译过来就是双端队列 使用构造函数deque()可以将字符串或数组转换为deque实例 12345from collections import dequenew_deque = deque([1, 2, 3, 4, 5])deque([1, 2, 3, 4, 5])new_deque = deque("12345")deque([1, 2, 3, 4, 5]) 使用 deque(maxlen=N) 构造函数会新建一个固定大小的队列,当新的元素加入并且这个队列已满的时候， 最老的元素会自动被移除掉。 123456789&gt;&gt;&gt; q = deque(maxlen=3)&gt;&gt;&gt; q.append(1)&gt;&gt;&gt; q.append(2)&gt;&gt;&gt; q.append(3)&gt;&gt;&gt; qdeque([1, 2, 3], maxlen=3)&gt;&gt;&gt; q.append(4)&gt;&gt;&gt; qdeque([2, 3, 4], maxlen=3) 如果未传入参数maxlen，则列队长度不受限制 实例方法 从队列头部快速增加和取出对象: deque_obj.popleft(), deque_obj.appendleft(value) 从末尾快速增加和取出对象: deque_obj.pop(), deque_obj.append(value) 将队尾的对象取出放到队列开头 deque_obj.rotate() 是值得注意的是，list对象从队列头部快速增加和取出对象的时间复杂度是 O(n) ，也就是说随着元素数量的增加耗时呈 线性上升。而使用deque对象则是 O(1) 的复杂度，所以当你的代码有这样的需求的时候， 一定要记得使用deque。 Counter计数器是一个非常常用的功能需求，collections也贴心的为你提供了这个功能。 1234567891011121314"""下面这个例子就是使用Counter模块统计一段句子里面所有字符出现次数"""from collections import Counters = '''A Counter is a dict subclass for counting hashable objects. It is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. The Counter class is similar to bags or multisets in other languages.'''.lower()c = Counter(s)# 获取出现频率最高的5个字符print c.most_common(5)# Result:[(' ', 54), ('e', 32), ('s', 25), ('a', 24), ('t', 24)] OrderedDict在Python中，dict这个数据结构由于hash的特性，是无序的，这在有的时候会给我们带来一些麻烦， 幸运的是，collections模块为我们提供了OrderedDict，当你要获得一个有序的字典对象时，用它就对了。 12345678910111213141516171819202122232425from collections import OrderedDictitems = ( ('A', 1),('B', 2),('C', 3) )regular_dict = dict(items)ordered_dict = OrderedDict(items)print ('Regular Dict:')for k, v in regular_dict.items(): print (k, v)print ('Ordered Dict:')for k, v in ordered_dict.items(): print (k, v)# Result:Regular Dict:A 1C 3B 2Ordered Dict:A 1B 2C 3 defaultdict我们都知道，在使用Python原生的数据结构dict的时候，如果用 d[key] 这样的方式访问， 当指定的key不存在时，是会抛出KeyError异常的。 但是，如果使用defaultdict，只要你传入一个默认的工厂方法(如果不传入仍会报错)，那么请求一个不存在的key时， 便会调用这个工厂方法使用其结果来作为这个key的默认值。 他的意义在于，可以利用工厂方法快速构建字典 12345678910from collections import defaultdict&gt;&gt;&gt; default_dict = defaultdict(list)&gt;&gt;&gt; default_dict['name'].append('lee')&gt;&gt;&gt; default_dictdefaultdict(&lt;class 'list'&gt;, &#123;'name': ['lee']&#125;)&gt;&gt;&gt; base_dict = &#123;&#125;&gt;&gt;&gt; base_dict['name'].append('lee')KeyError: 'name']]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python3-cookbook学习笔记]]></title>
    <url>%2F2018%2F06%2F21%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython3-cookbook%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[第一章:数据结构和算法1.4 查找最大或最小的 N 个元素（列表） 怎样从一个集合中获得最大或者最小的 N 个元素列表？heapq 模块有两个函数：nlargest() 和 nsmallest() 列表元素是数字：heapq.nlargest(3, list) 返回list中包含最大的三个元素的列表 列表元素是字典：heapq.nsmallest(3, portfolio, key=lambda s: s[&#39;price&#39;]) 返回所有字典的以price key键对应的值的最小值。 1.6 字典中的键映射多个值 一个字典就是一个键对应一个单值的映射。如果你想要一个键映射多个值，那么你就需要将这多个值放到另外的容器中， 比如列表或者集合里面 你可以很方便的使用 collections 模块中的 defaultdict 来构造这样的字典。比如dic = defaultdict(set) dic[&#39;a&#39;].add(1) 自己实现需要使用if判断key是否存在，不够简洁。 1.7 字典排序 为了能控制一个字典中元素的顺序，你可以使用collections 模块中的 OrderedDict 类。 在迭代操作的时候它会保持元素被插入(创建)时的顺序 一个 OrderedDict 的大小是一个普通字典的两倍,数据量过大时需要仔细衡量 1.8 字典的运算 为了对字典值执行计算操作(得到值的最大值等)，通常需要使用 zip() 函数先将键和值反转过来 min_price = min(zip(prices_dict.values(), prices_dict.keys())) zip() 函数创建的是一个只能访问一次的迭代器，赋值给变量后，第二次访问会报错。 1.17 从字典中提取子集 使用字典推导的方式创建新字典，运行速度是更快的;通过if后面的条件不同，可以得到不同的新字典 通过value获得新字典：dict1 = {key: value for key, value in base_dict.items() if value &gt; 200} 通过key值获得新字典：dict2 = {key: value for key, value in base_dict.items() if key in [&quot;name&quot;,&quot;price&quot;]} 1.18 映射名称到序列元素 namedtuple的意义在于，直接通过下标index访问list中的数据，意义不清晰。namedtuple为常规的list增加了key值访问的方式。可以通过namedtuple.key和namedtuple()[index]两种方式读取nametuple中的值 collections.namedtuple()工厂函数生成的实例可以将tuple转换为nametuple，并通过名称去访问元组元素，为代码增加可读性。 namedtuple()跟元组类型是可交换的，支持所有的普通元组操作,和字典的区别主要在于命名元组是不可更改的 基本使用如下，namedtuple()是一个构造函数，其中，第一个参数是类的名称，第二个参数是key值列表；然后在新类中传入value值列表，即形成了新的nametuple 123from collections import namedtupleBook = namedtuple('Book',['name','author','price'])book1 = Book('空之境界','奈须蘑菇','100') 1.19 转换并同时计算数据 生成器表达式 x * x for x in nums =&gt; result（for x in list）if ()=&gt; 对列表中的每一个对象进行同样的操作，后面可以加if作为条件。返回result，形成新的列表 生成器表达式返回的是一个&lt;generator object&gt; ,你可以使用[x * x for x in nums],形成临时列表再去使用sum()调用它，或者直接使用sum(x * x for x in nums),得出结果。后一种方案是更省内存的。 1.20 合并多个字典或映射 collections模块中的 ChainMap 将字典从逻辑上合并为一个单一的映射后执行某些操作，如果key值重复，任何操作都将指向第一个字典的key 当使用ChainMap映射的原字典更新了内容，ChainMap可以获取到最新的值。而update方法不可以。 第二章:字符串和文本2.1 使用多个界定符分割字符串 string 对象的 split() 方法只适应于非常简单的字符串分割情形， 它并不允许有多个分隔符或者是分隔符周围不确定的空格。 当你需要更加灵活的切割字符串的时候，最好使用 re.split() 方法：re.split(r&#39;[;,\s]\s*&#39;, line) 当partern使用分组时，分组里的内容也会被传入数组中：fields = re.split(r&#39;(;|,|\s)\s*&#39;, line) 2.2 字符串开头或结尾匹配 检查字符串开头或结尾的一个简单方法是使用 str.startswith() 或者是 str.endswith() 方法,返回True或False 多种匹配可能，只需要将所有的匹配项放入到一个tuple中去(必须时tuple，list类型不可以，需要转换)， 然后传给 startswith() 或者 endswith() 方法: strline.startswith((&#39;.py&#39;,&#39;.json&#39;)) 得到固定类型的文件名列表：[name for name in filenames if name.endswith((&#39;.c&#39;, &#39;.h&#39;)) ] 判断是否有某种类型的文件: any(name.endswith(&#39;.py&#39;) for name in filenames) 2.4 字符串匹配和搜索 如果你想匹配的是字面字符串，那么你通常只需要调用基本字符串方法就行， 比如 str.find() , str.endswith() , str.startswith() 或者类似的方法 当写正则式字符串的时候，相对普遍的做法是使用原始字符串比如 r&#39;(\d+)/(\d+)/(\d+)&#39;。 这种字符串将不去解析反斜杠，如果不这样做的话，你必须使用两个反斜杠，类似 &#39;(\\d+)/(\\d+)/(\\d+)&#39; 。 在正则中使用分组()的作用是，可以方便的调用group()方法得到相应的分组内容 2.5 字符串搜索和替换 对于简单的字面模式，直接使用字符串方法 str.replace(base_word,new_word) 方法即可，它会将所有的base_word替换为new_word 对于复杂的替换可以使用re.complain(pattern).sub(new_word,base_str) 2.6 字符串忽略大小写的搜索替换 为了在文本操作时忽略大小写，你需要在使用 re 模块的时候给这些操作提供 re.IGNORECASE 标志参数。 默认情况下，正则方法和字符串的搜索方法都是大小写严格匹配的 2.7 最短匹配模式 贪婪模式: re.compile(r&#39;&quot;(.*)&quot;&#39;) 这个规则匹配的是第一个&quot;到最后一个&quot; 非贪婪模式:str_pat = re.compile(r&#39;&quot;(.*?)&quot;&#39;) 这个规则匹配的是第一个&quot;和接下来遇到的第一个 &quot; 2.8 多行匹配模式 .不能匹配换行符 ?:开头的分组为不捕获分组r&#39;(?:.|\n)&#39;，不占索引位置，仅仅用来做匹配，而不能通过单独捕获或group方法得到。 通过定义正则r&#39;(?:.|\n)&#39;增加了匹配多行的功能，可以将这个分组当作.来使用 标志参数re.DOTALL可以让正则表达式中的点.匹配包括换行符在内的任意字符。 2.9 将Unicode文本标准化 同一个字符可能有不同的编码方式，比如拼音，它可以是由整体组成或者是有字母+音标的方式组成，虽然显示上一致，但是两个字符是不相等的。 可以使用unicodedata模块先将文本标准化(转换为你希望的格式),例如t1 = unicodedata.normalize(&#39;NFC&#39;, base_str) normalize() 第一个参数指定字符串标准化的方式。 NFC表示字符应该是整体组成(比如可能的话就使用单一编码)，而NFD表示字符应该分解为多个组合字符表示 unicodedata.combining(code)可以判断字符code是否为和音字符，结合分解方式的标准化可以用于清洗数据。 2.11 删除字符串中不需要的字符 strip() 方法能用于删除开始或结尾的字符。 lstrip() 和 rstrip() 分别从左和从右执行删除操作。 strip() 方法默认删除包括换行符在内的空白字符，但是可以传入参数来删除其他字符，strip(&#39;---&#39;) 2.13 字符串对齐与格式化字符串 对于基本的字符串对齐操作，可以使用字符串的 ljust() , rjust() 和 center() 方法。 &#39;hello world&#39;.center(20,&#39;*&#39;) str.format()方法比格式化文本的 % 操作符更强大,详见此文档format格式化字符串方法 2.14 合并拼接字符串 如果你想要合并的字符串是在一个序列或者 iterable 中，那么最快的方式就是使用join()方法 +合并字符串会创造临时变量，造成性能问题 2.15 字符串中插入变量 base.format(name=&#39;Guido&#39;, age=37)是目前最好的插入变量解决办法 如果希望能将变量域中的参数直接传入字符串中，使用base_str.format_map(vars())即可 以指定列宽格式化字符串(用于控制台) 可以使用 textwrap 模块来格式化字符串的输出。textwrap.fill(base_str, 70)每行的输出字符宽为70 可以结合 os.get_terminal_size() 方法来获取终端的大小尺寸 在字符串中处理html和xml 你想将HTML或者XML实体如 &amp;entity; 或 &amp;#code; 替换为对应的文本或者转换文本中特定的字符(比如&lt;, &gt;, 或 &amp;)，可以引入html模块 html.escape(base_str)会将&lt; 转换为 &amp;lt; 字符串令牌解析scanner.match是逐行匹配，匹配完第一行再匹配第二行。 使用示例 命名捕获组的正则表达式来定义所有可能的令牌，包括空格 123456789import reNAME = r'(?P&lt;NAME&gt;[a-zA-Z_][a-zA-Z_0-9]*)'NUM = r'(?P&lt;NUM&gt;\d+)'PLUS = r'(?P&lt;PLUS&gt;\+)'TIMES = r'(?P&lt;TIMES&gt;\*)'EQ = r'(?P&lt;EQ&gt;=)'WS = r'(?P&lt;WS&gt;\s+)'pat = re.compile('|'.join([NAME, NUM, PLUS, TIMES, EQ, WS])) 实现一个生成器 123456from collections import namedtupledef generate_tokens(pat, text): Token = namedtuple('Token', ['type', 'value']) scanner = pat.scanner(text) for m in iter(scanner.match, None): yield Token(m.lastgroup, m.group()) 调用并处理token 123456789101112131415161718text = '''foo = 42key = 100'''for tok in generate_tokens(master_pat, text): print(tok)# Token(type='NAME', value='foo')# Token(type='WS', value=' ')# Token(type='EQ', value='=')# Token(type='WS', value=' ')# Token(type='NUM', value='42')# Token(type='WS', value='\n')# Token(type='NAME', value='key')# Token(type='WS', value=' ')# Token(type='EQ', value='=')# Token(type='WS', value=' ')# Token(type='NUM', value='100')# Token(type='WS', value='\n') 要点: 如果一个re匹配模式恰好是另一个更长模式的子字符串，那么你需要确定长模式写在前面。(‘|’.join的顺序) scanner.match从字符串开头扫描到结尾，如果遇到不满足re匹配模式的地方立刻停止扫描，因此，需要考虑所有的情况，比如空白字符。 实现一个简单的递归下降分析器第三章 数字日期和时间数字的四舍五入执行精确的浮点数运算数字的格式化输出二八十六进制整数字节到大证书的打包与解包复数的数学运算无穷大与NaN分数运算大型数组运算矩阵与线性代数运算随机选择random 模块有大量的函数用来产生随机数和随机选择元素。 比如，要想从一个序列中随机的抽取一个元素，可以使用 random.choice() ： 123&gt;&gt;&gt; import random&gt;&gt;&gt; values = [1, 2, 3, 4, 5, 6]&gt;&gt;&gt; random.choice(values) 为了提取出N个不同元素的样本用来做进一步的操作，可以使用 random.sample() 12345&gt;&gt;&gt; random.sample(values, 2)[6, 2]&gt;&gt;&gt; random.sample(values, 2)[4, 3]&gt;&gt;&gt; random.sample(values, 3) 如果你仅仅只是想打乱序列中元素的顺序，可以使用 random.shuffle() ： 123&gt;&gt;&gt; random.shuffle(values)&gt;&gt;&gt; values[2, 4, 6, 5, 3, 1] 生成随机整数，请使用 random.randint() ： 123456&gt;&gt;&gt; random.randint(0,10)2&gt;&gt;&gt; random.randint(0,10)5&gt;&gt;&gt; random.randint(0,10)0 为了生成0到1范围内均匀分布的浮点数，使用 random.random()： 1234&gt;&gt;&gt; random.random()0.9406677561675867&gt;&gt;&gt; random.random()0.133129581343897 如果要获取N位随机位(二进制)的整数，使用 random.getrandbits() ： 12&gt;&gt;&gt; random.getrandbits(200)335837000776573622800628485064121869519521710558559406913275 计算最后一个周五的日期计算当前月份的日期范围字符串转换为日期结合时区的日期操作第四章 迭代器与生成器4.1手动遍历迭代器 对于一个genrator object,可以使用next(genertor)得到每一个结果，当越界后，程序会报StopIteration错误 我们可以使用next(genertor,msg)，第二个参数表示越界后返回的值来标记结尾。当传入第二个参数后，越界就不会报错了。 4.2 代理迭代4.3 使用生成器创建新的迭代模式 一个函数中需要有一个 yield 语句即可将其转换为一个生成器。 yield和return类似，每次被next调用时，会返回yield后面的变量值 我们通常使用for循环来调用一个迭代器，此时不用考虑越界等细节 4.4 实现迭代器协议4.5 反向迭代一个序列 可以通过reversed(list)来得到list的反向迭代器 4.6 带有外部状态的生成器函数 如果生成器函数需要跟你的程序其他部分打交道的话，可以定义类，并把迭代器放到 __iter__() 方法中 如果该类生成的实例不使用for进行迭代的化，需要先调用iter(object)，才能进行迭代操作，否则会报错。 4.7 迭代器切片 函数 itertools.islice(&lt;generator&gt;,start_num,end_num) 正好适用于在迭代器和生成器上做切片操作 itertools.islice()会消耗掉，因此只能使用一次 当end_num的值为None时，表示匹配的最后，当start_num的值为None时,表示从第一个开始匹配 4.8 跳过可迭代对象的开始部分 itertools模块的dropwhile可以跳过符合规则的内容：for line in dropwhile(lambda line: line.startswith(&#39;#&#39;), f): dropwhile它会返回一个迭代器对象，丢弃原有序列中直到函数返回Flase之前的所有元素，然后返回后面所有元素。 lambda作用是构建匿名函数，其中，:前的表示传入的参数，后面的表示return的结构 4.9 排列组合的迭代 itertools.permutations(items,num) 生成列表items内元素的所有排列组合(考虑顺序) itertools.combinations(items,num) 可得到输入集合中元素的所有的组合,不考虑顺序(1，2)和(2，1)算一个结果 更多的工具，可以在itertools中寻找 第五章:文件与IO5.1 读写文本数据123456789101112131415# Read the entire file as a single stringwith open('somefile.txt', 'a+',encoding = 'utf-8') as f: data = f.read()# Iterate over the lines of the filewith open('somefile.txt', 'a+ ',encoding = 'utf-8') as f: for line in f: # process line ...# 复杂情况可以使用readline()逐行读取；while line：保证到最后一行时会退出读取 '\n != EOF'with open('somefile.txt', 'a+',encoding = 'utf-8') as f: line = fh.readline() while line: print(line.strip()) line = f.readline() 换行符的识别问题(自动处理) 在Unix和Windows中是不一样的(分别是 \n 和 \r\n )。 在读取文本的时候，Python可以识别所有的普通换行符并将其转换为单个 \n 字符。 在输出时会将换行符 \n 转换为系统默认的换行符。 二进制模式的时候，python不会自动转化 open(file, mode=&#39;r&#39;, buffering=-1, encoding=None, errors=None, newline=None, closefd=True) buffering的可取值有0，1， &gt;1三个，0代表buffer关闭（只适用于二进制模式），1代表line buffer（只适用于文本模式），&gt;1表示初始化的buffer大小； encoding表示的是返回的数据采用何种编码，一般采用utf8或者gbk； errors的取值一般有strict，ignore，当取strict的时候，字符编码出现问题的时候，会报错，当取ignore的时候，编码出现问题，程序会忽略而过，继续执行下面的程序。 newline可以取的值有None, \n, \r, ‘’, ‘\r\n’ ，用于区分换行符，但是这个参数只对文本模式有效； closefd的取值，是与传入的文件参数有关，默认情况下为True，传入的file参数为文件的文件名，取值为False的时候，file只能是文件描述符，什么是文件描述符，就是一个非负整数，在Unix内核的系统中，打开一个文件，便会返回一个文件描述符。 open参数mode mode参数决定了file的行为,如果在r模式下使用file.write(lines)就会报错 r、w、a为打开文件的基本模式，对应着只读、只写、追加模式； b、t、+、U这四个字符，与以上的文件打开模式组合使用，二进制模式，文本模式，读写模式、通用换行符，根据实际情况组合使用、 open常见的mode取值组合 r或rt 默认模式，文本模式读 rb 二进制文件 w或wt 文本模式写，打开前文件存储被清空 wb 二进制写，文件存储同样被清空 a 追加模式，只能写在文件末尾 a+ 可读写模式，写只能写在文件末尾 w+ 可读写，与a+的区别是要清空文件内容 r+ 可读写，与a+的区别是可以写到文件任何位置 对照列表： 模式 描述 r 以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。 rb 以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。这是默认模式。 r+ 打开一个文件用于读写。文件指针将会放在文件的开头。 rb+ 以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。 w 打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb 以二进制格式打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 w+ 打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb+ 以二进制格式打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 a 打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 ab 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 a+ 打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。 ab+ 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。 5.2 打印输出至文件中 print接受关键字参数file，可以将输出重定向到一个文件中去,此时print的作用和f.write()相同 文件必须是文本文件，不能是二进制文件，否则会报错 重定向后print的内容不会在控制台输出 12with open('data.txt', 'w') as f: print("hello world!",file = f) 5.3 使用其它分隔符或行终止符打印 print() 函数支持使用 sep 和 end 关键字参数，sep表示每个参数之间的分割符,end表示打印的结尾：print(&#39;ACME&#39;, 50, 91.5, sep=&#39;,&#39;, end=&#39;!!\n&#39;) 5.4 读写字节数据 读取或写入二进制数据需要使用模式为 rb 或 wb 的 open() 函数：with open(&#39;somefile.bin&#39;, &#39;rb&#39;) as f: 二进制数据索引和迭代动作返回的是字节的值而不是字节字符串 123&gt;&gt;&gt; b = b'Hello World'&gt;&gt;&gt; b[0]72 如果你想从二进制模式的文件中读取或写入文本数据，必须确保要进行解码(decode)和编码(encode)操作。 1234567with open('somefile.bin', 'rb') as f: data = f.read(16) # 二进制格式 text = data.decode('utf-8') # 文本格式with open('somefile.bin', 'wb') as f: text = 'Hello World' # 文本格式 f.write(text.encode('utf-8')) # 二进制格式 5.5 文件不存在才能写入 可以在 open() 函数中使用 x 模式来代替 w 模式的方法来解决这个问题。文件存在时程序会报错：with open(&#39;somefile&#39;, &#39;xt&#39;) as f: 5.6 字符串的I/O操作 使用 io.StringIO() 和 io.BytesIO() 类来创建类文件对象操作字符串数据 5.11 文件路径名的操作 详见python操作文件和目录 5.12 测试文件是否存在(os.path) 测试一个文件是否存在,返回布尔值 os.path.exists(&#39;/etc/passwd&#39;) 测试这个文件时什么类型的,返回布尔值 os.path.isfile(&#39;/etc/passwd&#39;) os.path.isdir(&#39;/etc/passwd&#39;) os.path.islink(&#39;/usr/local/bin/python3&#39;) os.path.realpath(&#39;/usr/local/bin/python3&#39;) 获取元数据(比如文件大小或者是修改日期) os.path.getsize(&#39;/etc/passwd&#39;) os.path.getmtime(&#39;/etc/passwd&#39;) 5.13 获取文件夹中的文件列表 得到某种类型的文件 endswith进行匹配 pyfiles = [name for name in os.listdir(&#39;somedir&#39;) if name.endswith(&#39;.py&#39;)] 引入glob模块进行匹配: pyfiles = glob.glob(&#39;somedir/*.py&#39;) 5.14 忽略文件名编码123456789with open('jalape\xf1o.txt', 'w') as f: f.write('Spicy!')import osos.listdir('.')['jalapeño.txt']os.listdir(b'.')[b'jalapen\xcc\x83o.txt'] 5.19 创建临时文件和文件夹 tempfile 模块中有很多的函数可以完成这任务。 为了创建一个匿名的临时文件，可以使用 tempfile.TemporaryFile 1234567891011from tempfile import TemporaryFilewith TemporaryFile('w+t') as f: # Read/write to the file f.write('Hello World\n') f.write('Testing\n') # Seek back to beginning and read the data f.seek(0) data = f.read()# Temporary file is destroyed 5.20 与串行端口的数据通信 你想通过串行端口读写数据，典型场景就是和一些硬件设备打交道(比如一个机器人或传感器)。 但对于串行通信最好的选择是使用 pySerial包(第三方) 安装serial1pip install pyserial 列出端口1python -m serial.tools.list_ports将打印可用端口列表。也可以添加regexp作为第一个参数，列表将只包含匹配的条目。 或 1234567891011import serial.tools.list_portsplist = list(serial.tools.list_ports.comports())if len(plist) &lt;= 0: print("没有发现端口!")else: plist_0 = list(plist[0]) serialName = plist_0[0] serialFd = serial.Serial(serialName, 9600, timeout=60) print("可用端口名&gt;&gt;&gt;", serialFd.name) 注意 枚举可能不适用于所有操作系统。它可能不完整，列出不可用的端口或可能缺少端口的详细描述。 初始化Serial,其中支持的参数如下：1ser = serial.Serial(port = None，baudrate = 9600，bytesize = EIGHTBITS，parity = PARITY_NONE，stopbits = STOPBITS_ONE，timeout = None，xonxoff = False，rtscts = False，write_timeout = None，dsrdtr = False，inter_byte_timeout = None) 参数： port - 设备名称或无。 baudrate(int) - 波特率，如9600或115200等。 bytesize - 数据位数。可能的值： FIVEBITS，SIXBITS，SEVENBITS， EIGHBITS parity - 启用奇偶校验。可能的值： PARITY_NONE，PARITY_EVEN，PARITY_ODD PARITY_MARK，PARITY_SPACE stopbits - 停止位数。可能的值： STOPBITS_ONE，STOPBITS_ONE_POINT_FIVE， STOPBITS_TWO timeout(float) - 设置读取超时值。 xonxoff(bool) - 启用软件流控制。 rtscts(bool) - 启用硬件(RTS / CTS)流量控制。 dsrdtr(bool) - 启用硬件(DSR / DTR)流控制。 write_timeout(float) - 设置写超时值。 inter_byte_timeout(float) - 字符间超时，无禁用(默认）。 异常: ValueError - 当参数超出范围时将引发，例如波特率，数据位。 SerialException - 如果找不到设备或无法配置设备。 实例方法：ser.read(size=1)Parameters: size – 需要读取的字节数.Returns: 从端口返回的字节Return type: byte 从串行端口读取大小字节。如果设置了超时，则可以按要求返回较少的字符。在没有超时的情况下，它将被阻塞，直到读取请求的字节数为止。 ser.readline()Returns: 从端口返回的字节Return type: byte 从串口读取一行字节。如果设置了超时，则可以按要求返回较少的字符。在没有超时的情况下，它将被阻塞，直到读取请求的字节数为止。 write(data)Parameters: data – 要发送的数据.Returns: 写入的字节数Return type: int 向端口写入字节数据,Unicode字符串必须被编码,例如ser.write(&#39;hello&#39;.encode(&#39;utf-8&#39;)) 5.21 序列化Python对象 所谓序列化是指Python对象(字典，数组)以特定格式转换为一个字节流，以便将它保存到一个文件用于日后读写。 对于序列化最普遍的做法就是使用pickle模块，它可以将对象等转换为字节编码储存在文档中，但只有python支持，因此不推荐 你最好使用更加标准的数据编码格式如XML，CSV或JSON来存储或是序列化数据 第六章:数字据编码与处理6.1 读写CSV数据 对于大多数的CSV格式的数据读写问题，都可以使用 csv 库，而无需自己处理分隔符以及其它细节。 csv的默认分隔符是’,’，可以使用delimiter参数进行指定 将这些数据读取为一个元组的序列: 123456import csvwith open('stocks.csv') as f: f_csv = csv.reader(f,delimiter=',') headers = next(f_csv) # 通常第一行是headers，通过next得到lsit for row in f_csv: # row is list 可以使用csv.DictReader(file)和csv.DictWriter(file, headerlist)方便的以字典的形式读取或写入 6.2 读写JSON数据 python对象与JSON字符串互相转换,可以使用json.dumps() 和 json.loads() 123import jsonjson_str = json.dumps(data)data = json.loads(json_str) 如果你要处理的是文件而不是字符串，你可以使用 json.dump(data,file) 和 json.load(file) 来编码和解码JSON数据 json.dump()在序列化的同时写入了文件。 等价于data = json.dumps(data) file.write(data) 1234567# Writing JSON datawith open('data.json', 'w') as f: json.dump(data, f)# Reading data backwith open('data.json', 'r') as f: data = json.load(f) 可以使用object_pairs_hook或object_hook参数对传入的json_str进行处理,从而得到所需的数据类型 1data = json.loads(json_str, object_pairs_hook=OrderedDict) 读写ymal格式的文件对于读取yaml文件，我们可以使用pyymal第三方模块，而无需自己编写语法解析规则 首先安装pyyaml 1pip install pyyaml 然后读取并解析yaml 1234import yamlwith open(file_path,'r',encoding = "utf-8") as file: dic = yaml.load(file).get(key) ###6.8 与关系型数据库的交互 你可以使用Python标准库中的 sqlite3 模块 不要使用Python字符串格式化操作符(如%)或者 .format() 方法来创建这样的字符串。否则很有可能遭受SQL注入攻击。 与sqlite3交互步骤 链接数据库,得到数据库实例 db = sqlite3.connect(&#39;database.db&#39;) 创建游标 cursor = db.cursor() 执行语句cursor.execute(&#39;create table portfolio (symbol text, shares integer, price real)&#39;) 提交语句db.commit() 6.10 编码解码Base64数据 Base64编码仅仅用于面向字节的数据比如字节字符串和字节数组。 编码为base64:base64.b64encode( data) 解码为二进制字节:base64.b64decode(base64data) 1234567# Some byte datas = b'hello'import base64# Encode as Base64a = base64.b64encode(s) # b'aGVsbG8='# Decode from Base64base64.b64decode(a) # b'hello' 6.13 数据的累加与统计操作 于任何涉及到统计、时间序列以及其他相关技术的数据分析问题，都可以考虑使用Pandas库 。 第七章:函数7.1 可接受任意数量参数的函数 为了能让一个函数接受任意数量的位置参数，可以使用一个以*开头的参数,这个参数是个tuple 为了接受任意数量的关键字参数，使用一个以**开头的参数,这个参数是个dict 一个*参数只能出现在函数定义中最后一个位置参数后面，而 **参数只能出现在最后一个参数。 有一点要注意的是，在*参数后面仍然可以定义其他参数。这种参数就是我们所说的强制关键字参数 7.2 只接受关键字参数的函数 将强制关键字参数放到某个*位置参数或者单个*后面就能达到强制使用关键字参数传递 如果你还希望某个函数能同时接受任意数量的位置参数和关键字参数，可以同时使用*和** 123def anyargs(*args, **kwargs): print(args) # A tuple print(kwargs) # A dict 7.3 给函数参数增加元信息 函数的注解方法def add(x:int, y:int) -&gt; int: 注解和注释类似,python解释器不会对这些注解添加任何的语义,他们仅用于提示作用 函数注解只存储在函数的 __annotations__ 属性中,add.__annotations__ 7.4 返回多个值的函数 return a,b,c返回的是一个元组 我们使用的是逗号来生成一个元组，而不是用括号 b = 1,2,3 =&gt; b == (1,2,3) 7.5 定义有默认参数的函数 测试默认参数None时不能使用:if not b:而要使用if b is None:以排除0,&#39;&#39;等 默认参数的值仅仅在函数定义的时候赋值一次,比如将变量作为参数传入进去，实际传入的是变量的值的拷贝 7.6 定义匿名或内联函数 lambda x, y: x + y lambda 定义一个匿名函数，:之前是函数的参数:之后是函数返回的值 12add = lambda x, y: x + yadd(2,3) lambda表达式中的x是一个自由变量， 在运行时绑定值，而不是定义时就绑定 如果需要在定义时确认值，只需给相应的参数提供默认值即可:[lambda x, n=n: x+n for n in range(5)] [lambda x, n=n: x+n for n in range(5)]解析： 生成器表达式：for n in range(5)生成包含5个匿名函数的列表列表 lambda x,n=n:x+n lambda x, n=n: x+n,由于n有默认值，所以在定义时确定n值，n值分别为0,1,2,3,4 列表结果为(相似)：[lambda x: x+0,lambda x: x+1,lambda x: x+2,lambda x: x+3,lambda x: x+4] 如果n没有默认值,则n=4,即迭代的最后一个值 7.8 减少可调用对象的参数个数 new_func = partial(func,*params) 给一个或多个参数设置固定的值，减少接下来被调用时的参数个数。 partial(func,*params)的意义是在调用其它函数库接受的回调函数时用来微调参数个数。 很多时候 partial() 能实现的效果，lambda表达式也能实现,但是稍显臃肿 list_obj.sort(key=partial(distance,pt)) 列表的 list_obj.sort() 方法 接受一个回调函数key = func()的返回值作为新的列表排序依据， 但是它只能接受一个单个参数的函数，参数是列表的每个子元素 7.9 将单方法的类转换为函数 通常是为了保存额外状态来给函数使用，详见7.10 7.10 带额外状态信息的回调函数 有三种方式可以在回调函数的内部保存变量值 创建一个类，使需要保存的变量在类内部传递。回调函数是有对象实例化的一个方法。 创建一个闭包，使需要保存的变量在函数内部传递。回调函数是这个函数return的闭包。需要为变量声明nonlocal 创建一个协程，使需要保存的变量在协程内部传递。回调函数是这个协程启动的send方法。 1234567891011121314151617181920212223242526272829303132333435363738394041def apply_async(func, args, *, callback): result = func(*args) callback(result)def add(x, y): return x + y# 创建一个类保存变量class ResultHandler: def __init__(self): self.sequence = 0 def handler(self, result): self.sequence += 1 print('[&#123;&#125;] Got: &#123;&#125;'.format(self.sequence, result))r = ResultHandler()apply_async(add, (2, 3), callback=r.handler)# 创建一个闭包保存变量def make_handler(): sequence = 0 def handler(result): nonlocal sequence sequence += 1 print('[&#123;&#125;] Got: &#123;&#125;'.format(sequence, result)) return handlerhandler = make_handler()apply_async(add, (2, 3), callback=handler)# 创建一个协程保存变量def make_handler(): sequence = 0 while True: result = yield sequence += 1 print('[&#123;&#125;] Got: &#123;&#125;'.format(sequence, result))handler = make_handler()next(handler) # Advance to the yieldapply_async(add, (2, 3), callback=handler.send) 协程解析 协程通过yield关键字实现,他实际上是个generator 对于协程,第一次必须运行一次next(generator)启动它(进入到while True中,这才是协程运行的部分)，直接使用send方法会报错 协程 param = yield result对于yield来说,=并不是赋值的意思。等号前的值generator通过send方法传入的参数，使其内部读取,yield后的值是其返回给外界的值 generator每次执行后，遇到yield就会中断执行，直到使用next(generator)或generator.send(param)再次调用它，才会继续执行。 第八章:类与对象8.1 改变对象的字符串显示 __repr__() 方法返回一个实例的代码表示形式，通常用来重新构造这个实例。 内置的 repr() 函数返回这个字符串，跟我们使用交互式解释器显示的值是一样的。 __str__() 方法将实例转换为一个字符串，使用 str() 或 print() 函数会输出这个字符串。 为了更方便的调试代码，我们可以自定义类的 __repr__() 和 __str__()方法 8.3 让对象支持上下文管理协议 为了让一个对象兼容 with 语句，你需要实现 __enter__() 和 __exit__() 方法 12345678910111213141516171819from socket import socket, AF_INET, SOCK_STREAMclass LazyConnection: def __init__(self, address, family=AF_INET, type=SOCK_STREAM): self.address = address self.family = family self.type = type self.sock = None def __enter__(self): if self.sock is not None: raise RuntimeError('Already connected') self.sock = socket(self.family, self.type) self.sock.connect(self.address) return self.sock def __exit__(self, exc_ty, exc_val, tb): self.sock.close() self.sock = None 8.5 在类中封装属性名(私有属性) 第一个约定是任何以单下划线_开头的名字都应该是内部实现(私有属性或方法)。 Python并不会真的阻止别人访问内部名称,但是应该尽量去避免调用内部方法 双下划线__开头的属性或方法通过继承是无法被覆盖或者修改 有时候你定义的一个变量和某个保留关键字冲突，这时候可以使用单下划线作为后缀:lambda_ = 2.0 大多数而言，你应该让你的非公共名称以单下划线开头。但是，如果你清楚你的代码会涉及到子类， 并且有些内部属性应该在子类中隐藏起来，那么才考虑使用双下划线方案。 8.6 创建可管理的属性 你想给某个实例attribute增加除访问与修改之外的其他处理逻辑，比如类型检查或合法性验证。 1234567891011121314151617181920class Person: def __init__(self, first_name): self.first_name = first_name # Getter function @property def first_name(self): return self._first_name # Setter function @first_name.setter def first_name(self, value): if not isinstance(value, str): raise TypeError('Expected a string') self._first_name = value # Deleter function (optional) @first_name.deleter def first_name(self): raise AttributeError("Can't delete attribute") 8.25 创建缓存实例 在创建一个类的对象时，如果之前使用同样参数创建过这个对象， 你想返回它的缓存引用 详细实现请见9.13小节 8.9 创建新的类或实例属性(描述器) 定义描述器__get__ __set__ __delete__三种方法在类中 为了使用一个描述器，需将这个描述器的实例作为类属性放到一个类的定义中:x = Integer(&#39;x&#39;) 描述器可实现大部分Python类特性中的底层魔法， 包括 @classmethod 、@staticmethod 、@property 第九章:元编程 按照默认习惯，metaclass的类名总是以Metaclass结尾，以便清楚地表示这是一个metaclass metaclass用来控制类的创建行为。当我们在类中传入关键字参数metaclass时，魔术就生效了。 metaclass可以隐式地继承到子类，但子类自己却感觉不到。 9.1 在函数上添加装饰器 装饰器的用途是为函数增加额外功能而不影响代码的整体结构的一种方法，而@function是装饰器的语法糖 定义一个装饰器 123456789101112import timefrom functools import wrapsdef timethis(func): @wraps(func) def wrapper(*args, **kwargs): start = time.time() result = func(*args, **kwargs) end = time.time() print(func.__name__, end-start) return result return wrapper 使用装饰器 123456789@timethisdef countdown(n): while n &gt; 0: n -= 1&gt;&gt;&gt; countdown(100000)countdown 0.008917808532714844&gt;&gt;&gt; countdown(10000000)countdown 0.87188299392912 9.2 创建装饰器时保留函数元信息 任何时候你定义装饰器的时候，都应该使用 functools 库中的 @wraps 装饰器来注解底层包装函数。 9.3 解除一个装饰器 解除一个装饰器是指:一个装饰器已经作用在一个函数上，你想撤销它，直接访问原始的未包装的那个函数 假设装饰器是通过 @wraps (参考9.2小节)来实现的，那么你可以通过访问 __wrapped__ 属性来访问原始函数：orig_func = new_func.__wrapped__ 并不是所有的装饰器都使用了 @wraps ，因此这里的方案并不全部适用 9.4 定义一个带参数的装饰器 装饰器是可以使用参数的，关键点是包装器是可以使用传递给最外层的参数的 带参数的装饰器分为三层，最外层为装饰器名称以及参数:def logged(level, name=None, message=None):，中层为def decorate(func):,内层为@wraps(func) 9.5 可自定义属性的装饰器 你想写一个装饰器来包装一个函数，并且允许用户提供参数在运行时控制装饰器行为 9.6 带可选参数的装饰器 带可选参数的装饰器是指:你想写一个装饰器，既可以不传参数给它，比如 @decorator ，也可以传递可选参数给它，比如 @decorator(x,y,z) 主要实现装饰器带或者不带括号都可以正常工作，实现编程一致性 带参数与不带参数的装饰器区别是：初始化时，orig_func是否被传入 new_func = logged(orig_func) new_func = logged(level=logging.CRITICAL, name=&#39;example&#39;)(orig_func) 实现原理: 如果装饰器无参数，会传入func，跳过if语句内的partial方法,直接将orig_func传入 如果装饰器有参数，初始化时func为None，执行partial方法，导入其它参数并返回一个未完全初始化的自身，以确定除了orig_func之外其它参数。此时等价于无参装饰器。继续初始化执行new_func = logged(orig_func) 9.7 利用装饰器强制函数上的类型检查 inspect.signature(func) 函数，它可以得到func函数的参数： 123456789from inspect import signaturedef func(x, y, z=42): passsig = signature(func)sig # (x, y, z=42)sig.parameters # mappingproxy(OrderedDict([('x', &lt;Parameter at 0x10077a050 'x'&gt;),('y', &lt;Parameter at 0x10077a158 'y'&gt;), ('z', &lt;Parameter at 0x10077a1b0 'z'&gt;)]))sig.parameters['z'].name # 'z'sig.parameters['z'].default # 42sig.parameters['z'].kind # &lt;_ParameterKind: 'POSITIONAL_OR_KEYWORD'&gt; sig.bind()方法 sig.bind(int, 2, 3).arguments返回一个有序字典OrderedDict([(&#39;x&#39;, int), (&#39;y&#39;, 2), (&#39;z&#39;, 3)]),key值是被绑定的函数参数值,value是你指定的数据类型或者其他值。 sig.bind_partial(int,z=int)允许忽略一部分参数，而sig.bind方法不允许 核心原理： 使用sig.bind_partial(*ty_args, **ty_kwargs).arguments使bound_types指定根据装饰器参数形成一个有序字典，指定函数类型 使用sig.bind(*args, **kwargs)arguments方法使bound_values根据函数传入的值形成一个有序字典，为调用函数时传入的值 通过对比做出判断，然而这种方法不能判断出默认参数(内部转换)是否符合要求，由于可变对象[]不应作为参数，所以默认参数需做判断:if x == None：x = [] 9.10 为类和静态方法提供装饰器 为类的方法提供装饰器和为函数添加装饰器定义与使用方法是一致的 如果类中方法存在装饰器 @classmethod 和 @staticmethod ，要把他们放在最上面，否则会报错 如果希望装饰器访问类的属性，需做如下修改： 在wrapper传入参数self 类方法origin_func(self, *args, **kwargs)传入参数self 12345678910111213141516171819def catch_exception(origin_func): def wrapper(self, *args, **kwargs): try: u = origin_func(self, *args, **kwargs) return u except Exception: self.revive() #不用顾虑，直接调用原来的类的方法 return 'an Exception raised.' return wrapperclass Test(object): def __init__(self): pass def revive(self): print('revive from exception.') # do something to restore @catch_exception def read_value(self): print('here I will do something.') # do something. 9.13 使用元类控制实例的创建 一个类可以在__init_中规定它的创建方式。 我们希望通过创建元类改变实例创建方式来实现单例、缓存或其他类似的特性。 我们可以通过定义元类中的 __call__() 方法规定类的创建方式。并在创建类时通过metaclass关键字参数确定创建方式:class Spam(metaclass=NoInstances):... 定义只能创建唯一的实例的元类Singleton 1234567891011121314151617class Singleton(type): def __init__(self, *args, **kwargs): self.__instance = None super().__init__(*args, **kwargs) def __call__(self, *args, **kwargs): if self.__instance is None: self.__instance = super().__call__(*args, **kwargs) return self.__instance else: return self.__instance# Exampleclass Spam(metaclass=Singleton): def __init__(self): print('Creating Spam') 详细解析 元类Singleton(type)需要传入父类type(元类定义皆为如此) 元类中def __call__方法定义了类实例化时的行为 Singleton在Spam定义(解释器扫描到metaclass=Singleton)时被初始化，此时self.__instance = None 当Spam第一次实例化时，self.__instance == None返回super().__call__(*args, **kwargs),即Spam的实例 当Spam第二次实例化时，由于self.__instance等于Spam第一次返回的实例，这个值直接被返回了。 注意的是,当Spam第二次实例化时,由于super().__call__方法根本没有执行，所以他的__init__方法也不会被调用了 由此便实现了只能初始化一次的元类方法 9.21 避免重复的属性方法 本节展示了对类进行类型检查的三种方式，并逐步简化的过程 9.22 定义上下文管理器的简单方法(with) with的基本概念 with 语句适用于对资源进行访问的场合，确保不管使用过程中是否发生异常都会执行必要的“清理”操作。 有了上下文管理器，with 语句才能工作。 上下文管理器（Context Manager）：支持上下文管理协议的对象，这种对象实现了__enter__() 和 __exit__() 方法。 语句体（with-body）：with 语句包裹起来的代码块，在执行语句体之前会调用上下文管理器的 __enter__() 方法，执行完语句体之后会执行 __exit__() 方法。 with语句遇到错误也会抛出异常，区别是它在遇到异常后仍可以执行清理操作。 通常情况下，如果要写一个上下文管理器，你需要定义一个类，里面包含一个 __enter__() 和一个__exit__() 方法 更好的方法是使用contexlib 模块中的 @contextmanager 装饰器 @contextmanager使用方法 yield 之前的代码会在上下文管理器中作为 __enter__() 方法执行， yield 之后的代码会作为 __exit__() 方法执行。 yield 后如果含有值，会返回as后面的内容，类似于return 如果希望执行__exit__()方法,就在yield前加try:,yield后加finally;否则不执行清理操作 9.23 在局部变量域中执行代码(exec) exec(&#39;func_str&#39;)在全局作用域中可以获取并改变全局作用域中的变量 exec(&#39;func_str&#39;)在局部作用域中无法改变局部变量或全局变量的值，它获得的时变量的字典拷贝。 希望获得exec(&#39;func_str&#39;)在局部作用域中的运行结果，可以在exec之前使用locals_dic = locals()获得局部变量字典，这个字典的值是exec真正改变的值 每次调用locals() 会获取局部变量值中的值并覆盖字典中相应的变量。如果在exec之后调用locals()将无法获得正确结果 12345678# exec在局部作用域中无法改变局部变量或全局变量的值a = 10def exec_test(): locals_dic = locals() exec("a += 1") print('exec:',locals_dic['a'],'glabal:',a)exec_test() # exec: 11 glabal: 10 9.25 拆解Python字节码第十章:模块与包10.1 构建一个模块的层级包 一个文件夹内如果存在__init__.py,这个文件夹就成为了一个包 大部分情况下__init__.py文件为空即可 如果在子包的__init__.py中写入from . import jpg父模块便可以自动加载子模块jpg.py了 自动加载是指对于lib/jpg.py：import lib 之后,直接使用lib.jpg即可 否则需要from lib import jpg之后才能使用lib.jpg 直接使用文件夹内的函数 10.2 控制模块被全部导入的内容 强烈反对使用 from module import * 如果你不做任何事, 这样的导入将会导入所有不以下划线开头的。 如果在模块中定义了 __all__ , 那么只有被列举出的东西会被导出:__all__ = [&#39;spam&#39;, &#39;grok&#39;] 10.3 使用相对路径名导入包中子模块文件结构如下 123456789mypackage/ |-__init__.py |-A/ |-__init__.py |-spam.py |-grok.py |-B/ |-__init__.py |-bar.py 在spam中引入grok和bar,只需如此操作： 12from . import grokfrom ..B import bar 10.4 将模块分割成多个文件10.5 利用命名空间导入目录分散的代码 不同的目录有着相同的模块名称,希望将同名模块统一成唯一的模块直接引入 前提是在任何一个目录里都没有__init__.py文件。 1234import syssys.path.extend(['foo-package', 'bar-package'])import spam.blahimport spam.grok 10.6 重新加载模块10.7 运行目录或压缩文件 如果__main__.py存在于顶层目录，你可以简单地在顶级目录运行这个文件: python dirname 10.8 读取位于包中的数据文件 可以使用pkgutil.get_data来读取包中的文件 在包中尽量不使用I/O操作,1是I/O操作需要使用绝对文件名;二是包通常安装作为.zip或.egg文件,open方法此时不会工作 12import pkgutildata = pkgutil.get_data(__package__, 'somedata.dat') 10.9 将文件夹加入到sys.path10.10 通过字符串名导入模块 你想导入一个模块，但是模块的名字在字符串里。你想对字符串调用导入命令。 12import importlibmath = importlib.import_module('math') 10.11 通过钩子远程加载模块10.12 导入模块的同时修改模块10.13 安装私有的包 Python有一个用户安装目录，通常类似”~/.local/lib/python3.3/site-packages”。 要强制在这个目录中安装包，可使用安装选项“–user” 123python3 setup.py install --user# orpip install --user packagename 通常包会被安装到系统的site-packages目录中去 路径类似“/usr/local/lib/python3.3/site-packages”。 不过，这样做需要有管理员权限并且使用sudo命令。 就算你有这样的权限去执行命令，使用sudo去安装一个新的，可能没有被验证过的包有时候也不安全。 安装包到用户目录中通常是一个有效的方案，它允许你创建一个自定义安装。 10.14 创建新的Python环境10.15 分发包install_requires:指定了在安装这个包的过程中, 需要哪些其他包。 如果条件不满足, 则会自动安装依赖的库。 12setup(install_requires=["requests"]) # example1setup(install_requires=["numpy &gt;= 1.8.1", "pandas &gt;= 0.14.1"]) # example2 python核心 - 打包与发布 第十一章:网络与web编程11.1 作为客户端与HTTP服务交互对于真的很简单HTTP客户端代码，用内置的 urllib 模块通常就足够了。但是，如果你要做的不仅仅只是简单的GET或POST请求，那就真的不能再依赖它的功能了。这时候就是第三方模块比如 requests 大显身手的时候了。 第十二章:并发编程Python是运行在解释器中的语言，查找资料知道，python中有一个全局锁（GIL），在使用多线程Thread)的情况下，不能发挥多核的优势。而使用多进程(Multiprocess)，则可以发挥多核的优势真正地提高效率 一个实验对比 操作类型 CPU密集型操作 IO密集型操作 网络请求密集型操作 线性操作 94.91824996469 22.46199995279 7.3296000004 多线程操作 101.1700000762 24.8605000973 0.5053332647 多进程操作 53.8899999857 12.7840000391 0.5045000315 多线程在IO密集型的操作下似乎也没有很大的优势（也许IO操作的任务再繁重一些就能体现出优势），在CPU密集型的操作下明显地比单线程线性执行性能更差，但是对于网络请求这种忙等阻塞线程的操作，多线程的优势便非常显著了 多进程无论是在CPU密集型还是IO密集型以及网络请求密集型（经常发生线程阻塞的操作）中，都能体现出性能的优势。不过在类似网络请求密集型的操作上，与多线程相差无几，但却更占用CPU等资源，所以对于这种情况下，我们可以选择多线程来执行 第十三章:脚本编程与系统管理终止程序并给出错误信息 raise SystemExit(&#39;It failed!&#39;),它会将消息在 sys.stderr 中打印，然后程序以状态码1退出。 解析命令行选项 argparse 模块可被用来解析命令行选项 创建和解压归档文件 解压: shutil.unpack_archive(&#39;Python-3.3.0.tgz&#39;) 压缩: shutil.make_archive(&#39;new_file_name&#39;,&#39;zip&#39;,&#39;base_file_name&#39;) 得到shutil支持的压缩类型: shutil.get_archive_formats() 运行时弹出密码输入提示 passwd = getpass.getpass()主要是能够使用户在输入密码时不明文显示密码 user = getpass.getuser()会根据该用户的shell环境来使用当前用户的登录名，如果希望弹出用户名输入提示，使用内置的 input 函数：user = input(&#39;Enter your username: &#39;) 获取终端的大小 你需要知道当前终端的大小以便正确的格式化输出。可以使用 size = os.get_terminal_size() 函数来做到这一点 然后使用size.columns和size.lines来得到终端的行和列 实现一个计时器 通过计时器可以得到脚本运行的时间，文中给了个可以运行的计时器类用于计时 通过文件名查找文件 os.walk(basedir)返回一个生成器对象,每次返回一个三元数组，分别是当前目录绝对路径字符串,当前目录下的文件夹名称列表，当前目录下的文件名称列表 启动一个WEB浏览器 webbrowser模块可以快速的使用默认浏览器打开一个网页：webbrowser.open(&#39;http://www.python.org&#39;) 参考文档python3-cookbook]]></content>
  </entry>
  <entry>
    <title><![CDATA[python常用数据结构]]></title>
    <url>%2F2018%2F06%2F21%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[字符串1234567891011121314151617181920212223242526272829303132333435363738394041424344str1 = 'hello, world!'# 通过len函数计算字符串的长度print(len(str1)) # 13# 获得字符串首字母大写的拷贝print(str1.capitalize()) # Hello, world!# 获得字符串变大写后的拷贝print(str1.upper()) # HELLO, WORLD!# 从字符串中查找子串所在位置print(str1.find('or')) # 8print(str1.find('shit')) # -1# 与find类似但找不到子串时会引发异常# print(str1.index('or'))# print(str1.index('shit'))# 检查字符串是否以指定的字符串开头print(str1.startswith('He')) # Falseprint(str1.startswith('hel')) # True# 检查字符串是否以指定的字符串结尾print(str1.endswith('!')) # True# 将字符串以指定的宽度居中并在两侧填充指定的字符print(str1.center(50, '*'))# 将字符串以指定的宽度靠右放置左侧填充指定的字符print(str1.rjust(50, ' '))str2 = 'abc123456'# 从字符串中取出指定位置的字符(下标运算)print(str2[2]) # c# 字符串切片(从指定的开始索引到指定的结束索引)print(str2[2:5]) # c12print(str2[2:]) # c123456print(str2[2::2]) # c246print(str2[::2]) # ac246print(str2[::-1]) # 654321cbaprint(str2[-3:-1]) # 45# 检查字符串是否由数字构成print(str2.isdigit()) # False# 检查字符串是否以字母构成print(str2.isalpha()) # False# 检查字符串是否以数字和字母构成print(str2.isalnum()) # Truestr3 = ' jackfrued@126.com 'print(str3)# 获得字符串修剪左右两侧空格的拷贝print(str3.strip()) 列表列表可以使用下标得到相应位置的元素，下标支持正数，0和负数；0表示列表中第一个元素，-1表示列表倒数第一个元素。 123456789101112131415161718192021222324252627282930313233list1 = [1, 3, 5, 7, 100]print(list1)list2 = ['hello'] * 5print(list2)# 计算列表长度(元素个数)print(len(list1))# 下标(索引)运算print(list1[0])print(list1[4])# print(list1[5]) # IndexError: list index out of rangeprint(list1[-1])print(list1[-3])list1[2] = 300print(list1)# 添加元素# 在末尾列表添加元素list1.append(200)# 在列表下标为1的元素前面插入元素list1.insert(1, 400)list1.insert(-1, 400)# 列表拼接list1 += [1000, 2000]print(list1)print(len(list1))# 删除元素list1.remove(3)if 1234 in list1: list1.remove(1234)del list1[0]print(list1)# 清空列表元素list1.clear()print(list1) 列表的切片和字符串一样，列表也可以做切片操作，通过切片操作我们可以实现对列表的复制或者将列表中的一部分取出来创建出新的列表，代码如下所示。 12345678910111213141516171819def main(): fruits = ['grape', 'apple', 'strawberry', 'waxberry'] fruits += ['pitaya', 'pear', 'mango'] # 循环遍历列表元素 for fruit in fruits: print(fruit.title(), end=' ') print() # 列表切片 fruits2 = fruits[1:4] print(fruits2) # fruit3 = fruits # 没有复制列表只创建了新的引用 # 可以通过完整切片操作来复制列表 fruits3 = fruits[:] print(fruits3) fruits4 = fruits[-3:-1] print(fruits4) # 可以通过反向切片操作来获得倒转后的列表的拷贝 fruits5 = fruits[::-1] print(fruits5) 列表的排序下面的代码实现了对列表的排序操作。 12345678910111213141516171819def main(): list1 = ['orange', 'apple', 'zoo', 'internationalization', 'blueberry'] list2 = sorted(list1) # sorted函数返回列表排序后的拷贝不会修改传入的列表 # 函数的设计就应该像sorted函数一样尽可能不产生副作用 list3 = sorted(list1, reverse=True) # 通过key关键字参数指定根据字符串长度进行排序而不是默认的字母表顺序 list4 = sorted(list1, key=len) print(list1) print(list2) print(list3) print(list4) # 给列表对象发出排序消息直接在列表对象上进行排序 list1.sort(reverse=True) print(list1)if __name__ == '__main__': main() 创建列表我们还可以使用列表的生成式语法来创建列表，代码如下所示。 12345678910111213141516171819202122232425import sysdef main(): f = [x for x in range(1, 10)] print(f) f = [x + y for x in 'ABCDE' for y in '1234567'] print(f) # 用列表的生成表达式语法创建列表容器 # 用这种语法创建列表之后元素已经准备就绪所以需要耗费较多的内存空间 f = [x ** 2 for x in range(1, 1000)] print(sys.getsizeof(f)) # 查看对象占用内存的字节数 print(f) # 请注意下面的代码创建的不是一个列表而是一个生成器对象 # 通过生成器可以获取到数据但它不占用额外的空间存储数据 # 每次需要数据的时候就通过内部的运算得到数据(需要花费额外的时间) f = (x ** 2 for x in range(1, 1000)) print(sys.getsizeof(f)) # 相比生成式生成器不占用存储数据的空间 print(f) for val in f: print(val)if __name__ == '__main__': main() 元组Python 的元组与列表类似，不同之处在于元组的元素不能修改，在前面的代码中我们已经不止一次使用过元组了。顾名思义，我们把多个元素组合到一起就形成了一个元组，所以它和列表一样可以保存多条数据。下面的代码演示了如何定义和使用元组。 123456789101112131415161718192021222324252627282930def main(): # 定义元组 t = ('骆昊', 38, True, '四川成都') print(t) # 获取元组中的元素 print(t[0]) print(t[3]) # 遍历元组中的值 for member in t: print(member) # 重新给元组赋值 # t[0] = '王大锤' # TypeError # 变量t重新引用了新的元组原来的元组将被垃圾回收 t = ('王大锤', 20, True, '云南昆明') print(t) # 将元组转换成列表 person = list(t) print(person) # 列表是可以修改它的元素的 person[0] = '李小龙' person[1] = 25 print(person) # 将列表转换成元组 fruits_list = ['apple', 'banana', 'orange'] fruits_tuple = tuple(fruits_list) print(fruits_tuple)if __name__ == '__main__': main() 使用集合Python中的集合跟数学上的集合是一致的，不允许有重复元素，而且可以进行交集、并集、差集等运算。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def main(): set1 = &#123;1, 2, 3, 3, 3, 2&#125; print(set1) print('Length =', len(set1)) set2 = set(range(1, 10)) print(set2) set1.add(4) set1.add(5) set2.update([11, 12]) print(set1) print(set2) set2.discard(5) # remove的元素如果不存在会引发KeyError if 4 in set2: set2.remove(4) print(set2) # 遍历集合容器 for elem in set2: print(elem ** 2, end=' ') print() # 将元组转换成集合 set3 = set((1, 2, 3, 3, 2, 1)) print(set3.pop()) print(set3) # 集合的交集、并集、差集、对称差运算 print(set1 &amp; set2) # print(set1.intersection(set2)) print(set1 | set2) # print(set1.union(set2)) print(set1 - set2) # print(set1.difference(set2)) print(set1 ^ set2) # print(set1.symmetric_difference(set2)) # 判断子集和超集 print(set2 &lt;= set1) # print(set2.issubset(set1)) print(set3 &lt;= set1) # print(set3.issubset(set1)) print(set1 &gt;= set2) # print(set1.issuperset(set2)) print(set1 &gt;= set3) # print(set1.issuperset(set3))if __name__ == '__main__': main() 说明：Python中允许通过一些特殊的方法来为某种类型或数据结构自定义运算符（后面的章节中会讲到），上面的代码中我们对集合进行运算的时候可以调用集合对象的方法，也可以直接使用对应的运算符，例如&amp;运算符跟intersection方法的作用就是一样的，但是使用运算符让代码更加直观。 使用字典字典是另一种可变容器模型，类似于我们生活中使用的字典，它可以存储任意类型对象，与列表、集合不同的是，字典的每个元素都是由一个键和一个值组成的“键值对”，键和值通过冒号分开。下面的代码演示了如何定义和使用字典。 1234567891011121314151617181920212223242526272829def main(): scores = &#123;'骆昊': 95, '白元芳': 78, '狄仁杰': 82&#125; # 通过键可以获取字典中对应的值 print(scores['骆昊']) print(scores['狄仁杰']) # 对字典进行遍历(遍历的其实是键再通过键取对应的值) for elem in scores: print('%s\t---&gt;\t%d' % (elem, scores[elem])) # 更新字典中的元素 scores['白元芳'] = 65 scores['诸葛王朗'] = 71 scores.update(冷面=67, 方启鹤=85) print(scores) if '武则天' in scores: print(scores['武则天']) print(scores.get('武则天')) # get方法也是通过键获取对应的值但是可以设置默认值 print(scores.get('武则天', 60)) # 删除字典中的元素 print(scores.popitem()) print(scores.popitem()) print(scores.pop('骆昊', 100)) # 清空字典 scores.clear() print(scores)if __name__ == '__main__': main()]]></content>
  </entry>
  <entry>
    <title><![CDATA[js判断元素是否对于人眼可见]]></title>
    <url>%2F2018%2F06%2F20%2F%E5%89%8D%E7%AB%AF%2Fjs%E5%88%A4%E6%96%AD%E5%85%83%E7%B4%A0%E6%98%AF%E5%90%A6%E9%9A%90%E8%97%8F%2F</url>
    <content type="text"><![CDATA[jquery中通过 $(“#id”).is(“:hidden”); 判断一个元素是否是隐藏状态， 其最终调用的代码如下： 12345jQuery.expr.filters.hidden = function( elem ) &#123;// Support: Opera &lt;= 12.12// Opera reports offsetWidths and offsetHeights less than zero on some elementsreturn elem.offsetWidth &lt;= 0 &amp;&amp; elem.offsetHeight &lt;= 0;&#125;; 因此本质上可以通过元素的offsetWidth 和 offsetHeight 同时小于等于0判断元素是否被隐藏 使用场景：父元素可能设置了display:none 需要判断子元素当前是否显示 代码如下： 123456789101112131415161718192021222324252627function isElementVisible(el) &#123; var rect = el.getBoundingClientRect(), vWidth = window.innerWidth || document.documentElement.clientWidth, vHeight = window.innerHeight || document.documentElement.clientHeight, efp = function (p, x, y) &#123; var els = document.elementsFromPoint(x, y); // 获取某点的所有元素, 最顶层的元素在最前面 for (var index = 0; index &lt; els.length; index++) &#123; var style = getComputedStyle(els[index]); // 如果此前的元素是半透明的，并且不是当前元素，则跳过当前元素 if (p != els[index] &amp;&amp; (style.opacity &lt; 1 || style.display == 'none' || ['collapse', 'hidden'].indexOf(el.style.visibility) == -1)) &#123; continue; &#125; else return els[index]; &#125; return els[0]; &#125;; // Return false if it's not in the viewport if (rect.right &lt; 0 || rect.bottom &lt; 0 || rect.left &gt; vWidth || rect.top &gt; vHeight) return false; return ( el.contains(efp(el, rect.left, rect.top)) || el.contains(efp(el, rect.right, rect.top)) || el.contains(efp(el, rect.right, rect.bottom)) || el.contains(efp(el, rect.left, rect.bottom))) || el.contains(efp(el, rect.left + (rect.right - rect.left) / 2, rect.top + (rect.bottom - rect.top) / 2)); &#125; 大致思路： 先判断元素是否在视窗区域内（视窗指浏览器窗口，webview的窗口）在判断元素的四角和中心点是否在最顶层，如果有遮罩则去掉遮罩的影响（遮罩比如是透明或者半透明的元素）]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bootstrap中table宽度设置无效解决办法]]></title>
    <url>%2F2018%2F06%2F15%2F%E5%89%8D%E7%AB%AF%2Fbootstrap%E4%B8%ADtable%E5%AE%BD%E5%BA%A6%E8%AE%BE%E7%BD%AE%E6%97%A0%E6%95%88%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[bootstrap中需要为table设置table-layout:fixed属性，才可以设置table的宽度，否则设置无效 因此可以： 1&lt;table style="table-layout:fixed; width:100%; height:90%;" border="1"&gt; 或者 1table &#123;table-layout:fixed; width:100%;&#125;]]></content>
      <tags>
        <tag>web前端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyautogui的使用]]></title>
    <url>%2F2018%2F06%2F14%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpyautogui%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[pyantogui简介pyantogui的核心是通过截屏，寻图的方式，得到目标的坐标，继而操控鼠标，键盘模拟人类对计算机的操作，从而实现自动化控制。 参考文档pyantogui项目地址]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pywinauto简明教程]]></title>
    <url>%2F2018%2F06%2F13%2Fpython%2F3.python%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93%2Fpywinauto%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[下载安装pywinauto直接使用pip安装pip install --upgrade pywinauto (Py2.7+, Py3.3+) 或者也可以手动安装(离线项目主机) 安装 pyWin32 extensions 下载 six并执行 python setup.py install来安装 下载 comtypes 并执行 python setup.py install来安装 下载 the latest pywinauto 并执行 python setup.py install来安装 注意事项 six，comtypes，pywinauto可以使用 pip download pywinauto 得到相应的包 离线情况下安装依赖pywin32 安装pywin32 要注意python版本，位数（要和系统统一），setutools版本等问题。官方下载地址 支持的控件标准Win32控件：MFC, WTL, VB6和其他一些使用WinForms的老应用所有基于MS UI Automation的标准部件：WPF, Qt, 所有浏览器, Windows文件资源管理器和其他 对于非标准控件，简单情况下，我们可以在得到窗口句柄后，模拟键盘命令对其进行操作。 快速开始实例讲解1234567891011121314151617181920from pywinauto.application import Application# 打开一个记事本(如果已经打开，可以忽略)app = Application().start("notepad.exe")# 绑定进程,class_name和title是可选的，可以灵活使用，如果找到多个货没有找到该程序，程序会报错app = Application().connect(class_name="Notepad",title="无标题 - 记事本")# 得到可操作的窗口，可以传入标题，类名，或者将标题传入键值win = app.window(title="无标题 - 记事本")# 或者(通常使用此方法)win = app["无标题 - 记事本"]# 可以使用Edit对可编辑区进行编辑win.Edit.type_keys('test.txt')win.menu_select("文件-&gt;保存")# 当弹出新的窗口时，窗口标题变化，因此需要重新确定可操作窗口win = app['另存为']win.Edit.type_keys('test.txt')# 窗口内含有的按钮等名称，同样可以作为键值传入，从而得到控件win['保存'].click() 如果不能确定如何寻找控件，可以使用以下方法打印出所有控件 1win.print_control_identifiers() 一个完整的例子12345678910from pywinauto.application import Applicationapp = Application().start("notepad.exe")app = Application().connect(class_name="Notepad")app['无标题 - 记事本'].Edit.type_keys('test01')app['无标题 - 记事本'].menu_select("文件-&gt;保存")app['另存为'].Edit.type_keys('test.txt')app['另存为']['保存'].click()app['确认另存为']['是'].click() 方法简介指定可用的Application实例像要操作某个窗口，必须先实例化这个窗口，实例化窗口有下面两种方法可以实现： start （self ， cmd_line ， timeout = app_start_timeout ） connect （self ， ** kwargs ） start()启动程序并实例化app = Application().start(r&quot;c:\path\to\your\application -a -n -y --arguments&quot;) 其中超时参数是可选的，如果应用程序需要很长时间来启动，则只需要使用该参数。 connect()实例化已经启动的程序：connect()是当自动化程序已经启动时来使用，可以传入以下几种参数进行绑定： 进程： 应用的过程ID app = Application().connect(process=2341) 句柄：应用程序的窗口句柄 app = Application().connect(handle=0x010f0c) 路径：进程中可执行文件路径（GetModuleFileNameEx)用于查找每个进程的路径并将其传入的值进行比较） app = Application().connect （path = r “c：\ windows \ system32 \ notepad.exe” ） 任何窗口参数的组合，都会传递给pywinauto.findwindows.find_elements()函数，例如： app = Application().connect （title_re = “。* Notepad” ， class_name = “Notepad” ） 注意：应用程序在你使用connect()之前窗口必须已经准备好。如果无法确定的话，你需要睡眠或者编写一个循环等待来等待应用程序完全启动。 如何指定应用程序的对话框应用程序实例化完成之后，接着需要指定这个窗口。 例如： dlg = app.Notepad dlg = app[&#39;Notepad&#39;] 接下来是一个最简单的方法，去询问top_window()函数 dlg = app.top_window() 它将返回这个应用程序最高层级的窗口 注意：这是目前尚未测试的，所以我们并不清楚它是否会正确的返回 如果上述还不能进行有效控制，那么你可以使用项目参数传递给findwindows.find.window() dlg = app.window(title_re=&quot;Page Setup&quot;, class_name=&quot;#32770&quot;) 最后介绍一个你可以进行多控制的方法 dialogs = app.windows() 这将返回应用程序中所有可见，启用的顶层窗口列表，然后你就可以使用handleprops模块中的某些方法所选用的对话框，一旦你拥有其句柄，就可以使用 app.window(handle=win) 注意：如果对话框的标题很长，可以使用正则进行匹配 app.window(title_re=&quot;.*Part of Title.*&quot;) 其中 title 和 title_re的区别是: title参数：无法使用正则，需要完全匹配标题 title_re参数:支持正则表达式 如何在对话框上指定控件有很多方法，最简单的就是 12app.dlg.controlapp['dlg']['control'] 代码依据如下内容来构建多个标识符： 标题 相关类 标题 + 相关类 如果标签的文本为空（或者删除不能使用的字符后为），那么文本就不能被使用。相反，我们会寻找上面和最右边的控制，并附加其相关类，所以列表就是： 相关类 联系最紧密的文字+ 相关类 一旦对话框中所有控件创建了一组标识符，我们就将它们消除歧义 使用WindowSpecification.print_control_identifiers() 例如 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071dlg_spec = app['无标题 - 记事本']dlg_spec.print_control_identifiers()&gt;&gt;&gt;Control Identifiers:Dialog - '无标题 - 记事本' (L481, T434, R1281, B802)['无标题 - 记事本Dialog', 'Dialog', '无标题 - 记事本']child_window(title="无标题 - 记事本", control_type="Window") | | Edit - '文本编辑器' (L489, T485, R1273, B794) | ['', 'Edit', '0', '1'] | child_window(title="文本编辑器", auto_id="15", control_type="Edit") | | | | ScrollBar - '垂直滚动条' (L1256, T485, R1273, B794) | | ['垂直滚动条ScrollBar', '垂直滚动条', 'ScrollBar'] | | child_window(title="垂直滚动条", auto_id="NonClientVerticalScrollBar", control_type="ScrollBar") | | | | | | Button - '上一行' (L1256, T485, R1273, B502) | | | ['上一行', '上一行Button', 'Button', 'Button0', 'Button1'] | | | child_window(title="上一行", auto_id="UpButton", control_type="Button") | | | | | | Button - '下一行' (L1256, T777, R1273, B794) | | | ['下一行', '下一行Button', 'Button2'] | | | child_window(title="下一行", auto_id="DownButton", control_type="Button") | | TitleBar - 'None' (L505, T437, R1273, B465) | ['2', 'TitleBar'] | | | | Menu - '系统' (L489, T442, R511, B464) | | ['系统Menu', '系统', 'Menu', '系统0', '系统1', 'Menu0', 'Menu1'] | | child_window(title="系统", auto_id="MenuBar", control_type="MenuBar") | | | | | | MenuItem - '系统' (L489, T442, R511, B464) | | | ['系统2', 'MenuItem', '系统MenuItem', 'MenuItem0', 'MenuItem1'] | | | child_window(title="系统", control_type="MenuItem") | | | | Button - '最小化' (L1134, T435, R1181, B465) | | ['最小化Button', '最小化', 'Button3'] | | child_window(title="最小化", control_type="Button") | | | | Button - '最大化' (L1181, T435, R1227, B465) | | ['最大化Button', '最大化', 'Button4'] | | child_window(title="最大化", control_type="Button") | | | | Button - '关闭' (L1227, T435, R1274, B465) | | ['关闭', '关闭Button', 'Button5'] | | child_window(title="关闭", control_type="Button") | | Menu - '应用程序' (L489, T465, R1273, B484) | ['应用程序', 'Menu2', '应用程序Menu'] | child_window(title="应用程序", auto_id="MenuBar", control_type="MenuBar") | | | | MenuItem - '文件(F)' (L489, T465, R541, B484) | | ['文件(F)MenuItem', 'MenuItem2', '文件(F)'] | | child_window(title="文件(F)", control_type="MenuItem") | | | | MenuItem - '编辑(E)' (L541, T465, R594, B484) | | ['MenuItem3', '编辑(E)', '编辑(E)MenuItem'] | | child_window(title="编辑(E)", control_type="MenuItem") | | | | MenuItem - '格式(O)' (L594, T465, R650, B484) | | ['格式(O)', '格式(O)MenuItem', 'MenuItem4'] | | child_window(title="格式(O)", control_type="MenuItem") | | | | MenuItem - '查看(V)' (L650, T465, R704, B484) | | ['查看(V)MenuItem', '查看(V)', 'MenuItem5'] | | child_window(title="查看(V)", control_type="MenuItem") | | | | MenuItem - '帮助(H)' (L704, T465, R759, B484) | | ['帮助(H)', 'MenuItem6', '帮助(H)MenuItem'] | | child_window(title="帮助(H)", control_type="MenuItem") 注意：此方法打印的标识符已经通过标识的唯一进程。所以如果你有两个编辑框，它们都会在其中列出。实际上，第一个可以被称之为“编辑”，“编辑0”，“编辑1”和第二个应该被称为“编辑2” 注意：你不需要精确！ 如何使用pywinauto在英文之外的环境在py2中，Python的编码一直是蛋疼的问题，但是py3的出现改变了这一现状。Python3中，字符串是以Unicode编码的，也就是说，Python的字符串支持多语言。使用如下方法来进行属性控制 在英文文档中，此部分还是以Python2为基础 app.dialog_ident.control_ident.click() app[&#39;dialog_ident&#39;][&#39;control_ident&#39;].click() app.window(title_re=&quot;NonAsciiCharacters&quot;).window(title=&quot;MoreNonAsciiCharacters&quot;).click() 如何处理不按照预期进行响应的控件（例如OwnerDraw控件）对于非标准控件，无法定位到控件并对其进行操作。我们可以通过定位到其窗口，并模拟键盘操作的方式来操控它,使用上下箭头移动或则使用快捷键进行操作。 dialog.type_keys(&quot;{HOME}{DOWN 2}{ENTER}ABC&quot;) 上面的例子表示模拟键盘依次键入了HOME DOWN DOWN ENTER A B C 对于一些特殊符号的快捷键，对应的码表如下: 按键名称 对应符号 SHIFT + CTRL ^ ALT % SPACE {SPACE} BACKSPACE {BACKSPACE} {BS} or{BKSP} BREAK {BREAK} CAPS LOCK {CAPSLOCK} DEL or DELETE {DELETE} or {DEL} DOWN ARROW {DOWN} END {END} ENTER {ENTER} or ~ ESC {ESC} HELP {HELP} HOME {HOME} INS or INSERT {INSERT} or {INS} LEFT ARROW {LEFT} NUM LOCK {NUMLOCK} PAGE DOWN {PGDN} PAGE UP {PGUP} PRINT SCREEN {PRTSC} RIGHT ARROW {RIGHT} SCROLL LOCK {SCROLLLOCK} TAB {TAB} UP ARROW {UP} + {ADD} - {SUBTRACT} * {MULTIPLY} / {DIVIDE} 如何访问系统托盘（SysTray，通知区域）123import pywinauto.applicationapp = pywinauto.application.Application().connect(path="explorer")systray_icons = app.ShellTrayWnd.NotificationAreaToolbar 任务栏模块提供对系统托盘的初步访问。 它定义了以下变量: explorer_app 定义连接到正在运行的资源管理器的Application()对象。你可能不需要直接使用它。 任务栏 任务栏的句柄(包括开始按钮，QuickLaunch图标，正在运行的任务等) 开始按钮 “启动我”:-)我想你可能会知道这是什么！ 快速启动 具有快速启动图标的工具栏 SystemTray中 包含时钟和系统托盘图标的窗口 时钟 SystemTrayIcons 表示系统托盘图标的工具栏 RunningApplications 工具条表示运行中的应用程序 我还在模块中提供了两个可以用来点击系统托盘图标的功能： ClickSystemTrayIcon(button) 您可以使用此按钮左键单击系统托盘中的可见图标。我不得不具体说可见的图标，因为可能有许多看不见的图标显然不能被点击。按钮可以是任意整数。如果您指定3，那么它会找到并单击第3个可见按钮。（几乎不会在这里执行错误检查，但这种方法将来会更有可能被移动/重命名。） RightClickSystemTrayIcon(button) 类似于ClickSytemTrayIcon但执行右键单击。 通常，当您点击/右键单击图标时，您将收到一个弹出菜单。在这一点上要记住的是，弹出菜单是应用程序的一部分，而不是资源管理器的一部分。 例如： 12345678# connect to outlookoutlook = Application.connect(path='outlook.exe')# click on Outlook's icontaskbar.ClickSystemTrayIcon("Microsoft Outlook")# Select an item in the popup menuoutlook.PopupMenu.Menu().get_menu_path("Cancel Server Request")[0].click() 常用方法app.click() 点击控件app.close() 关闭窗口app.Check() 勾选复选框app.Uncheck() 取消勾选复选框 所有控件的官方文档python自动化工具之pywinautoPYWINAUTO入门指南]]></content>
  </entry>
  <entry>
    <title><![CDATA[python变量作用域]]></title>
    <url>%2F2018%2F06%2F12%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E5%8F%98%E9%87%8F%E4%BD%9C%E7%94%A8%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[python作用域Python的作用域一共有4中，分别是： L （Local） 局部作用域 E （Enclosing） 闭包函数外的函数中 G （Global） 全局作用域 B （Built-in） 内建作用域 以 L –&gt; E –&gt; G –&gt;B 的规则查找，即：在局部找不到，便会去局部外的局部找（例如闭包），再找不到就会去全局找，再者去内建中找。 局部作用域中的变量，无法改变全局作用域变量的值。 Python除了def/class/lambda 外，其他如: if/elif/else/ try/except for/while并不能改变其作用域。定义在他们之内的变量，外部还是可以访问。 1234567&gt;&gt;&gt; if True:... a = 'I am A'...&gt;&gt;&gt; a'I am A'# 定义在if语言中的变量a，外部还是可以访问的。# 但是需要注意如果if被 def/class/lambda 包裹，在内部赋值，就变成了此 函数/类/lambda 的局部作用域。 在 def/class/lambda内进行赋值，就变成了其局部的作用域，局部作用域会覆盖全局作用域，但不会影响全局作用域。 123456789g = 1 #全局的def fun(): g = 2 #局部的 return gprint fun()# 结果为2print g# 结果为1 但是要注意，有时候想在函数内部引用全局的变量，疏忽了就会出现错误，比如： 123456789101112131415#file1.pyvar = 1def fun(): print var var = 200print fun()#file2.pyvar = 1def fun(): var = var + 1 return varprint fun()# 这两个函数都会报错UnboundLocalError: local variable 'var' referenced before assignment 在未被赋值之前引用的错误！为什么？因为在函数的内部，解释器探测到var被重新赋值了，所以var成为了局部变量，但是在没有被赋值之前就想使用var，便会出现这个错误。解决的方法是在函数内部添加 globals var 但运行函数后全局的var也会被修改。 locals() 和 globals()globals()global 和 globals() 是不同的，global 是关键字用来声明一个局部变量为全局变量。globals() 和 locals() 提供了基于字典的访问全局和局部变量的方式 比如：如果函数f1内需要定义一个局部变量，名字另一个函数f2相同，但又要在函数f1内引用这个函数f2。 123456789101112def f2(): passdef f1(): f2 = 'Just a String' f3 = globals()['f2'] print(f2) return type(f3)print f2()# Just a String# &lt;type 'function'&gt; locals()如果你使用过Python的Web框架，那么你一定经历过需要把一个视图函数内很多的局部变量传递给模板引擎，然后作用在HTML上。虽然你可以有一些更聪明的做法，还你是仍想一次传递很多变量。先不用了解这些语法是怎么来的，用做什么，只需要大致了解locals()是什么。可以看到，locals()把局部变量都给打包一起扔去了。 123456789@app.route('/')def view(): user = User.query.all() article = Article.query.all() ip = request.environ.get('HTTP_X_REAL_IP', request.remote_addr) s = 'Just a String' return render_template('index.html', user=user, article = article, ip=ip, s=s) # 等价于 return render_template('index.html', **locals()) 参考资料(Python 变量作用域)[https://blog.csdn.net/cc7756789w/article/details/46635383]]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python正则表达式]]></title>
    <url>%2F2018%2F06%2F12%2Fpython%2F2.python%E6%A0%87%E5%87%86%E5%BA%93%2Fpython%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F1%2F</url>
    <content type="text"><![CDATA[正则表达式是一个特殊的字符序列，它能帮助你方便的检查一个字符串是否与某种模式匹配。 1. re模块的基本使用python正则表达式整体来讲只需两步即可使用： 12345import re# 1.获得pattern对象，pattern = re.compile('正则表达式')pattern = re.compile('^one')# 2.使用re方法获得所需内容pattern.search('one1two2three3four4') 实际使用过程中,直接链式调用即可 123&gt;&gt;&gt; import re&gt;&gt;&gt; re.compile('^one').search('one1two2three3four4').group()one 2. re.compile与修饰符compile 函数用于编译正则表达式，生成一个正则表达式（ Pattern ）对象，供、match() 和 search() 等函数使用。 1. 语法1re.compile(pattern[, flags]) 2. 参数 pattern : 一个字符串形式的正则表达式 flags 可选，表示匹配模式，比如忽略大小写，多行模式等，具体参数为： re.I 忽略大小写 re.L 表示特殊字符集 \w, \W, \b, \B, \s, \S 依赖于当前环境 re.M 多行模式，当指定时，模式字符 &#39;^&#39; 在字符串的开头和每行的开始处（紧跟每个换行符之后）匹配；并且模式字符 &#39;$&#39; 在字符串的末尾和每行的末尾（紧接在每个换行符之前）匹配。默认情况下，&#39;^&#39; 仅在字符串的开头匹配，&#39;$&#39; 仅在字符串的结尾，紧接在字符串结尾的换行符（如果有）之前。 re.S 即为.并且包括换行符在内的任意字符（默认.不包括换行符） re.U 表示特殊字符集 \w, \W, \b, \B, \d, \D, \s, \S 依赖于 Unicode 字符属性数据库 re.X 为了增加可读性，忽略空格和’ # ‘后面的注释 3. re模块方法1. 字符串搜索1. 从初始位置搜索 matchre.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none。 语法： 1re.compile(pattern[, flags]).match(string) 示例： 12345678910111213141516import re line = "Cats are smarter than dogs"matchObj = re.compile( r'(.*) are (.*?) .*',re.M|re.I).match(line) if matchObj: print ("matchObj.group() : ", matchObj.group()) print ("matchObj.group(1) : ", matchObj.group(1)) print ("matchObj.group(2) : ", matchObj.group(2))else: print ("No match!!") # OUTPUTmatchObj.group() : Cats are smarter than dogsmatchObj.group(1) : CatsmatchObj.group(2) : smarter 2. 扫描整个字符串 searchre.search 扫描整个字符串并返回第一个成功的匹配,如果没有匹配项就返回None。 语法： 1re.compile(pattern[, flags]).search(string) 示例： 12345import re print(re.search('www', 'www.runoob.com').span()) # 在起始位置匹配 print(re.search('com', 'www.runoob.com').span()) # 不在起始位置匹配# OUTPUT(0, 3)(11, 14) 3. 返回所有匹配结果 findall在字符串中找到正则表达式所匹配的所有子串，并返回一个列表，如果没有找到匹配的，则返回空列表。语法： 1findall(string[, pos[, endpos]]) 参数： string 待匹配的字符串。 pos 可选参数，指定字符串的起始位置，默认为 0。 endpos 可选参数，指定字符串的结束位置，默认为字符串的长度。 示例：查找字符串中的所有数字： 123456789101112import repattern = re.compile(r'\d+') # 查找数字result1 = pattern.findall('runoob 123 google 456')result2 = pattern.findall('run88oob123google456', 0, 10)print(result1)print(result2)输出结果：['123', '456']['88', '12'] 区别： match 和 search 是匹配一次 findall 匹配所有。match 和 search 返回的是ReObject，findall返回的是包含匹配字符串的数组 2.检索和替换Python 的re模块提供了re.sub用于替换字符串中的匹配项。 语法： 1re.compile(pattern[, flags]).sub(repl, string, count=0) 参数： repl : 替换的字符串，也可为一个函数。 string : 要被查找替换的原始字符串。 count : 模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。 示例： 123456789101112import re phone = "2004-959-559 # 这是一个电话号码" # 删除注释 num = re.compile(r'#.*$').sub("", phone) print ("电话号码 : ", num) # 移除非数字的内容 num = re.compile(r'\D').sub("", phone) print ("电话号码 : ", num)# OUTPUT电话号码 : 2004-959-559 电话号码 : 2004959559 repl 参数是一个函数时，可以接受匹配的结果作为参数，进行操作，返回新的替换值 以下实例中将字符串中的匹配的数字乘于 2： 12345678910import re # 将匹配的数字乘于 2 def double(matched): value = int(matched.group('value')) return str(value * 2) s = 'A23G4HFD567' print(re.compile('(?P&lt;value&gt;\d+)').sub( double, s))# OUTPUTA46G8HFD1134 4. ReObject对象方法REObject.span()span()方法可以得到匹配结果在字符串中的初始和结尾序号 REObject.group(num= 0) group(num= 0) 返回匹配到的字符串 groups() 返回包含所有匹配到的字符串的元组(puple) 123456789line = "Cats are smarter than dogs"matchObj = re.match( r'(.*) are (.*?) .*', line, re.M|re.I)if matchObj: print ("matchObj.group() : ", matchObj.group()) print ("matchObj.group(1) : ", matchObj.group(1)) print ("matchObj.group(2) : ", matchObj.group(2))else: print ("No match!!") 以上实例执行结果如下： 123matchObj.group() : Cats are smarter than dogsmatchObj.group(1) : CatsmatchObj.group(2) : smarter group()方法可以得到匹配到的字符串，其中group()或group(0)表示整个compile(实例中为(.*) are (.*?) .*)匹配到的字符串，group(1)表示第一个()内的正则匹配到的字符串 5. 贪婪模式与非贪婪模式贪婪模式：(.*) 如果没有限制(匹配分组后面接字符),贪婪模式会匹配到字符串的末尾 如果有限制,贪婪模式会匹配到最后一个符合要求的字符 非贪婪模式(.*?) 如果没有限制,非贪婪模式不匹配任何内容 re.compile().search(‘hello world’) 如果有限制,贪婪模式会匹配到第一个符合要求的字符 由于两种模式的区别,正确的使用匹配模式非常重要，示例如下: 1234567import resite = 'https://www.google.com'result_a = re.compile(r'(.*?):(.*)').search(site).group()result_b = re.compile(r'(.*?):(.*?)').search(site).group()print('贪婪:&#123;0&#125;\n非贪婪:&#123;1&#125;'.format(result_a,result_b))# 贪婪:https://www.google.com# 非贪婪:https: 6. 正则表达式模式模式字符串使用特殊的语法来表示一个正则表达式： 字母和数字表示他们自身。一个正则表达式模式中的字母和数字匹配同样的字符串。 多数字母和数字前加一个反斜杠时会拥有不同的含义。 标点符号只有被转义时才匹配自身，否则它们表示特殊的含义。 反斜杠本身需要使用反斜杠转义。 由于正则表达式通常都包含反斜杠，所以你最好使用原始字符串来表示它们。模式元素(如 r’\t’，等价于 \t )匹配相应的特殊字符。 下表列出了正则表达式模式语法中的特殊元素。如果你使用模式的同时提供了可选的标志参数，某些模式元素的含义会改变。 参考资料菜鸟教程 12]]></content>
      <tags>
        <tag>python 终稿</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python内网电脑离线/使用代理安装依赖]]></title>
    <url>%2F2018%2F06%2F11%2Fpython%2F4.python%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%2F%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85python%E7%AC%AC%E4%B8%89%E6%96%B9%E4%BE%9D%E8%B5%96%E5%8C%85%2F</url>
    <content type="text"><![CDATA[python内网电脑安装依赖npip常用命令1234567891011#安装包pip install xxx#升级包，可以使用-U 或者 --upgradepip install -U xxx#卸载包pip uninstall xxx#列出已安装的包pip list pip离线安装依赖包Step 1. 下载需要离线安装的Packages在一台可以访问外网的机器上执行如下命令： 123$ pip install &lt;package&gt;$ pip download &lt;package&gt;$ pip freeze &gt; requirements.txt Step 2. 将下载好的Packages拷贝至内网服务器使用scp、sftp等方式将下载好的Packages拷贝至需要离线安装这些包的内网服务器。 Step 3. 安装Packages假设内网服务器的目录 /tmp/transferred_packages 包含你上一步远程拷贝过来packages，在内网服务器上执行如下命令 安装单个Package的情况 1$ pip install --no-index --find-links="/tmp/tranferred_packages" &lt;package&gt; 安装多个Packages 1$ pip install --no-index --find-links="/tmp/tranferred_packages" -r requirements.txt pip使用代理安装依赖包正常网络情况下我们安装如果比较多的python包时，会选择使用pip install -r requirements.txt -i https://pypi.douban.com/simple –trusted-host=pypi.douban.com这种国内的镜像来加快下载速度。但是，当这台被限制上网时（公司安全考虑）就不能连外网了，如果懒得一个个下载，又懒得找运维开网络权限时，可以选择设置代理来解决。 有三种常用方式：①永久设置：1234vim /etc/profile： export http_proxy='http://代理服务器IP:端口号' export https_proxy='http://代理服务器IP:端口号'source /etc/profile ②临时设置（重连后失效）：12export http_proxy='http://192.168.71.60:1080'export https_proxy='http://192.168.71.60:1080' 注意：设置之后可能使用ping时还是无法连接外网，但是pip时可以的，因为ping的协议不一样不能使用这个代理 ③单次设置：直接在pip时设置代理也是可以的： 1pip install -r requirements.txt --proxy=代理服务器IP:端口号]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python操作文件路径和目录]]></title>
    <url>%2F2018%2F06%2F11%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E6%93%8D%E4%BD%9C%E6%96%87%E4%BB%B6%E5%92%8C%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[python获取文件路径 os.path.abspath(__file__)在python文件中使用路径,应该使用os.path.abspath(__file__)。永远不要使用os.path.abspath(.)或者os.getpwd(),因为后者具有不确定性。 os.path.abspath(.)和os.getcwd()是等价的，它们表示当前的工作路径，可以被os.chdir()改变 os.path.abspath(__file__)表示当前文件所在路径 它们返回的都是绝对路径。 例1： 1234567&gt;&gt;&gt; os.path.abspath('.')'/mnt/c/Users/user/gaianote.github.io'&gt;&gt;&gt; os.chdir('source')&gt;&gt;&gt; os.getcwd()'/mnt/c/Users/user/gaianote.github.io/source'&gt;&gt;&gt; os.path.abspath('.')'/mnt/c/Users/user/gaianote.github.io/source' 例2: 希望得到工程的根目录BASE_PATH： 12345BASE_PATH||-lib |-config.py # 设置文件，定义了path路径|-main.py # 入口文件 在config.py中书写os.path.abspath(__file__),和 os.path.abspath(&#39;.&#39;),在main.py中执行，第一个得到了config的路径，第二个得到了BASE_PATH的路径 路径拼接不要用abspath(‘.’)，不要用字符串拼接 1path =os.path.abspath（ os.path.abspath('..') + '\\driver\\chromedriver.exe' ） 正确方法,使用os.path.join进行拼接 1234BASE_PATH = os.path.split(os.path.dirname(os.path.abspath(__file__)))[0]# BASE_PATH还可以这么写BASE_PATH = os.path.abspath('.')path =os.path.join(BASE_PATH,'driver','chromedriver.exe') os 模块Python 的 os 模块封装了常见的文件和目录操作，本文只列出部分常用的方法，更多的方法可以查看官方文档。 下面是部分常见的用法： 方法 说明 os.name 指示你正在使用的工作平台。比如对于Windows，它是’nt’，而对于Linux/Unix用户，它是’posi os.getcwd() 得到当前工作目录，即当前python脚本工作的目录路径。 os.listdir() 返回指定目录下的所有文件和目录名 os.stat(file) 获取文件属性 os.mkdir() 创建目录 os.rmdir() 删除空目录或文件 os.system(shell) 运行操作系统命令行 os.rename 重命名 os.remove 删除文件 os.getcwd 获取当前工作路径 os.walk 遍历目录 os.path.join 连接目录与文件名 os.path.split 分割文件名与目录 os.path.abspath 获取绝对路径 os.path.dirname 获取路径(指包含文件的目录) os.path.basename 获取文件名或文件夹名 os.path.splitext 分离文件名与扩展名 os.path.isfile 判断给出的路径是否是一个文件 os.path.isdir 判断给出的路径是否是一个目录 os.path.exits() 判断一个路径目录或者文件是否存在 标准库shutilshutil.move(src,dst)1shutil.move('tmp/20180128/new','tmp/20180128/test') # 移动文件, 重命名等 shutil.copytree(src, dst, symlinks=False, ignore=None)1234shutil.copytree("b","c") # 递归复制。复制一个文件夹及其内容到另一个文件夹，另一个文件夹已存在时报错#(复制一个文件夹路径，把左边的文件夹路径替换为右边的，而不是作为右边的子文件夹)#复制过程中跳过后缀名为参数名的文件shutil.copytree('folder1', 'folder2', ignore=shutil.ignore_patterns('*.py')) shutil.rmtree(path, ignore_errors=False, onerror=None)1shutil.rmtree('tmp/a') # 递归删除目录树.删除一个文件夹(包括这个文件夹)及其内容(文件夹不存在报错) shutil.get_archive_formats()1shutil.get_archive_formats() # 返回支持的 压缩格式列表, 如 [(name,desc),('tar','uncompressed tar file')], shutil.make_archive(base_name, format, root_dir=None, base_dir=None, verbose=0, dry_run=0, owner=None, group=None, logger=None)123456shutil.make_archive('tmp/a/new2','zip',root_dir='/tmp/a') # 创建压缩文件,base_name : 压缩包的文件名, 也可以使压缩包的路径.format : 压缩种类root_dir : 要压缩的文件夹路径, 默认当前目录owner : 用户, 默认当前用户group : 组, 默然当前组 shutil.copy(src, dst)1shutil.copy("a.txt","d.txt") # 复制文件及权限,文件已存在则覆盖 shutil.copyfileobj(fsrc, fdst, length=16384)12shutil.copyfileobj(open('old.xml','r'), open('new.xml', 'w'))# 将文件内容拷贝到另一个文件 shutil.copyfile(src, dst)1shutil.copyfile('f1.log', 'f2.log') # 拷贝文件 shutil.copymode(src, dst)1shutil.copymode('f1.log', 'f2.log') # 仅拷贝权限,内容,用户,组不变 shutil.copystat(src, dst)1shutil.copystat('f1.log', 'f2.log') # 仅拷贝状态信息 shutil.copy2(src, dst)1shutil.copy2('f1.log', 'f2.log') # 拷贝文件和状态信息 环境变量在操作系统中定义的环境变量，全部保存在os.environ这个变量中 12os.environos.environ.get('PATH') 操作文件和目录查看当前目录的绝对路径 1dir = os.path.abspath('.') 在当前目录操作一个新目录newdir,首先需要得到这个目录的绝对路径 1newdir = os.path.join(dir, 'newdir') 路径操作不应使用字符串拼接，而应该使用os.path模块;因为不同操作系统的路径分隔符是不同的 创建新目录 1os.mkdir(newdir) 删除目录 1os.rmdir(newdir) 得到当前目录下所有的文件夹 1[x for x in os.listdir('.') if os.path.isdir(x)] 得到当前目录下所有的.py文件 1[x for x in os.listdir('.') if os.path.isfile(x) and os.path.splitext(x)[1]=='.py']]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python函数的参数]]></title>
    <url>%2F2018%2F06%2F07%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[必选参数必选参数指调用时必须传入的参数，如果没有传入，python解释器会报错 声明: 12def power(x)： return x * x 调用: 12power(10)100 默认参数 定义参数时必选参数在前，默认参数在后，否则Python的解释器会报错 定义默认参数时必须指向不可变对象,否则默认参数的值会随函数调用而变化，造成错误 声明: 12def power(x,y = 5): return x * y 调用: 123456power(10)50power(10,10)100power(10,y = 10)100 可变参数 可变参数传入的是一个puple 可变参数可以省略 声明: 123def printall(*words): for word in words: print(word) 调用: 12345678&gt;&gt;&gt; printall('hello','world')'hello''world'&gt;&gt;&gt; words = ['hello','world']&gt;&gt;&gt; printall(*words)'hello''world' words 与 *words: *words代表的是传入的可变参数，例如1,2,3 而words表示有传入的可变参数组成的puple，比如 (1,2,3) 1234567def printall(*words): print(*words) print(words)&gt;&gt;&gt; printall(1,2,3)1 2 3(1, 2, 3) 关键字参数 关键字参数传入的是一个字典 关键字参数可以省略 声明： 12def person(name, age, **kw): print('name:', name, 'age:', age, 'other:', kw) 调用: 123456&gt;&gt;&gt; person('Bob', 35, city='Beijing')name: Bob age: 35 other: &#123;'city': 'Beijing'&#125;&gt;&gt;&gt; extra = &#123;'city': 'Beijing', 'job': 'Engineer'&#125;&gt;&gt;&gt; person('Jack', 24, **extra)name: Jack age: 24 other: &#123;'city': 'Beijing', 'job': 'Engineer'&#125; 命名关键字参数声明： 命名关键字参数可以有缺省值，从而简化调用 12def person(name, age, *, city='Beijing', job): print(name, age, city, job) 调用: 12&gt;&gt;&gt; person('Jack', 24, job='Engineer')Jack 24 Beijing Engineer 参数组合参数定义的顺序必须是：必选参数、默认参数、可变参数、命名关键字参数和关键字参数]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[获取元素的CSS值]]></title>
    <url>%2F2018%2F06%2F04%2F%E5%89%8D%E7%AB%AF%2F%E8%8E%B7%E5%8F%96%E5%85%83%E7%B4%A0%E7%9A%84CSS%E5%80%BC%2F</url>
    <content type="text"><![CDATA[getComputedStylegetComputedStyle是一个可以获取当前元素所有最终使用的CSS属性值。返回的是一个CSS样式声明对象([object CSSStyleDeclaration])，只读。 getComputedStyle() gives the final used values of all the CSS properties of an element. 语法如下： 1var style = window.getComputedStyle("元素", "伪类"); 例如： 12var dom = document.getElementById("test"), style = window.getComputedStyle(dom , ":after"); 参考链接获取元素CSS值之getComputedStyle方法熟悉]]></content>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python在控制台输出颜色]]></title>
    <url>%2F2018%2F05%2F30%2Fpython%2F4.python%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%2Fpython%E5%9C%A8%E6%8E%A7%E5%88%B6%E5%8F%B0%E8%BE%93%E5%87%BA%E9%A2%9C%E8%89%B2%2F</url>
    <content type="text"><![CDATA[首先安装termcolor库 1pip install termcolor Example123456from termcolor import colored, cprinttext = colored('Hello, World!', 'red', attrs=['reverse', 'blink'])print(text)cprint('Hello, World!', 'green', 'on_red') Text Properties文字颜色:12345678greyredgreenyellowbluemagentacyanwhite 文字背景颜色:12345678on_greyon_redon_greenon_yellowon_blueon_magentaon_cyanon_white 文字属性:123456bolddarkunderlineblinkreverseconcealed windows支持termcolor仅支持linux系统，对于windows，需要结合colorama库进行使用 12345678from colorama import initfrom termcolor import colored# use Colorama to make Termcolor work on Windows tooinit()# then use Termcolor for all colored text outputprint(colored('Hello, World!', 'green', 'on_red'))]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python面向对象编程]]></title>
    <url>%2F2018%2F05%2F29%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[继承理解Python中super()和init()方法我试着理解super()方法.从表面上看,两个子类实现的功能都一样.我想问它们俩的区别在哪里? 123456789101112131415161718class Base(object): def __init__(self): print "Base created"class ChildA(Base): def __init__(self): Base.__init__(self)class ChildB(Base): def __init__(self): super(ChildB, self).__init__() # 这里无需传入参数selfclass ChildC(Base): def __init__(self): super().__init__() # python3中 可以用super().__init__()替换super(ChildB, self).__init__() print(self.name)print ChildA(),ChildB() super()的好处就是可以避免直接使用父类的名字. 多重继承通过多重继承，一个子类就可以同时获得多个父类的所有功能,从而可以避免设计复杂的层级继承关系。这种设计模式称为MixIn 为了更好地看出继承关系，我们把Runnable和Flyable改为RunnableMixIn和FlyableMixIn。类似的，你还可以定义出肉食动物CarnivorousMixIn和植食动物HerbivoresMixIn，让某个动物同时拥有好几个MixIn： 12class Dog(Animal, RunnableMixIn, CarnivorousMixIn): pass 方法组合除了多重继承，我们还可以通过方法组合的方式灵活获得一个工具类，可以需要实际需求进行选取 1234class GameObject(object): def __init__(self): self.player = Player() self.friend = friend()]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[unittest单元测试]]></title>
    <url>%2F2018%2F05%2F29%2Fpython%2F2.python%E6%A0%87%E5%87%86%E5%BA%93%2Funittest%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[一个简单的测试12345678910111213141516171819202122232425import unittestclass TestMathFunc(unittest.TestCase): """Test mathfuc.py""" def test_add(self): """Test method add(a, b)""" self.assertEqual(3, add(1, 2)) self.assertNotEqual(3, add(2, 2)) def test_minus(self): """Test method minus(a, b)""" self.assertEqual(1, minus(3, 2)) def test_multi(self): """Test method multi(a, b)""" self.assertEqual(6, multi(2, 3)) def test_divide(self): """Test method divide(a, b)""" self.assertEqual(2, divide(6, 3)) self.assertEqual(2.5, divide(5, 2))if __name__ == '__main__': unittest.main() 这就是一个简单的测试，有几点需要说明的： 在第一行给出了每一个用例执行的结果的标识，成功是 .，失败是 F，出错是 E，跳过是S。从上面也可以看出，测试的执行跟方法的顺序没有关系，test_divide写在了第4个，但是却是第2个执行的。 每个测试方法均以 test 开头，否则是不被unittest识别的。 在unittest.main()中加 verbosity 参数可以控制输出的错误报告的详细程度，默认是 1，如果设为 0，则不输出每一用例的执行结果，即没有上面的结果中的第1行；如果设为 2，则输出详细的执行结果 用例顺序人为指定用例顺序1234567891011import unittestfrom test_mathfunc import TestMathFuncif __name__ == '__main__': suite = unittest.TestSuite() tests = [TestMathFunc("test_add"), TestMathFunc("test_minus"), TestMathFunc("test_divide")] suite.addTests(tests) runner = unittest.TextTestRunner(verbosity=2) runner.run(suite) https://blog.csdn.net/huilan_same/article/details/52944782 默认顺序使用addTest的方法比较繁琐，而sunittest的默认顺序是按照文件名进行排序的，因此在实际使用过程中，可以数字作为序号的方式进行命名，从而达到排序的目的 123456789101112131415161718192021import unittestclass TestMathFunc(unittest.TestCase): def test_1_add(self): """Test method add(a, b)""" self.assertEqual(3, add(1, 2)) self.assertNotEqual(3, add(2, 2)) def test_2_minus(self): """Test method minus(a, b)""" self.assertEqual(1, minus(3, 2)) def test_3_multi(self): """Test method multi(a, b)""" self.assertEqual(6, multi(2, 3)) def test_4_divide(self): """Test method divide(a, b)""" self.assertEqual(2, divide(6, 3)) self.assertEqual(2.5, divide(5, 2)) setup与teardown执行每条测试用例前调用一次当类里面定义了 setUp() 方法的时候，测试程序会在执行每条测试项前先调用此方法；同样地，在全部测试项执行完毕后，tearDown() 方法也会被调用。 1234567891011121314import unittestclass simple_test(unittest.TestCase): def setUp(self): self.foo = list(range(10)) def test_1st(self): # 这里调用一次setUp self.assertEqual(self.foo.pop(),9) def test_2nd(self): # 这里又调用一次setUp self.assertEqual(self.foo.pop(),9)if __name__ == '__main__': unittest.main() 一个类全程只调用一次 setUp/tearDown那如果我们想全程只调用一次 setUp/tearDown 该怎么办呢？就是用 setUpClass() 和 tearDownClass() 类方法。注意使用这两个方法的时候一定要用 @classmethod 装饰器装饰起来： 123456789101112131415import unittestclass simple_test(unittest.TestCase): @classmethod def setUpClass(self): self.foo = list(range(10)) def test_1st(self): self.assertEqual(self.foo.pop(),9) def test_2nd(self): self.assertEqual(self.foo.pop(),8)if __name__ == '__main__': unittest.main() 整个文件级别上只调用一次 setUp/tearDown整个文件级别上只调用一次 setUp/tearDown，这时候就要用 setUpModule() 和 tearDownModule() 这两个函数了，注意是函数，与 TestCase 类同级： 1234567import unittestdef setUpModule(): passclass simple_test(unittest.TestCase): ... unittest断言断言使用示例123class demoTest(unittest.TestCase): def test1(self): self.assertEqual(4 + 5,9) 常用断言 断言语法 解释 assertEqual(a, b) 判断a==b assertNotEqual(a, b) 判断a！=b assertTrue(x) bool(x) is True assertFalse(x) bool(x) is False assertIs(a, b) a is b assertIsNot(a, b) a is not b assertIsNone(x) x is None assertIsNotNone(x) x is not None]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python条件判断与循环]]></title>
    <url>%2F2018%2F05%2F29%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD%E4%B8%8E%E5%BE%AA%E7%8E%AF%2F</url>
    <content type="text"><![CDATA[条件判断ifif和执行可以写入一行内12is_fat = Trueif is_fat: print('fat') 三元运算符，if和else可以写入一行12is_fat = Truestate = "fat" if is_fat else "not fat" or在if中的使用123result = 5if result == 1 or 2 or 3 or 4: print('result is right') 输出结果为’result is right’,可能与你的预期不符，它的实际判断为： 12if (result == 1) or 2 or 3 or 4: print('result is right') 这个表达式是恒成立的，如果希望达到所需设想，可以使用如下示例： 12if result in [1,2,3,4]: print('result is right') 循环break和continue语句及循环中的else子句break 语句可以跳出 for 和 while 的循环体。如果你从 for 或 while 循环中终止（使用break），任何对应的循环 else 块将不执行。 否则在正常执行完毕后，会继续执行else后的语句,实例如下： 123456789sites = ["Baidu", "Google","Runoob","Taobao"]for site in sites: if site == "Runoob": print("菜鸟教程!") break print("循环数据 " + site)else: print("没有循环数据!")print("完成循环!") 输出结果: 1234循环数据 Baidu循环数据 Google菜鸟教程!完成循环! 123456789sites = ["Baidu", "Google","Runoob","Taobao"]for site in sites: if site == "Runoob": print("菜鸟教程!") break print("循环数据 " + site)else: print("没有循环数据!")print("完成循环!") 123456循环数据 Baidu循环数据 Google循环数据 Runoob循环数据 Taobao没有循环数据!完成循环!]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python算法之笛卡尔积]]></title>
    <url>%2F2018%2F05%2F25%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E7%AE%97%E6%B3%95%E4%B9%8B%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF%2F</url>
    <content type="text"><![CDATA[什么是笛卡尔积笛卡尔乘积是指在数学中，两个集合X和Y的笛卡尓积（Cartesian product），又称直积 假设集合A={a, b}，集合B={0, 1, 2}，则两个集合的笛卡尔积为{(a, 0), (a, 1), (a, 2), (b, 0), (b, 1), (b, 2)} 笛卡尔积的python实现python内置了itertools库，直接引入即可，无需第三方依赖 123456import itertools a = [1,2,3]b = [4,5,6] print ("a,b的笛卡尔乘积：") for x in itertools.product(a,b): print (x) 输出结果为: 12345678910a,b的笛卡尔乘积：(1, 4)(1, 5)(1, 6)(2, 4)(2, 5)(2, 6)(3, 4)(3, 5)(3, 6)]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 标准库中 time 和 datetime 的区别与联系]]></title>
    <url>%2F2018%2F05%2F25%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2FPython%E6%A0%87%E5%87%86%E5%BA%93%E4%B8%ADtime%E5%92%8Cdatetime%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[Python 中提供了对时间日期的多种多样的处理方式，主要是在 time 和 datetime 这两个模块里。今天稍微梳理一下这两个模块在使用上的一些区别和联系。 time在 Python 文档里，time是归类在Generic Operating System Services中，换句话说， 它提供的功能是更加接近于操作系统层面的。通读文档可知，time 模块是围绕着 Unix Timestamp 进行的。 该模块主要包括一个类 struct_time，另外其他几个函数及相关常量。 需要注意的是在该模块中的大多数函数是调用了所在平台C library的同名函数， 所以要特别注意有些函数是平台相关的，可能会在不同的平台有不同的效果。另外一点是，由于是基于Unix Timestamp，所以其所能表述的日期范围被限定在 1970 - 2038 之间，如果你写的代码需要处理在前面所述范围之外的日期，那可能需要考虑使用datetime模块更好。文档解释比较费劲，具体看看怎么用： 123456789101112131415161718192021222324252627In [1]: import timeIn [2]: time.time()Out[2]: 1414332433.345712In [3]: timestamp = time.time()In [4]: time.gmtime(timestamp)Out[4]: time.struct_time(tm_year=2014, tm_mon=10, tm_mday=26, tm_hour=14, tm_min=7, tm_sec=13, tm_wday=6, tm_yday=299, tm_isdst=0)In [5]: time.localtime(timestamp)Out[5]: time.struct_time(tm_year=2014, tm_mon=10, tm_mday=26, tm_hour=22, tm_min=7, tm_sec=13, tm_wday=6, tm_yday=299, tm_isdst=0)In [6]: struct_time = time.localtime(timestamp)In [7]: time.ctime(timestamp)Out[7]: 'Sun Oct 26 22:07:13 2014'In [8]: time.asctime(struct_time)Out[8]: 'Sun Oct 26 22:07:13 2014'In [9]: time.mktime(struct_time)Out[9]: 1414332433.0In [10]: time.strftime("%Y-%m-%d_%H-%M-%S")Out[10]: 2018-05-25_10-04-49In [11]: time.strptime("30 Nov 00", "%d %b %y")Out[11]: time.struct_time(tm_year=2000, tm_mon=11, tm_mday=30, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=3, tm_yday=335, tm_isdst=-1) 问题不大，可能有时候需要注意一下使用的时区。 datetimedatetime 比 time 高级了不少，可以理解为 datetime 基于 time 进行了封装，提供了更多实用的函数。在datetime 模块中包含了几个类，具体关系如下: 123456object timedelta # 主要用于计算时间跨度 tzinfo # 时区相关 time # 只关注时间 date # 只关注日期 datetime # 同时有时间和日期 名称比较绕口，在实际实用中，用得比较多的是 datetime.datetime 和 datetime.timedelta ，另外两个 datetime.date 和 datetime.time 实际使用和 datetime.datetime 并无太大差别。 下面主要讲讲 datetime.datetime 的使用。使用datetime.datetime.now()可以获得当前时刻的datetime.datetime 实例。 对于一个 datetime.datetime 实例，主要会有以下属性及常用方法，看名称就能理解，应该没有太大问题： 1234567891011121314datetime.yeardatetime.monthdatetime.daydatetime.hourdatetime.minutedatetime.seconddatetime.microseconddatetime.tzinfodatetime.date() # 返回 date 对象datetime.time() # 返回 time 对象datetime.replace(name=value) # 前面所述各项属性是 read-only 的，需要此方法才可更改datetime.timetuple() # 返回time.struct_time 对象dattime.strftime(format) # 按照 format 进行格式化输出 除了实例本身具有的方法,类本身也提供了很多好用的方法： 12345datetime.today()a # 当前时间，localtimedatetime.now([tz]) # 当前时间默认 localtimedatetime.utcnow() # UTC 时间datetime.fromtimestamp(timestamp[, tz]) # 由 Unix Timestamp 构建对象datetime.strptime(date_string, format) # 给定时间格式解析字符串 请注意，上面省略了很多和时区相关的函数，如需使用请查文档。对于日期的计算，使用timedelta也算是比较简单的： 1234567891011In [1]: import datetimeIn [2]: time_now = datetime.datetime.now()In [3]: time_nowOut[3]: datetime.datetime(2014, 10, 27, 21, 46, 16, 657523)In [4]: delta1 = datetime.timedelta(hours=25)In [5]: print(time_now + delta1)2014-10-28 22:46:16.657523In [6]: print(time_now - delta1)2014-10-26 20:46:16.657523 甚至两个 datetime 对象直接相减就能获得一个 timedelta 对象。如果有需要计算工作日的需求，可以使用 business_calendar这个库，不需要装其他依赖就可使用。]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python字典]]></title>
    <url>%2F2018%2F05%2F25%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E5%AD%97%E5%85%B8%2F</url>
    <content type="text"><![CDATA[知道value如何找到key值1234&gt;&gt;&gt; dic = &#123;&apos;a&apos;: &#123;&apos;c&apos;:&apos;001&apos;&#125;, &apos;b&apos;:&apos;002&apos;&#125;&gt;&gt;&gt; list(dic.keys())[list(dic.values()).index(&quot;001&quot;)]&apos;a&apos;&gt;&gt;&gt; dic = {‘a’:’001’, ‘b’:’002’}引用一段Python3文档里面的原话。 If keys, values and items views are iterated over with no intervening modifications to the dictionary, the order of items will directly correspond. 也就是说，在你迭代的过程中如果没有发生对字典的修改，那么.keys() and .values 这两个函数返回的 dict-view对象总是保持对应关系。]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[npm安装依赖]]></title>
    <url>%2F2018%2F05%2F25%2Fnodejs%2Fnpm%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%2F</url>
    <content type="text"><![CDATA[如何更新在package.json 写好的依赖到到最新版本，并更新package.json到最新内容npm-check-updates 是一个工具，它使用所有依赖项的最新版本自动调整 package.json 123$ npm install -g npm-check-updates$ npm-check -updates -u$ npm install]]></content>
      <tags>
        <tag>nodejs npm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python异常处理]]></title>
    <url>%2F2018%2F05%2F25%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[try…except12345678try: fh = open("testfile", "w") fh.write("这是一个测试文件，用于测试异常!!")except: print ("Error: 没有找到文件或读取文件失败")else: print ("内容写入文件成功") fh.close() 异常处理基础语法如下:12345678try:&lt;语句&gt; #运行别的代码except &lt;名字&gt;：&lt;语句&gt; #如果在try部份引发了'name'异常except &lt;名字&gt;，&lt;数据&gt;:&lt;语句&gt; #如果引发了'name'异常，获得附加的数据else:&lt;语句&gt; #如果没有异常发生 使用raise抛出异常使用raise会导致程序中断 1raise Exception("发生了一些错误，程序中断") 用户自定义异常通过创建一个新的异常类，程序可以命名它们自己的异常。异常应该是典型的继承自Exception类，通过直接或间接的方式。 以下为与RuntimeError相关的实例,实例中创建了一个类，基类为RuntimeError，用于在异常触发时输出更多的信息。 在try语句块中，用户自定义的异常后执行except块语句，变量 e 是用于创建Networkerror类的实例。 123class Networkerror(RuntimeError): def __init__(self, arg): self.args = arg 在你定义以上类后，你可以触发该异常，如下所示： 12345try: raise Networkerror("Bad hostname")except Networkerror,e: print e.args raise Networkerror("Bad hostname") # raise是用来抛出异常，不适用则不会中断程序]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python类的继承]]></title>
    <url>%2F2018%2F05%2F24%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E7%B1%BB%E7%9A%84%E7%BB%A7%E6%89%BF%2F</url>
    <content type="text"><![CDATA[概述 面向对象编程 (OOP) 语言的一个主要功能就是“继承”。继承是指这样一种能力：它可以使用现有类的所有功能，并在无需重新编写原来的类的情况下对这些功能进行扩展。 类的继承继承的定义12345678910111213141516171819class Person(object): # 定义一个父类 def talk(self): # 父类中的方法 print("person is talking....") class Chinese(Person): # 定义一个子类， 继承Person类 def walk(self): # 在子类中定义其自身的方法 print('is walking...') c = Chinese()c.talk() # 调用继承的Person类的方法c.walk() # 调用本身的方法 # 输出 person is talking....is walking... 构造函数的继承 如果我们要给实例 c 传参，我们就要使用到构造函数，那么构造函数该如何继承，同时子类中又如何定义自己的属性？ 继承类的构造方法： 经典类的写法： 父类名称.init(self,参数1，参数2，…) 新式类的写法：super(子类，self).init(参数1，参数2，….) 12345678910111213141516171819202122232425class Person(object): def __init__(self, name, age): self.name = name self.age = age self.weight = 'weight' def talk(self): print("person is talking....") class Chinese(Person): def __init__(self, name, age, language): # 先继承，在重构 Person.__init__(self, name, age) #继承父类的构造方法，也可以写成：super(Chinese,self).__init__(name,age) self.language = language # 定义类的本身属性 def walk(self): print('is walking...') class American(Person): pass c = Chinese('bigberg', 22, 'Chinese') 如果我们只是简单的在子类Chinese中定义一个构造函数，其实就是在重构。这样子类就不能继承父类的属性了。所以我们在定义子类的构造函数时，要先继承再构造，这样我们也能获取父类的属性了。 子类构造函数基础父类构造函数过程如下： 实例化对象c —-&gt; c 调用子类init() —- &gt; 子类init()继承父类init() —– &gt; 调用父类 init() 子类对父类方法的重写如果我们对基类/父类的方法需要修改，可以在子类中重构该方法。如下的talk()方法 1234567891011121314151617181920212223242526272829class Person(object): def __init__(self, name, age): self.name = name self.age = age self.weight = &apos;weight&apos; def talk(self): print(&quot;person is talking....&quot;) class Chinese(Person): def __init__(self, name, age, language): Person.__init__(self, name, age) self.language = language print(self.name, self.age, self.weight, self.language) def talk(self): # 子类 重构方法 print(&apos;%s is speaking chinese&apos; % self.name) def walk(self): print(&apos;is walking...&apos;) c = Chinese(&apos;bigberg&apos;, 22, &apos;Chinese&apos;)c.talk() # 输出bigberg 22 weight Chinesebigberg is speaking chinese 参考资料python类的继承]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python多进程]]></title>
    <url>%2F2017%2F05%2F21%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[基本使用在 multiprocessing 中，每一个进程都用一个 Process 类来表示。首先看下它的API 1Process([group [, target [, name [, args [, kwargs]]]]]) target 表示调用对象，你可以传入方法的名字 args 表示被调用对象的位置参数元组，比如target是函数a，他有两个参数m，n，那么args就传入(m, n)即可 kwargs 表示调用对象的字典 name 是别名，相当于给这个进程取一个名字 group 分组，实际上不使用 我们先用一个实例来感受一下： 123456789import multiprocessingdef process(num): print ('Process:', num)if __name__ == '__main__': for i in range(5): p = multiprocessing.Process(target=process, args=(i,)) p.start() 最简单的创建Process的过程如上所示，target传入函数名，args是函数的参数，是元组的形式，如果只有一个参数，那就是长度为1的元组。 然后调用start()方法即可启动多个进程了。 另外你还可以通过 cpu_count() 方法还有 active_children() 方法获取当前机器的 CPU 核心数量以及得到目前所有的运行的进程。 1234567891011121314151617import multiprocessingimport timedef process(num): time.sleep(num) print ('Process:', num)if __name__ == '__main__': for i in range(5): p = multiprocessing.Process(target=process, args=(i,)) p.start() print('CPU number:' + str(multiprocessing.cpu_count())) for p in multiprocessing.active_children(): print('Child process name: ' + p.name + ' id: ' + str(p.pid)) print('Process Ended') Lock并行输出结果会导致错位，我们可以通过Lock进行加锁 12345678910111213141516171819202122from multiprocessing import Process, Lockimport timeclass MyProcess(Process): def __init__(self, loop, lock): Process.__init__(self) self.loop = loop self.lock = lock def run(self): for count in range(self.loop): time.sleep(0.1) self.lock.acquire() print('Pid: ' + str(self.pid) + ' LoopCount: ' + str(count)) self.lock.release()if __name__ == '__main__': lock = Lock() for i in range(10, 15): p = MyProcess(i, lock) p.start() Python Queue模块详解Python Queue模块详解]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sublime]]></title>
    <url>%2F2017%2F05%2F18%2Fwindows%2Fsublime%2F</url>
    <content type="text"><![CDATA[快捷键与常用功能Ctrl+P ：Goto AnythingCtrl+P : 查找项目中的文件： 直接输入名称：在不同文件中切换，支持级联的目录模式 :：+ 行号：Ctrl+G 定位到具体的行。@：+ 符号：Ctrl+R 定位到具体的符号，例如：JS函数名，CSS选择器名。#：+ 关键字：Ctrl+; 匹配到具体的匹配的关键字。主要是模糊匹配。 Ctrl+shif+D 复制一行Ctrl+B 在sublime中执行python程序通常，我们将一个演示教程中的代码复制到终端时，会发生格式错乱导致示例代码无法运行。因此，复制到sublime中执行不失为一个好办法。输入快捷键ctrl+B可以快速的执行脚本并在sublime中观察到返回结果。 find in floder 批量修改替换右键点击相应的文件夹，选择find in floder，sublime会在文件夹内列出所有匹配到文字的所有文件，并替换为你所希望更换的内容。然后点击File -&gt; save all，会保存所有被修改的文件。 创建工程在菜单栏选择 Project -&gt; Save Project As 会将sublime当前状态(包括边栏选择的文件夹，当前打开的文件等)保存为工程在菜单栏选择 Project -&gt; Quick Switch Project 可以快速选择之前保存好的工程在菜单栏选择 Preferences -&gt; key Bindings,为快速打开工程设置快捷键,之后可以使用ctrl+alt+p对工程快速进行选择 123[ &#123;"keys": ["ctrl+alt+p"],"command": "prompt_select_workspace"&#125;] sublime配置文件个人正在使用的sublime配置文件，解决了文件名中文显示为□□□的问题，并且将制表符用4个空格代替，避免tab和space的混合使用(尤其针对python语言) 12345678910111213141516171819202122&#123; "bold_folder_labels": true, "dpi_scale": 1.0, "font_face": "Consolas", "font_size": 16, "highlight_line": true, "highlight_modified_tabs": true, "ignored_packages": [ "Vintage" ], "line_padding_bottom": 1, "line_padding_top": 1, "save_on_focus_lost": true, "show_encoding": true, "tab_size": 4, "translate_tabs_to_spaces": true, "trim_trailing_white_space_on_save": true, "word_wrap": false, "hot_exit": false, "remember_open_files": false&#125; 插件安装安装插件管理包 Package Control 打开Package Control的官方网页,点击右侧的 Install Now 按钮 复制对应版本 2.0或 3.0的代码段 Ctrl + ~ 打开Sublime Text控制台，将之前复制的代码粘贴到控制台里，按下“Enter”键 重启程序,点开菜单 Preferences 可见 Package Control 项，说明插件管理包已安装成功。 安装的两种方式以 ConvertToUTF8 插件安装为例： 功能说明 对于一些编码格式会导致中文乱码,ConcertToUTF8专为解决该问题而编写 ConvertToUTF8 能将除UTF8编码之外的其他编码文件在 Sublime Text 中转换成UTF8编码 在保存文件之后原文件的编码格式不会改变 安装方法 通过 Package Control 在线安装 菜单 Preferences -&gt; Package Control -&gt; :Install Package 由于网络等问题,可能会等待数秒或更长时间才会响应,待出现插件搜索框后,输入需要的插件名称 插件会自动安装,安装过程无任何提示,由于网络等问题,可能会等待数秒或更长时间 安装成功后,会弹出Package Control Messages页面,而packag setting中也会出现该插件名称 通过文件夹的方式本地安装 菜单 Preferences -&gt; Brower Package 打开Package本地文件 将解压好的插件包复制到这个 Packages 目录下 markdown解决方案markdown语法高亮 首先安装 Markdown Extended + Monokai Extended 这两个主题, 不知道为什么在package中无法搜索到这两个包，因此使用github下载 12git clone https://github.com/jonschlinkert/sublime-markdown-extended.git &quot;sublime-markdown-extended&quot;git clone https://github.com/jonschlinkert/sublime-monokai-extended.git &quot;sublime-monokai-extended&quot; 选择 Preference &gt; Color Scheme &gt; Monokai Extended 更换主题颜色为 Monokai Extended 打开一个 markdown 文件，选择 View &gt; Syntax &gt; open all with current ... &gt; Markdown Extended 设置 markdown 语法规则为 Markdown Extended 个人插件和使用方法AdvancedNewFile : 快速新建文件。假设有文件夹file。我们正在输入代码，又想在新的子目录下新建html文件的话用传统方式得很多步，新建目录，新建文件，保存等等等。 但是有了该插件之后，事情就变得简单了许多，只需要按下 Ctrl+ALT+N ，输入文件夹以及文件名，你就会看到如下效果:（回车，你会发现已经子目录下的文件已经新建完成了！） Terminal ：在Sublime Text直接打开命令行默认快捷键 Ctrl+Shift+T。 在windows下默认会打开Windows PowerShell，那界面简直丑到不行好吗！！ 根据上面的经验同样找到preference–&gt;package Settings–&gt;Terminal–&gt;Terminal Settings-users：进行下面的设置： 1234&#123; "terminal": "F:\\Program Files\\cmder\\Cmder.exe", "parameters": ["/START", "%CWD%"]&#125; 在sublime下运行python程序sublime 自带运行 python 程序功能，使用快捷键 ctrl + B 即可。 更美观的界面或者进入命令行交互模式还需要插件的支持，这里推荐 sublimeREPL 插件 在你写好的python文件的界面里(这点需要注意)，点击上方菜单栏的tools-&gt;sublimeREPL-&gt;python-&gt;python run current file，即可交互输入 使用快捷键运行程序 在 preferences–&gt; key binding–&gt; user 中输入以下内容: 123456789101112&#123; "keys": ["f5"], "caption": "Python - RUN current file", "command": "repl_open", "args": &#123; "type": "subprocess", "encoding": "utf8", "cmd": ["python", "-u", "$file_basename"], "cwd": "$file_path", "syntax": "Packages/Python/Python.tmLanguage", "external_id": "python", "extend_env": &#123;"PYTHONIOENCODING": "utf-8"&#125; &#125;&#125;]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python获取本机IP、mac地址、计算机名]]></title>
    <url>%2F2017%2F05%2F11%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E8%8E%B7%E5%8F%96%E6%9C%AC%E6%9C%BAIP%E3%80%81mac%E5%9C%B0%E5%9D%80%E3%80%81%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%90%8D%2F</url>
    <content type="text"><![CDATA[在Python中获取ip地址和在PHP中有很大不同，在php中往往比较简单。那再python中怎么做呢？我们先来看一下python 获得本机MAC地址： 1234import uuiddef get_mac_address(): mac=uuid.UUID(int = uuid.getnode()).hex[-12:] return ":".join([mac[e:e+2] for e in range(0,11,2)]) 下面再来看一下python获取IP的方法：使用socket 1234567import socket#获取本机电脑名myname = socket.getfqdn(socket.gethostname( ))#获取本机ipmyaddr = socket.gethostbyname(myname)print mynameprint myaddr 在linux下可用1234567891011121314151617181920import socketimport fcntlimport structdef get_ip_address(ifname): s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) return socket.inet_ntoa(fcntl.ioctl( s.fileno(), 0x8915, # SIOCGIFADDR struct.pack('256s', ifname[:15]) )[20:24])get_ip_address('lo')# '127.0.0.1'get_ip_address('eth0')# '38.113.228.130'get_ip_address('ppp0')# '123.163.166.00']]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos6/7安装tinyproxy]]></title>
    <url>%2F2017%2F05%2F09%2Flinux%2Fcentos6-7%E5%AE%89%E8%A3%85tinyproxy%2F</url>
    <content type="text"><![CDATA[安装tinyproxy123yum install -y epel-releaseyum updateyum -y install tinyproxy 配置tinyproxy修改Allow 127.0.0.1为自己IP，只允许自己使用，或者在Allow前面打#注释，允许任何IP都可以连接 1vi /etc/tinyproxy/tinyproxy.conf 启动Tinyproxy服务，并设置开机自启12345service tinyproxy restartchkconfig --level 345 tinyproxy on#centos7如下设置:systemctl restart tinyproxy.servicesystemctl enable tinyproxy.service 防火墙开放8888（或已经自定义）端口1234iptables -I INPUT -p tcp --dport 8888 -j ACCEPT#centos7如下设置:firewall-cmd --zone=public --add-port=8888/tcp --permanentfirewall-cmd --reload]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下vi命令速查手册]]></title>
    <url>%2F2017%2F05%2F09%2Flinux%2Flinux%E4%B8%8Bvi%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[进入vi的命令vi filename :打开或新建文件，并将光标置于第一行首vi +n filename ：打开文件，并将光标置于第n行首vi + filename ：打开文件，并将光标置于最后一行首vi +/pattern filename：打开文件，并将光标置于第一个与pattern匹配的串处vi -r filename ：在上次正用vi编辑时发生系统崩溃，恢复filenamevi filename….filename ：打开多个文件，依次进行编辑 移动光标类命令h ：光标左移一个字符l ：光标右移一个字符space：光标右移一个字符Backspace：光标左移一个字符k或Ctrl+p：光标上移一行j或Ctrl+n ：光标下移一行Enter ：光标下移一行w或W ：光标右移一个字至字首b或B ：光标左移一个字至字首e或E ：光标右移一个字至字尾) ：光标移至句尾( ：光标移至句首}：光标移至段落开头{：光标移至段落结尾nG：光标移至第n行首n+：光标下移n行n-：光标上移n行n$：光标移至第n行尾H ：光标移至屏幕顶行M ：光标移至屏幕中间行L ：光标移至屏幕最后行0：（注意是数字零）光标移至当前行首$：光标移至当前行尾 屏幕翻滚类命令Ctrl+u：向文件首翻半屏Ctrl+d：向文件尾翻半屏Ctrl+f：向文件尾翻一屏Ctrl＋b；向文件首翻一屏nz：将第n行滚至屏幕顶部，不指定n时将当前行滚至屏幕顶部。 插入文本类命令i ：在光标前I ：在当前行首a：光标后A：在当前行尾o：在当前行之下新开一行O：在当前行之上新开一行r：替换当前字符R：替换当前字符及其后的字符，直至按ESC键s：从当前光标位置处开始，以输入的文本替代指定数目的字符S：删除指定数目的行，并以所输入文本代替之ncw或nCW：修改指定数目的字nCC：修改指定数目的行 删除命令ndw或ndW：删除光标处开始及其后的n-1个字do：删至行首d$：删至行尾ndd：删除当前行及其后n-1行x或X：删除一个字符，x删除光标后的，而X删除光标前的Ctrl+u：删除输入方式下所输入的文本 搜索及替换命令/pattern：从光标开始处向文件尾搜索pattern?pattern：从光标开始处向文件首搜索patternn：在同一方向重复上一次搜索命令N：在反方向上重复上一次搜索命令：s/p1/p2/g：将当前行中所有p1均用p2替代：n1,n2s/p1/p2/g：将第n1至n2行中所有p1均用p2替代：g/p1/s//p2/g：将文件中所有p1均用p2替换 选项设置all：列出所有选项设置情况term：设置终端类型ignorance：在搜索中忽略大小写list：显示制表位(Ctrl+I)和行尾标志（$)number：显示行号report：显示由面向行的命令修改过的数目terse：显示简短的警告信息warn：在转到别的文件时若没保存当前文件则显示NO write信息nomagic：允许在搜索模式中，使用前面不带“\”的特殊字符nowrapscan：禁止vi在搜索到达文件两端时，又从另一端开始mesg：允许vi显示其他用户用write写到自己终端上的信息 最后行方式命令：n1,n2 co n3：将n1行到n2行之间的内容拷贝到第n3行下：n1,n2 m n3：将n1行到n2行之间的内容移至到第n3行下：n1,n2 d ：将n1行到n2行之间的内容删除：w ：保存当前文件：e filename：打开文件filename进行编辑：x：保存当前文件并退出：q：退出vi：q!：不保存文件并退出vi：!command：执行shell命令command：n1,n2 w!command：将文件中n1行至n2行的内容作为command的输入并执行之，若不指定n1，n2，则表示将整个文件内容作为command的输入：r!command：将命令command的输出结果放到当前行 寄存器操作“?nyy：将当前行及其下n行的内容保存到寄存器？中，其中?为一个字母，n为一个数字“?nyw：将当前行及其下n个字保存到寄存器？中，其中?为一个字母，n为一个数字“?nyl：将当前行及其下n个字符保存到寄存器？中，其中?为一个字母，n为一个数字“?p：取出寄存器？中的内容并将其放到光标位置处。这里？可以是一个字母，也可以是一个数字ndd：将当前行及其下共n行文本删除，并将所删内容放到1号删除寄存器中。 参考资料]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh原理及应用]]></title>
    <url>%2F2017%2F05%2F09%2Flinux%2Fssh%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1. 什么是SSH？简单说，SSH是一种网络协议，用于计算机之间的加密登录。如果一个用户从本地计算机，使用SSH协议登录另一台远程计算机，我们就可以认为，这种登录是安全的，即使被中途截获，密码也不会泄露。最早的时候，互联网通信都是明文通信，一旦被截获，内容就暴露无疑。1995年，芬兰学者Tatu Ylonen设计了SSH协议，将登录信息全部加密，成为互联网安全的一个基本解决方案，迅速在全世界获得推广，目前已经成为Linux系统的标准配置。需要指出的是，SSH只是一种协议，存在多种实现，既有商业实现，也有开源实现。本文针对的实现是OpenSSH，它是自由软件，应用非常广泛。此外，本文只讨论SSH在Linux Shell中的用法。如果要在Windows系统中使用SSH，会用到另一种软件PuTTY，这需要另文介绍。 2. 最基本的用法SSH主要用于远程登录。假定你要以用户名user，登录远程主机host，只要一条简单命令就可以了。 1$ ssh user@host 如果本地用户名与远程用户名一致，登录时可以省略用户名。 1$ ssh host SSH的默认端口是22，也就是说，你的登录请求会送进远程主机的22端口。使用p参数，可以修改这个端口。 1$ ssh -p 2222 user@host 上面这条命令表示，ssh直接连接远程主机的2222端口。 3. 公钥登录使用密码登录，每次都必须输入密码，非常麻烦。好在SSH还提供了公钥登录，可以省去输入密码的步骤。所谓”公钥登录”，原理很简单，就是用户将自己的公钥储存在远程主机上。登录的时候，远程主机会向用户发送一段随机字符串，用户用自己的私钥加密后，再发回来。远程主机用事先储存的公钥进行解密，如果成功，就证明用户是可信的，直接允许登录shell，不再要求密码。这种方法要求用户必须提供自己的公钥。如果没有现成的，可以直接用ssh-keygen生成一个： 1. 生成公钥 1$ ssh-keygen 运行上面的命令以后，系统会出现一系列提示，可以一路回车。其中有一个问题是，要不要对私钥设置口令（passphrase），如果担心私钥的安全，这里可以设置一个。运行结束以后，在 $HOME/.ssh/ 目录下，会新生成两个文件：id_rsa.pub 和 id_rsa。前者是你的公钥，后者是你的私钥。 这时再输入下面的命令，将公钥传送到远程主机host上面： 2. 发送公钥 1$ ssh-copy-id user@host 好了，从此你再登录，就不需要输入密码了。 4. scpscp用于本地主机和远程主机之间进行文件传输，常用的有以下命令： 1. 本地文件复制到远程复制文件 1scp -P 2222 local_filename root@host:remote_filename 复制目录 1scp -P 2222 -r local_folder root@host:remote_folder 2. 远程文件复制到本地复制文件 1scp -P 2222 root@host:remote_filename local_filename 复制目录 1scp -P 2222 -r root@host:remote_folder local_folder 3. 参数说明 -v 和大多数 linux 命令中的 -v 意思一样 , 用来显示进度 . 可以用来查看连接 , 认证 , 或是配置错误 . -C 使能压缩选项 . -P 选择端口 . 注意 -p 已经被 rcp 使用 . -4 强行使用 IPV4 地址 . -6 强行使用 IPV6 地址 . 4. 行为模式 如果目录san存在，则会将remoteIO目录移动到san目录下 123$ scp -r remoteIO root@192.168.71.195:/tmp/ztest/san$ ls sanremoteIO 如果目录san不存在，则会创建san目录，并将remoteIO内的内容复制到san目录下 123$ scp -r remoteIO root@192.168.71.195:/tmp/ztest/san$ ls sanconfig.yaml ... # remoteIO的内容 5. 参考资料SSH原理与运用（一）：远程登录]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[node基础教程之http模块]]></title>
    <url>%2F2017%2F05%2F08%2Fnodejs%2Fnode%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E4%B9%8Bhttp%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[第一个http服务器要开发HTTP服务器程序，从头处理TCP连接，解析HTTP是不现实的。这些工作实际上已经由Node.js自带的http模块完成了。应用程序并不直接和HTTP协议打交道，而是操作http模块提供的request和response对象。 request对象封装了HTTP请求，我们调用request对象的属性和方法就可以拿到所有HTTP请求的信息； response对象封装了HTTP响应，我们操作response对象的方法，就可以把HTTP响应返回给浏览器。 用Node.js实现一个HTTP服务器程序非常简单。我们来实现一个最简单的Web程序hello.js，它对于所有请求，都返回Hello world!： 123456789101112131415161718const http = require('http');const server = http.createServer((req, res) =&gt; &#123; // 回调函数接收request和response对象, // 获得HTTP请求的method和url: console.log(req.method + ': ' + req.url); // 将HTTP响应200写入response, 同时设置Content-Type: text/html: res.writeHead(200, &#123;'Content-Type': 'text/html'&#125;); // 将HTTP响应的正文写入response: res.end('&lt;h1&gt;Hello world!&lt;/h1&gt;');&#125;);//错误处理server.on('clientError', (err, socket) =&gt; &#123; socket.end('HTTP/1.1 400 Bad Request\r\n\r\n');&#125;);// 让服务器监听8080端口:server.listen(8080);console.log('Server is running at http://127.0.0.1:8080/'); 在命令提示符下运行该程序，可以看到以下输出： 12$ node hello.jsServer is running at http://127.0.0.1:8080/ 不要关闭命令提示符，直接打开浏览器输入http://localhost:8080，即可在浏览器看到服务器响应的内容Hello world： 同时，在命令提示符窗口，可以看到程序打印的请求信息： 12GET: /GET: /favicon.ico 这就是我们编写的第一个HTTP服务器程序！ 文件服务器我们需要使用 nodejs 提供的 url 模块对 req.url 进行解析，使用 fs 模块对文件进行处理 解析url12var url = require('url');console.log(url.parse('http://user:pass@host.com:8080/path/to/file?query=string#hash')); 结果如下： 12345678910111213Url &#123; protocol: 'http:', slashes: true, auth: 'user:pass', host: 'host.com:8080', port: '8080', hostname: 'host.com', hash: '#hash', search: '?query=string', query: 'query=string', pathname: '/path/to/file', path: '/path/to/file?query=string', href: 'http://user:pass@host.com:8080/path/to/file?query=string#hash' &#125; 构建文件服务器文件结构如下 12|-index.html|-file-server.js index.html 123456789&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;hello world!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; file-server.js 1234567891011121314151617181920212223242526272829const fs = require('fs'), url = require('url'), path = require('path'), http = require('http');const server = http.createServer((req, res) =&gt; &#123; //通过req.url得到本地对应的文件路径 const pathname = url.parse(req.url).pathname; const filepath = path.join(__dirname, pathname); fs.stat(filepath, (err, stats) =&gt; &#123; if (!err &amp;&amp; stats.isFile()) &#123; //如果文件存在并且未出现错误，读取文件并传给res console.log('200 ' + req.url); res.writeHead(200); fs.createReadStream(filepath).pipe(res); &#125; else &#123; //否则返回404错误 console.log('404 ' + req.url); res.writeHead(404); res.end('404 Not Found'); &#125; &#125;);&#125;);server.listen(8080);console.log('Server is running at http://127.0.0.1:8080/'); 当我们使用浏览器访问 http://127.0.0.1:8080/index.html 时，可以看到 hello world！ 字样，访问其他路径时，会得到 404 not found 的提示 发送请求get()get 方法用于发送 GET 请求,使用格式如下（详细用法件request） 1http.get(options[, callback]) 1234567891011121314151617181920function getTestPersonaLoginCredentials(callback) &#123; return http.get(&#123; host: 'personatestuser.org', path: '/email' &#125;, function(response) &#123; var body = ''; response.on('data', function(d) &#123; body += d; &#125;); response.on('end', function() &#123; var parsed = JSON.parse(body); callback(&#123; email: parsed.email, password: parsed.pass &#125;); &#125;); &#125;);&#125;, request()request方法用于发出HTTP请求，它的使用格式如下。 1http.request(options[, callback]) request方法的options参数，可以是一个对象，也可以是一个字符串。如果是字符串，就表示这是一个URL，Node内部就会自动调用url.parse()，处理这个参数。 options对象可以设置如下属性: host：HTTP请求所发往的域名或者IP地址，默认是localhost。 hostname：该属性会被url.parse()解析，优先级高于host。 port：远程服务器的端口，默认是80。 localAddress：本地网络接口。 socketPath：Unix网络套接字，格式为host:port或者socketPath。 method：指定HTTP请求的方法，格式为字符串，默认为GET。 path：指定HTTP请求的路径，默认为根路径（/）。可以在这个属性里面，指定查询字符串，比如/index.html?page=12。如果这个属性里面包含非法字符（比如空格），就会抛出一个错误。 headers：一个对象，包含了HTTP请求的头信息。 auth：一个代表HTTP基本认证的字符串user:password。 agent：控制缓存行为，如果HTTP请求使用了agent，则HTTP请求默认为Connection: keep-alive，它的可能值如下： undefined（默认）：对当前host和port，使用全局Agent。 Agent：一个对象，会传入agent属性。 false：不缓存连接，默认HTTP请求为Connection: close。 keepAlive：一个布尔值，表示是否保留socket供未来其他请求使用，默认等于false。 keepAliveMsecs：一个整数，当使用KeepAlive的时候，设置多久发送一个TCP KeepAlive包，使得连接不要被关闭。默认等于1000，只有keepAlive设为true的时候，该设置才有意义。]]></content>
      <tags>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http教程]]></title>
    <url>%2F2017%2F05%2F08%2F%E5%89%8D%E7%AB%AF%2Fhttp%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[HTTP教程HTTP协议（HyperText Transfer Protocol，超文本传输协议）是因特网上应用最为广泛的一种网络传输协议，所有的WWW文件都必须遵守这个标准。HTTP是一个基于TCP/IP通信协议来传递数据（HTML 文件, 图片文件, 查询结果等）。 HTTP消息结构客户端请求消息 客户端发送一个HTTP请求到服务器的请求消息包括以下格式：请求行（request line）、请求头部（header）、空行和请求数据四个部分组成HTTP响应也由四个部分组成，分别是：状态行、消息报头、空行和响应正文 客户端请求： 1234GET /hello.txt HTTP/1.1User-Agent: curl/7.16.3 libcurl/7.16.3 OpenSSL/0.9.7l zlib/1.2.3Host: www.example.comAccept-Language: en, mi 服务端响应: 123456789HTTP/1.1 200 OKDate: Mon, 27 Jul 2009 12:28:53 GMTServer: ApacheLast-Modified: Wed, 22 Jul 2009 19:15:56 GMTETag: &quot;34aa387-d-1568eb00&quot;Accept-Ranges: bytesContent-Length: 51Vary: Accept-EncodingContent-Type: text/plain HTTP请求方法根据HTTP标准，HTTP请求可以使用多种请求方法。 HTTP1.0定义了三种请求方法： GET, POST 和 HEAD方法。HTTP1.1新增了五种请求方法：OPTIONS, PUT, DELETE, TRACE 和 CONNECT 方法。 GET 请求指定的页面信息，并返回实体主体。 HEAD 类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 PUT 从客户端向服务器传送的数据取代指定的文档的内容。 DELETE 请求服务器删除指定的页面。 CONNECT HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 OPTIONS 允许客户端查看服务器的性能。 TRACE 回显服务器收到的请求，主要用于测试或诊断。]]></content>
      <tags>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP代理原理及实现]]></title>
    <url>%2F2017%2F05%2F08%2F%E5%89%8D%E7%AB%AF%2FHTTP%E4%BB%A3%E7%90%86%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[普通代理服务器普通代理服务器充当中间人的角色，对于连接到它的客户端来说，它是服务端；对于要连接的服务端来说，它是客户端。它就负责在两端之间来回传送 HTTP 报文。 1234567891011121314151617181920212223242526const http = require('http');const net = require('net');const url = require('url');function request(cReq, cRes) &#123; const u = url.parse(cReq.url); const options = &#123; hostname : u.hostname, port : u.port || 80, path : u.path, method : cReq.method, headers : cReq.headers &#125;; // 解析options，新建到服务端的请求 const pReq = http.request(options, function(pRes) &#123; cRes.writeHead(pRes.statusCode, pRes.headers); pRes.pipe(cRes); &#125;).on('error', function(e) &#123; cRes.end(); &#125;); // 把代理收到的请求转发给新建的请求 cReq.pipe(pReq);&#125;http.createServer().on('request', request).listen(8888, '0.0.0.0'); 以上代码运行后，会在本地 8888 端口开启 HTTP 代理服务，这个服务从请求报文中解析出请求 URL 和其他必要参数，新建到服务端的请求，并把代理收到的请求转发给新建的请求，最后再把服务端响应返回给浏览器。修改浏览器的 HTTP 代理为 127.0.0.1:8888 后再访问 HTTP 网站，代理可以正常工作。 这个代理提供的是 HTTP 服务，没办法承载 HTTPS 服务 ，HTTPS 代理需要使用隧道代理实现 隧道代理HTTP 客户端通过 CONNECT 方法请求隧道代理创建一条到达任意目的服务器和端口的 TCP 连接，并对客户端和服务器之间的后继数据进行盲转发。 123456789101112131415161718const http = require('http');const net = require('net');const url = require('url');function connect(cReq, cSock) &#123; const u = url.parse('http://' + cReq.url); const pSock = net.connect(u.port, u.hostname, function() &#123; cSock.write('HTTP/1.1 200 Connection Established\r\n\r\n'); pSock.pipe(cSock); &#125;).on('error', function(e) &#123; cSock.end(); &#125;); cSock.pipe(pSock);&#125;http.createServer().on('connect', connect).listen(8888, '0.0.0.0'); 隧道代理主要用于 https ，因此我们将上述两段代码合二为一,同时处理 http 与 https 请求 123456789101112131415161718192021222324252627282930313233343536373839404142var http = require('http');var net = require('net');var url = require('url');function request(cReq, cRes) &#123; var u = url.parse(cReq.url); var options = &#123; hostname : u.hostname, port : u.port || 80, path : u.path, method : cReq.method, headers : cReq.headers &#125;; var pReq = http.request(options, function(pRes) &#123; cRes.writeHead(pRes.statusCode, pRes.headers); pRes.pipe(cRes); &#125;).on('error', function(e) &#123; cRes.end(); &#125;); cReq.pipe(pReq);&#125;function connect(cReq, cSock) &#123; var u = url.parse('http://' + cReq.url); var pSock = net.connect(u.port, u.hostname, function() &#123; cSock.write('HTTP/1.1 200 Connection Established\r\n\r\n'); pSock.pipe(cSock); &#125;).on('error', function(e) &#123; cSock.end(); &#125;); cSock.pipe(pSock);&#125;http.createServer() .on('request', request) .on('connect', connect) .listen(8888, '0.0.0.0'); 参考资料HTTP 代理原理及实现]]></content>
  </entry>
  <entry>
    <title><![CDATA[sequelize和mysql对照]]></title>
    <url>%2F2017%2F05%2F04%2F%E5%89%8D%E7%AB%AF%2Fsequelize%E5%92%8Cmysql%E5%AF%B9%E7%85%A7%2F</url>
    <content type="text"><![CDATA[建立数据库连接1234567891011121314var sequelize = new Sequelize( 'dbname', // 数据库名 'username', // 用户名 'password', // 用户密码 &#123; 'dialect': 'mysql', // 数据库使用mysql 'host': 'localhost', // 数据库服务器ip 'port': 3306, // 数据库服务器端口 'define': &#123; // 字段以下划线（_）来分割（默认是驼峰命名风格） 'underscored': true &#125; &#125;); 定义单张表123456789101112131415161718192021var User = sequelize.define( // 默认表名（一般这里写单数），生成时会自动转换成复数形式 // 这个值还会作为访问模型相关的模型时的属性名，所以建议用小写形式 'user', // 字段定义（主键、created_at、updated_at默认包含，不用特殊定义） &#123; 'emp_id': &#123; 'type': Sequelize.CHAR(10), // 字段类型 'allowNull': false, // 是否允许为NULL 'unique': true // 字段是否UNIQUE(唯一) &#125;, 'nick': &#123; 'type': Sequelize.CHAR(10), 'allowNull': false &#125;, 'department': &#123; 'type': Sequelize.STRING(64), 'allowNull': true &#125; &#125;); 单表增删改查通过Sequelize获取的模型对象都是一个DAO（Data Access Object）对象，这些对象会拥有许多操作数据库表的实例对象方法（比如：save、update、destroy等），需要获取“干净”的JSON对象可以调用get({‘plain’: true})。 通过模型的类方法可以获取模型对象（比如：findById、findAll等）。 增Sequelize： 12345678910111213141516// 方法1：build后对象只存在于内存中，调用save后才操作dbvar user = User.build(&#123; 'emp_id': '1', 'nick': '小红', 'department': '技术部'&#125;);user = await user.save();console.log(user.get(&#123;'plain': true&#125;));// 方法2：直接操作dbvar user = await User.create(&#123; 'emp_id': '2', 'nick': '小明', 'department': '技术部'&#125;);console.log(user.get(&#123;'plain': true&#125;)); Sequelize会为主键 id 设置 DEFAULT 值来让数据库产生自增值，还将当前时间设置成了 created_at 和 updated_at 字段，非常方便。 改12345678910// 方法1：操作对象属性（不会操作db），调用save后操作dbuser.nick = '小白';user = await user.save();console.log(user.get(&#123;'plain': true&#125;));// 方法2：直接update操作dbuser = await user.update(&#123; 'nick': '小白白'&#125;);console.log(user.get(&#123;'plain': true&#125;)); 更新操作时，Sequelize将将当前时间设置成了updated_at，非常方便。 如果想限制更新属性的白名单，可以这样写： 12345678910// 方法1user.emp_id = '33';user.nick = '小白';user = await user.save(&#123;'fields': ['nick']&#125;);// 方法2user = await user.update( &#123;'emp_id': '33', 'nick': '小白'&#125;, &#123;'fields': ['nick']&#125;&#125;); 这样就只会更新nick字段，而emp_id会被忽略。这种方法在对表单提交过来的一大推数据中只更新某些属性的时候比较有用。 删1await user.destroy(); 这里有个特殊的地方是，如果我们开启了paranoid（偏执）模式，destroy的时候不会执行DELETE语句，而是执行一个UPDATE语句将deleted_at字段设置为当前时间（一开始此字段值为NULL）。我们可以使用user.destroy({force: true})来强制删除，从而执行DELETE语句进行物理删除。 查查全部 12var users = await User.findAll();console.log(users); 限制字段 1234var users = await User.findAll(&#123; 'attributes': ['emp_id', 'nick']&#125;);console.log(users); 字段重命名 123456var users = await User.findAll(&#123; 'attributes': [ 'emp_id', ['nick', 'user_nick'] ]&#125;);console.log(users); where子句Sequelize的where配置项基本上完全支持了SQL的where子句的功能，非常强大。我们一步步来进行介绍。 基本条件12345678var users = await User.findAll(&#123; 'where': &#123; 'id': [1, 2, 3], 'nick': 'a', 'department': null &#125;&#125;);console.log(users); 操作符操作符是对某个字段的进一步约束，可以有多个（对同一个字段的多个操作符会被转化为AND）。 Sequelize： 12345678910111213141516171819202122232425262728var users = await User.findAll(&#123; 'where': &#123; 'id': &#123; '$eq': 1, // id = 1 '$ne': 2, // id != 2 '$gt': 6, // id &gt; 6 '$gte': 6, // id &gt;= 6 '$lt': 10, // id &lt; 10 '$lte': 10, // id &lt;= 10 '$between': [6, 10], // id BETWEEN 6 AND 10 '$notBetween': [11, 15], // id NOT BETWEEN 11 AND 15 '$in': [1, 2], // id IN (1, 2) '$notIn': [3, 4] // id NOT IN (3, 4) &#125;, 'nick': &#123; '$like': '%a%', // nick LIKE '%a%' '$notLike': '%a' // nick NOT LIKE '%a' &#125;, 'updated_at': &#123; '$eq': null, // updated_at IS NULL '$ne': null // created_at IS NOT NULL &#125; &#125;&#125;); 条件上面我们说的条件查询，都是AND查询，Sequelize同时也支持OR、NOT、甚至多种条件的联合查询。 AND条件 12345678var users = await User.findAll(&#123; 'where': &#123; '$and': [ &#123;'id': [1, 2]&#125;, &#123;'nick': null&#125; ] &#125;&#125;); OR条件 12345678var users = await User.findAll(&#123; 'where': &#123; '$or': [ &#123;'id': [1, 2]&#125;, &#123;'nick': null&#125; ] &#125;&#125;); NOT条件 12345678var users = await User.findAll(&#123; 'where': &#123; '$not': [ &#123;'id': [1, 2]&#125;, &#123;'nick': null&#125; ] &#125;&#125;); 批量操作插入 1234567var users = yield User.bulkCreate( [ &#123;'emp_id': 'a', 'nick': 'a'&#125;, &#123;'emp_id': 'b', 'nick': 'b'&#125;, &#123;'emp_id': 'c', 'nick': 'c'&#125; ]); 这里需要注意，返回的users数组里面每个对象的id值会是null。如果需要id值，可以重新取下数据。 更新 12345678var affectedRows = yield User.update( &#123;'nick': 'hhhh'&#125;, &#123; 'where': &#123; 'id': [2, 3, 4] &#125; &#125;); 删除 123var affectedRows = yield User.destroy(&#123; 'where': &#123;'id': [2, 3, 4]&#125;&#125;); 关系关系一般有三种：一对一、一对多、多对多。Sequelize提供了清晰易用的接口来定义关系、进行表间的操作。 一对一模型定义1234567891011121314151617181920212223242526var User = sequelize.define('user', &#123; 'emp_id': &#123; 'type': Sequelize.CHAR(10), 'allowNull': false, 'unique': true &#125; &#125;);var Account = sequelize.define('account', &#123; 'email': &#123; 'type': Sequelize.CHAR(20), 'allowNull': false &#125; &#125;);/* * User的实例对象将拥有getAccount、setAccount、addAccount方法 */User.hasOne(Account);/* * Account的实例对象将拥有getUser、setUser、addUser方法 */Account.belongsTo(User); 可以看到，这种关系中外键user_id加在了Account上。另外，Sequelize还给我们生成了外键约束。 一般来说，外键约束在有些自己定制的数据库系统里面是禁止的，因为会带来一些性能问题。所以，建表的SQL一般就去掉约束，同时给外键加一个索引（加速查询），数据的一致性就靠应用层来保证了。 关系操作增 123var user = yield User.create(&#123;'emp_id': '1'&#125;);var account = user.createAccount(&#123;'email': 'a'&#125;);console.log(account.get(&#123;'plain': true&#125;)); 改 1234var anotherAccount = yield Account.create(&#123;'email': 'b'&#125;);console.log(anotherAccount);anotherAccount = yield user.setAccount(anotherAccount);console.log(anotherAccount); 删 1yield user.setAccount(null); 查 12var account = yield user.getAccount();console.log(account); 这里就是调用user的getAccount方法，根据外键来获取对应的account。 但是其实我们用面向对象的思维来思考应该是获取user的时候就能通过user.account的方式来访问account对象。这可以通过Sequelize的eager loading（急加载，和懒加载相反）来实现。 eager loading的含义是说，取一个模型的时候，同时也把相关的模型数据也给我取过来（我很着急，不能按默认那种取一个模型就取一个模型的方式，我还要更多）。方法如下： 1234567891011121314151617var user = yield User.findById(1, &#123; 'include': [Account]&#125;);console.log(user.get(&#123;'plain': true&#125;));/* * 输出类似： &#123; id: 1, emp_id: '1', created_at: Tue Nov 03 2015 15:25:27 GMT+0800 (CST), updated_at: Tue Nov 03 2015 15:25:27 GMT+0800 (CST), account: &#123; id: 2, email: 'b', created_at: Tue Nov 03 2015 15:25:27 GMT+0800 (CST), updated_at: Tue Nov 03 2015 15:25:27 GMT+0800 (CST), user_id: 1 &#125; &#125; */ 可以看到，我们对2个表进行了一个外联接，从而在取user的同时也获取到了account。 其他补充说明 如果我们重复调用user.createAccount方法，实际上会在数据库里面生成多条user_id一样的数据，并不是真正的一对一。 所以，在应用层保证一致性时，就需要我们遵循良好的编码约定。新增就用user.createAccount，更改就用user.setAccount。 也可以给user_id加一个UNIQUE约束，在数据库层面保证一致性，这时就需要做好try/catch，发生插入异常的时候能够知道是因为插入了多个account。 另外，我们上面都是使用user来对account进行操作。实际上反向操作也是可以的，这是因为我们定义了Account.belongsTo(User)。在Sequelize里面定义关系时，关系的调用方会获得相关的“关系”方法，一般为了两边都能操作，会同时定义双向关系（这里双向关系指的是模型层面，并不会在数据库表中出现两个表都加上外键的情况，请放心）。 一对多模型定义1234567891011121314151617181920212223242526var User = sequelize.define('user', &#123; 'emp_id': &#123; 'type': Sequelize.CHAR(10), 'allowNull': false, 'unique': true &#125; &#125;);var Note = sequelize.define('note', &#123; 'title': &#123; 'type': Sequelize.CHAR(64), 'allowNull': false &#125; &#125;);/* * User的实例对象将拥有getNotes、setNotes、addNote、createNote、removeNote、hasNote方法 */User.hasMany(Note);/* * Note的实例对象将拥有getUser、setUser、createUser方法 */Note.belongsTo(User); 关系操作增 12345678//方法1var user = yield User.create(&#123;'emp_id': '1'&#125;);var note = yield user.createNote(&#123;'title': 'a'&#125;);console.log(note);//方法2var user = yield User.create(&#123;'emp_id': '1'&#125;);var note = yield Note.create(&#123;'title': 'b'&#125;);yield user.addNote(note); 改 123456789// 为user增加note1、note2var user = yield User.create(&#123;'emp_id': '1'&#125;);var note1 = yield user.createNote(&#123;'title': 'a'&#125;);var note2 = yield user.createNote(&#123;'title': 'b'&#125;);// 先创建note3、note4var note3 = yield Note.create(&#123;'title': 'c'&#125;);var note4 = yield Note.create(&#123;'title': 'd'&#125;);// user拥有的note更改为note3、note4yield user.setNotes([note3, note4]); 删 1yield user.removeNote(note); 查 情况1 查询user的所有满足条件的note数据。 12345678910var notes = yield user.getNotes(&#123; 'where': &#123; 'title': &#123; '$like': '%css%' &#125; &#125;&#125;);notes.forEach(function(note) &#123; console.log(note);&#125;); 情况2 查询所有满足条件的note，同时获取note属于哪个user。 123456789101112var notes = yield Note.findAll(&#123; 'include': [User], 'where': &#123; 'title': &#123; '$like': '%css%' &#125; &#125;&#125;);notes.forEach(function(note) &#123; // note属于哪个user可以通过note.user访问 console.log(note);&#125;); 情况3 查询所有满足条件的user，同时获取该user所有满足条件的note。 123456789101112var users = yield User.findAll(&#123; 'include': [Note], 'where': &#123; 'created_at': &#123; '$lt': new Date() &#125; &#125;&#125;);users.forEach(function(user) &#123; // user的notes可以通过user.notes访问 console.log(user);&#125;); 多对多关系在多对多关系中，必须要额外一张关系表来将2个表进行关联，这张表可以是单纯的一个关系表，也可以是一个实际的模型（含有自己的额外属性来描述关系）。我比较喜欢用一个模型的方式，这样方便以后做扩展。 模型定义123456789101112131415161718192021222324252627282930var Note = sequelize.define('note', &#123; 'title': &#123; 'type': Sequelize.CHAR(64), 'allowNull': false &#125; &#125;);var Tag = sequelize.define('tag', &#123; 'name': &#123; 'type': Sequelize.CHAR(64), 'allowNull': false, 'unique': true &#125; &#125;);var Tagging = sequelize.define('tagging', &#123; 'type': &#123; 'type': Sequelize.INTEGER(), 'allowNull': false &#125; &#125;);// Note的实例拥有getTags、setTags、addTag、addTags、createTag、removeTag、hasTag方法Note.belongsToMany(Tag, &#123;'through': Tagging&#125;);// Tag的实例拥有getNotes、setNotes、addNote、addNotes、createNote、removeNote、hasNote方法Tag.belongsToMany(Note, &#123;'through': Tagging&#125;); 增 方法1 12var note = yield Note.create(&#123;'title': 'note'&#125;);yield note.createTag(&#123;'name': 'tag'&#125;, &#123;'type': 0&#125;); 方法2 123var note = yield Note.create(&#123;'title': 'note'&#125;);var tag = yield Tag.create(&#123;'name': 'tag'&#125;);yield note.addTag(tag, &#123;'type': 1&#125;); 方法3 1234var note = yield Note.create(&#123;'title': 'note'&#125;);var tag1 = yield Tag.create(&#123;'name': 'tag1'&#125;);var tag2 = yield Tag.create(&#123;'name': 'tag2'&#125;);yield note.addTags([tag1, tag2], &#123;'type': 2&#125;); 改 123456789// 先添加几个tagvar note = yield Note.create(&#123;'title': 'note'&#125;);var tag1 = yield Tag.create(&#123;'name': 'tag1'&#125;);var tag2 = yield Tag.create(&#123;'name': 'tag2'&#125;);yield note.addTags([tag1, tag2], &#123;'type': 2&#125;);// 将tag改掉var tag3 = yield Tag.create(&#123;'name': 'tag3'&#125;);var tag4 = yield Tag.create(&#123;'name': 'tag4'&#125;);yield note.setTags([tag3, tag4], &#123;'type': 3&#125;); 删 123456789101112// 先添加几个tagvar note = yield Note.create(&#123;'title': 'note'&#125;);var tag1 = yield Tag.create(&#123;'name': 'tag1'&#125;);var tag2 = yield Tag.create(&#123;'name': 'tag2'&#125;);var tag3 = yield Tag.create(&#123;'name': 'tag2'&#125;);yield note.addTags([tag1, tag2, tag3], &#123;'type': 2&#125;);// 删除一个yield note.removeTag(tag1);// 全部删除yield note.setTags([]); 删除一个很简单，直接将关系表中的数据删除。 全部删除时，首先需要查出关系表中note_id对应的所有数据，然后一次删掉。 查 情况1 查询note所有满足条件的tag。 1234567var tags = yield note.getTags(&#123; //这里可以对tags进行where&#125;);tags.forEach(function(tag) &#123; // 关系模型可以通过tag.tagging来访问 console.log(tag);&#125;); 情况2 查询所有满足条件的tag，同时获取每个tag所在的note。 12345678910111213var tags = yield Tag.findAll(&#123; 'include': [ &#123; 'model': Note // 这里可以对notes进行where &#125; ] // 这里可以对tags进行where&#125;);tags.forEach(function(tag) &#123; // tag的notes可以通过tag.notes访问，关系模型可以通过tag.notes[0].tagging访问 console.log(tag);&#125;); 情况3 查询所有满足条件的note，同时获取每个note所有满足条件的tag。 12345678910111213var notes = yield Note.findAll(&#123; 'include': [ &#123; 'model': Tag // 这里可以对tags进行where &#125; ] // 这里可以对notes进行where&#125;);notes.forEach(function(note) &#123; // note的tags可以通过note.tags访问，关系模型通过note.tags[0].tagging访问 console.log(note);&#125;); 参考资料sequelize和mysql对照]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[koa2学习系列教程7 koa与REST规范]]></title>
    <url>%2F2017%2F05%2F03%2Fkoa%2Fkoa2%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B7koa%E4%B8%8EREST%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[hello RESTREST描述的是在网络中client和server的一种交互形式，它要求我们用 URL 定位资源，用 HTTP 动词（GET,POST,DELETE,DETC）描述操作。 12345rest-hello|-controller| |-api.js # 符合 REST 规范的 product 信息管理|-app.js|-controller.js 我们在 api.js 内实现 REST 规范，用于管理 product 信息，当 GET 请求 /api/products 时，返回包含product信息的 json 字符串，当 POST 请求 /api/products 时，将新产品添加到产品列表。信息交流统一以 json 格式进行 api.js 1234567891011121314151617181920212223242526272829//用于模拟数据库，储存products产品信息，简化流程var products = [&#123; name: 'iPhone', price: 6999&#125;, &#123; name: 'Kindle', price: 999&#125;];module.exports = &#123; //获取商品信息 'GET /api/products': async (ctx, next) =&gt; &#123; ctx.response.type = 'application/json'; ctx.response.body = &#123; products: products &#125;; &#125;, //新增商品 'POST /api/products': async (ctx, next) =&gt; &#123; var p = &#123; name: ctx.request.body.name, price: ctx.request.body.price &#125;; products.push(p); ctx.response.type = 'application/json'; ctx.response.body = p; &#125;&#125;; REST规范http请求规范REST 规范定义了资源的通用访问格式，虽然它不是一个强制要求，但遵守该规范可以让人易于理解。 GET 用于获取资源 POST 用于新建资源 PUT 用于更新资源 DELETE 用于删除资源 例如，商品Product就是一种资源。获取所有Product的URL如下： 1GET /api/products 而获取某个指定的Product，例如，id为123的Product，其URL如下： 1GET /api/products/123 新建一个Product使用POST请求，JSON数据包含在body中，URL如下： 1POST /api/products 更新一个Product使用PUT请求，例如，更新id为123的Product，其URL如下： 1PUT /api/products/123 删除一个Product使用DELETE请求，例如，删除id为123的Product，其URL如下： 1DELETE /api/products/123 资源还可以按层次组织。例如，获取某个Product的所有评论，使用： 1GET /api/products/123/reviews 当我们只需要获取部分数据时，可通过参数限制返回的结果集，例如，返回第2页评论，每页10项，按时间排序： 1GET /api/products/123/reviews?page=2&amp;size=10&amp;sort=time URL 与数据通信格式约定在实际工程中，一个Web应用既有REST，还有MVC，可能还需要集成其他第三方系统。如何组织URL？ 为了利于开发与实际应用，我们进行如下规定: REST API的返回值全部是object对象，而不是简单的number、boolean、null或者数组； REST API必须使用前缀/api/ 封装 ctx.rest() 输出 json 数据定义 rest.restify 中间件每次输出 json 数据时，都要使用 ctx.response.type = &#39;application/json&#39;; 不够又优雅，我们可以可以通过一个 middleware 给 ctx 添加一个 rest() 方法，直接输出JSON数据 12345678910111213141516module.exports = &#123; restify: (pathPrefix) =&gt; &#123; pathPrefix = pathPrefix || '/api/'; return async (ctx, next) =&gt; &#123; if (ctx.request.path.startsWith(pathPrefix)) &#123; console.log(`Process API $&#123;ctx.request.method&#125; $&#123;ctx.request.url&#125;...`); ctx.rest = (data) =&gt; &#123; ctx.response.type = 'application/json'; ctx.response.body = data; &#125; &#125; else &#123; await next(); &#125; &#125;; &#125;&#125;; 此后，输出json数据时，使用 ctx.rest(data) 即可 使用 rest.restify 中间件在 app.js 中调用中间件 12const rest = require('./rest');app.use(rest.restify()); 在 api.js 中使用 ctx.rest(data) 输出json数据 1234ctx.rest(&#123;products: products&#125;)//等价于ctx.response.type = 'application/json';ctx.response.body = &#123;products: products&#125;; 错误处理两种错误类型在涉及到REST API的错误时，我们必须先意识到，客户端会遇到两种类型的REST API错误。 403，404，500等错误 这些错误实际上是HTTP请求可能发生的错误。REST请求只是一种请求类型和响应类型均为JSON的HTTP请求，因此，这些错误在REST请求中也会发生。针对这种类型的错误，客户端除了提示用户“出现了网络错误，稍后重试”以外，并无法获得具体的错误信息。 业务逻辑错误 例如，输入了不合法的Email地址，试图删除一个不存在的Product，等等。这种类型的错误完全可以通过JSON返回给客户端，这样，客户端可以根据错误信息提示用户“Email不合法”等，以便用户修复后重新请求API。 错误响应第一类的错误实际上客户端可以识别，并且我们也无法操控HTTP服务器的错误码。 第二类的错误信息是一个JSON字符串，例如： 1234&#123; "code": "10000", "message": "Bad email address"&#125; 对于第二类错误的 HTTP 返回码，我们做出如下约定：正确的REST响应使用 200，对错误的REST响应使用 400 但是，要注意，绝不能混合其他HTTP错误码。例如，使用401响应“登录失败”，使用403响应“权限不够”。这会使客户端无法有效识别HTTP错误码和业务错误，，其原因在于HTTP协议定义的错误码十分偏向底层，而REST API属于“高层”协议，不应该复用底层的错误码。 定义错误码我们约定使用字符串作为错误码。原因在于，使用数字作为错误码时，API提供者需要维护一份错误码代码说明表，并且，该文档必须时刻与API发布同步，否则，客户端开发者遇到一个文档上没有写明的错误码，就完全不知道发生了什么错误。 我们定义的REST API错误格式如下： 1234&#123; &quot;code&quot;: &quot;错误代码&quot;, &quot;message&quot;: &quot;错误描述信息&quot;&#125; 其中，错误代码命名规范为大类:子类，例如，口令不匹配的登录错误代码为 auth:bad_password，用户名不存在的登录错误代码为 auth:user_not_found。这样，客户端既可以简单匹配某个类别的错误，也可以精确匹配某个特定的错误。 返回错误使用ctx.rest()返回错误 如果一个REST异步函数想要返回错误，一个直观的想法是调用ctx.rest()： 12345678910user = processLogin(username, password);if (user != null) &#123; ctx.rest(user);&#125; else &#123; ctx.response.status = 400; ctx.rest(&#123; code: 'auth:user_not_found', message: 'user not found' &#125;);&#125; 这种方式不好，因为控制流程会混乱，而且，错误只能在Controller函数中输出。 使用throw语句抛出错误 更好的方式是异步函数直接用throw语句抛出错误，让middleware去处理错误： 123456user = processLogin(username, password);if (user != null) &#123; ctx.rest(user);&#125; else &#123; throw new APIError('auth:user_not_found', 'user not found');&#125; 这种方式可以在异步函数的任何地方抛出错误，包括调用的子函数内部。 我们只需要稍稍改写一个middleware就可以处理错误： 12345678910111213141516171819202122232425262728293031module.exports = &#123; APIError: function (code, message) &#123; this.code = code || 'internal:unknown_error'; this.message = message || ''; &#125;, restify: (pathPrefix) =&gt; &#123; pathPrefix = pathPrefix || '/api/'; return async (ctx, next) =&gt; &#123; if (ctx.request.path.startsWith(pathPrefix)) &#123; // 绑定rest()方法: ctx.rest = (data) =&gt; &#123; ctx.response.type = 'application/json'; ctx.response.body = data; &#125; try &#123; await next(); &#125; catch (e) &#123; // 返回错误: ctx.response.status = 400; ctx.response.type = 'application/json'; ctx.response.body = &#123; code: e.code || 'internal:unknown_error', message: e.message || '' &#125;; &#125; &#125; else &#123; await next(); &#125; &#125;; &#125;&#125;; 这个错误处理的好处在于，不但简化了Controller的错误处理（只需要throw，其他不管），并且，在遇到非APIError的错误时，自动转换错误码为internal:unknown_error。 受益于async/await语法，我们在middleware中可以直接用try…catch捕获异常。如果是callback模式，就无法用try…catch捕获，代码结构将混乱得多。 最后，顺便把APIError这个对象export出去。 抛出错误 我们在 api.js 中，通过 throw new APIError() 返回错误： 1234567891011module.exports = &#123; 'DELETE /api/products/:id': async (ctx, next) =&gt; &#123; console.log(`delete product $&#123;ctx.params.id&#125;...`); var p = products.deleteProduct(ctx.params.id); if (p) &#123; ctx.rest(p); &#125; else &#123; throw new APIError('product:not_found', 'product not found by id.'); &#125; &#125;&#125;]]></content>
      <tags>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git速查手册]]></title>
    <url>%2F2017%2F05%2F02%2Fgit%E5%AD%A6%E4%B9%A0%2Fgit%E9%80%9F%E6%9F%A5%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[git-ssh 配置和使用使用https的方式push文件，每次都需要输入用户名以及密码，使用起来繁琐麻烦，因此可以通过配置ssh的方式快捷进行push操作 设置Git的user name和email：(如果是第一次的话) 12$ git config --global user.name "gaianote"$ git config --global user.email "gaianote@163.com" 生成密钥 1$ ssh-keygen -t rsa -C "gaianote@163.com" 连续3个回车。如果不需要密码的话。最后得到了两个文件：id_rsa和id_rsa.pub。如果不是第一次，就选择overwrite. 添加密钥到ssh-agent 确保 ssh-agent 是可用的。ssh-agent是一种控制用来保存公钥身份验证所使用的私钥的程序，其实ssh-agent就是一个密钥管理器，运行ssh-agent以后，使用ssh-add将私钥交给ssh-agent保管，其他程序需要身份验证的时候可以将验证申请交给ssh-agent来完成整个认证过程。 123# start the ssh-agent in the background$ eval &quot;$(ssh-agent -s)&quot;Agent pid 59566 添加生成的 SSH key 到 ssh-agent。 1$ ssh-add ~/.ssh/id_rsa 登陆Github, 添加 ssh 。 把id_rsa.pub文件里的内容复制到SSH Keys这里 测试： 1$ ssh -T git@github.com 你将会看到： 123The authenticity of host &apos;github.com (207.97.227.239)&apos; can&apos;t be established.RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.Are you sure you want to continue connecting (yes/no)? 选择 yes 1Hi gaianote! You&apos;ve successfully authenticated, but GitHub does not provide shell access. 如果看到Hi后面是你的用户名，就说明成功了。 修改.git文件夹下config中的url。(.git文件夹在项目根目录，它是默认隐藏的) 修改前 123[remote &quot;origin&quot;]url = https://github.com/gaianote/gaianote.github.io.gitfetch = +refs/heads/*:refs/remotes/origin/* 修改后 123[remote &quot;origin&quot;]url = git@github.com:gaianote/gaianote.github.io.gitfetch = +refs/heads/*:refs/remotes/origin/* 忽略不想提交的文件当一个文件提交了，但是又更新了 .gitignore 文件，可以使用如下方式删除缓存，再重新提交 1git rm --cached file_path 示例，项目文件结构如下： 12345learn-js|-node |-.gitignore |-koa2 |-node_moudles 我未创建 .gitignore 文件便进行提交，之后希望忽略 node_moudles ，只需进行如下操作 1git rm -r --cached node_moudles git单次push内容过大http.postBuffer默认上限为1M,当你修改的内容超过这个上限时，git push 就会报错： 1fatal: The remote end hung up unexpectedly 解决方法是在 .git/config 文件中加入: 12[http]postBuffer = 524288000 或者: 1git config http.postBuffer 524288000]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[node模块与commonjs规范]]></title>
    <url>%2F2017%2F04%2F28%2Fnodejs%2Fnode%E6%A8%A1%E5%9D%97%E4%B8%8Ecommonjs%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[Node应用由模块组成，采用CommonJS模块规范。 根据这个规范，每个文件就是一个模块，有自己的作用域。在一个文件里面定义的变量、函数、类，都是私有的，对其他文件不可见。 定义模块12345678910// example.jsvar invisible = function () &#123; console.log("invisible");&#125;exports.message = "hi";exports.say = function () &#123; console.log(message);&#125; 运行下面的命令，可以输出exports对象。 123456var example = require('./example.js');example// &#123;// message: "hi",// say: [Function]// &#125; 如果模块输出的是一个函数，那就不能定义在exports对象上面，而要定义在module.exports变量上面。 12345module.exports = function () &#123; console.log("hello world")&#125;require('./example2.js')() 上面代码中，require命令调用自身，等于是执行module.exports，因此会输出 hello world。 加载规则require命令的基本功能是，读入并执行一个JavaScript文件，然后返回该模块的exports对象。如果没有发现指定模块，会报错。 require 路径规则require命令用于加载文件，后缀名默认为.js。 123var foo = require('foo');// 等同于var foo = require('foo.js'); 根据参数的不同格式，require命令去不同路径寻找模块文件。 如果参数字符串以“/”开头，则表示加载的是一个位于绝对路径的模块文件。比如，require(‘/home/marco/foo.js’)将加载/home/marco/foo.js。 如果参数字符串以“./”开头，则表示加载的是一个位于相对路径（跟当前执行脚本的位置相比）的模块文件。比如，require(‘./circle’)将加载当前脚本同一目录的circle.js。 如果参数字符串不以“./“或”/“开头，则表示加载的是一个默认提供的核心模块（位于Node的系统安装目录中），或者一个位于各级node_modules目录的已安装模块（全局安装或局部安装）。 引入的同时执行了被引入模块son.js 1234console.log(msg)module.exports = function () &#123; console.log(msg)&#125; father.js 123msg = 'hello world'const foo = require('./child')foo() 输出结果为: 12hello worldhello world 模块之间变量不共享]]></content>
      <tags>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[koa2学习系列教程6 koa-static静态服务器的搭建]]></title>
    <url>%2F2017%2F04%2F28%2Fkoa%2Fkoa2%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B6koa-static%E9%9D%99%E6%80%81%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[我们可以使用 koa-static 快速搭建静态服务器，用于访问 koa 服务器内的js，css，img等静态文件。 hello koa-static首先安装 koa-static 1npm install koa-static --save 然后定义 static 服务器文件夹所在目录，这里定义为 ./static , static 文件夹根目录对应网站根目录，里面的内容可以通过 url 直接访问 12const serve = require('koa-static');app.use(serve(__dirname + '/static')); 文件结构1234567hello-koa-static|-views| |-index.html|-static| |-img| |-koa.pngapp.js index.html 1234567891011&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;&#123;&#123;title&#125;&#125;&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div style="text-align: center;"&gt; &lt;img src="img/koa.png"&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; app.js 123456789101112131415161718const Koa = require('koa');const app = new Koa();const nunjucks = require('nunjucks');nunjucks.configure('views', &#123; autoescape: true &#125;);const serve = require('koa-static');app.use(serve(__dirname + '/static'));app.use(async (ctx,next)=&gt;&#123; if (ctx.path === '/') &#123; ctx.body = nunjucks.render('index.html', &#123; title: 'hello koa-static' &#125;) &#125; else &#123; await next; &#125;&#125;)app.listen(3000); 打开 192.168.1.101:3000 可以看到 koa 的 logo 图片 ，同样，我们也可以直接访问 192.168.1.101:3000/img/koa.png 查看此图片。]]></content>
      <tags>
        <tag>koa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[koa2学习系列教程5 模板引擎 Nunjucks]]></title>
    <url>%2F2017%2F04%2F27%2Fkoa%2Fkoa2%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B5%E6%A8%A1%E6%9D%BF%E5%BC%95%E6%93%8E_Nunjucks%2F</url>
    <content type="text"><![CDATA[hello NunjucksNunjucks是Mozilla开发的一个纯JavaScript编写的模板引擎，语法与Python的模板引擎jinja2类似 目录结构1234hello-Nunjucks/|-views # 用于存放模板文件| |-hello.html # HTML模板文件|-app.js # 入口js 安装Nunjucks1npm install Nunjucks --save 开始使用app.js 12345678910const Koa = require('koa');const app = new Koa();const nunjucks = require('nunjucks');nunjucks.configure('views', &#123; autoescape: true &#125;);app.use(ctx =&gt; &#123; ctx.body = nunjucks.render('hello.html', &#123; name: 'nunjucks' &#125;)&#125;);app.listen(3000); hello.html 1&lt;h1&gt;Hello &#123;&#123; name &#125;&#125;&lt;/h1&gt; 启动服务器，打开 http://127.0.0.1:3000/ 就可以看到 Hello nunjucks 的字样，是不是很简单？ 常用语法for遍历一维数组 1var items = [&#123; title: "foo", id: 1 &#125;, &#123; title: "bar", id: 2&#125;]; 12345678&lt;h1&gt;Posts&lt;/h1&gt;&lt;ul&gt;&#123;% for item in items %&#125; &lt;li&gt;&#123;&#123; item.title &#125;&#125;&lt;/li&gt;&#123;% else %&#125; &lt;li&gt;This would display if the 'item' collection were empty&lt;/li&gt;&#123;% endfor %&#125;&lt;/ul&gt; 遍历二维数组 1var points = [[0, 1, 2], [5, 6, 7], [12, 13, 14]]; 123&#123;% for x, y, z in points %&#125; Point: &#123;&#123; x &#125;&#125;, &#123;&#123; y &#125;&#125;, &#123;&#123; z &#125;&#125;&#123;% endfor %&#125; 遍历字典 12345var food = &#123; 'ketchup': '5 tbsp', 'mustard': '1 tbsp', 'pickle': '0 tbsp'&#125;; 123&#123;% for ingredient, amount in food %&#125; Use &#123;&#123; amount &#125;&#125; of &#123;&#123; ingredient &#125;&#125;&#123;% endfor %&#125; 模板继承parent.html 1234567891011121314&#123;% block header %&#125;This is parent top content!&#123;% endblock %&#125;&lt;section class="left"&gt; &#123;% block left %&#125; &#123;% endblock %&#125;&lt;/section&gt;&lt;section class="right"&gt; &#123;% block right %&#125; This is parent right content! &#123;% endblock %&#125;&lt;/section&gt; child.html 123456789&#123;% extends "parent.html" %&#125;&#123;% block left %&#125;This is child left content!&#123;% endblock %&#125;&#123;% block right %&#125;This is clild right content!&#123;% endblock %&#125; app.js 12345678910const Koa = require('koa');const app = new Koa();const nunjucks = require('nunjucks');nunjucks.configure('views', &#123; autoescape: true &#125;);app.use(ctx =&gt; &#123; ctx.body = nunjucks.render('child.html', &#123; name: 'nunjucks' &#125;)&#125;);app.listen(3000); 输出结果如下: 1234567891011121314This is father top content&lt;section class=&quot;left&quot;&gt;This is child left content!&lt;/section&gt;&lt;section class=&quot;right&quot;&gt;This is clild right content!&lt;/section&gt; 1可以看出，继承的使用方式为：使用 `&#123;% block &lt;blockname&gt; %&#125;&#123;% endblock %&#125;` 进行定义，使用 `&#123;% extends &quot;parent.html&quot; %&#125;` 进行继承，重新在子页面定义的内容将被重写 题外话 1当文中出现 &#123;% block &lt;blockname&gt; %&#125;&#123;% endblock %&#125; 的时候，hexo g 报错，是因为 hexo 模板引擎的原因;所以我将上面那句话放到了代码块中 参考资料Nunjucks官网文档]]></content>
      <tags>
        <tag>koa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nodejs基础教程之fs模块]]></title>
    <url>%2F2017%2F04%2F26%2Fnodejs%2Fnodejs%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B%E4%B9%8Bfs%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[fs是filesystem的缩写，该模块提供的文件读写能力,几乎对所有操作提供异步和同步两种操作方式，供开发者选择 12const fs = require('fs')const fileName = __dirname + '/test.txt' 文件读写文件读取 fs.readFile(file[, options], callback) 异步读取 12345const fs = require('fs')fs.readFile(__dirname + '/test.txt', (err, data) =&gt; &#123; if (err) throw err; console.log(data);&#125;); 1&lt;Buffer 68 65 6c 6c 6f 20 6e 6f 64 65 6a 73 ef bc 81&gt; fs.readFileSync(file[, options]) 同步读取 123let fileName = __dirname + '/test.txt'let data = fs.readFileSync(fileName, 'utf8');console.log('readFileSync_str:',data) 文件写入 writeFile方法用于异步写入文件。 1234fs.writeFile(fileName, 'Hello Node.js', 'utf8',(err) =&gt; &#123; if (err) throw err; console.log('It\'s saved!');&#125;); writeFileSync方法用于同步写入文件。 1fs.writeFileSync(fileName, str, 'utf8'); 目录操作mkdir() &amp; mkdirSync()12345//mode default value 0o777fs.mkdir(path[, mode], callback)#//return undefinedfs.mkdirSync(path[, mode]) readdir() &amp; readdirSync()123456fs.readdir(path, function(err,files)&#123; if (err) throw err; console.log(files);&#125;)fs.readdirSync(path) files是一个包含了文件或目录名的数组 1[&apos;readdir.js&apos;,&apos;readFile.js&apos;,&apos;stat.js&apos;,&apos;test.txt&apos;,&apos;writeFile.js&apos; ] fs.stat我们可以通过fs.stat用于得到文件信息，判断文件是否存在，以及是文件还是目录 123fs.stat(fileName,(err,stats)=&gt;&#123; console.log(stats)&#125;) 如果文件不存在，输出undefined，如果文件存在，输出以下信息： 12345678910111213141516&#123; dev: 2114, ino: 48064969, mode: 33188, nlink: 1, uid: 85, gid: 100, rdev: 0, size: 527, blksize: 4096, blocks: 8, atime: Mon, 10 Oct 2011 23:24:11 GMT, mtime: Mon, 10 Oct 2011 23:24:11 GMT, ctime: Mon, 10 Oct 2011 23:24:11 GMT, birthtime: Mon, 10 Oct 2011 23:24:11 GMT&#125; stats的方法 1234567stats.isFile()stats.isDirectory()stats.isBlockDevice()stats.isCharacterDevice()stats.isSymbolicLink() (only valid with fs.lstat())stats.isFIFO()stats.isSocket() watchfile() &amp; unwatchfile()watchfile方法监听一个文件，如果该文件发生变化，就会自动触发回调函数。 123456var fs = require('fs');fs.watchFile('message.text', (curr, prev) =&gt; &#123; console.log(`the current mtime is: $&#123;curr.mtime&#125;`); console.log(`the previous mtime was: $&#123;prev.mtime&#125;`);&#125;); unwatchfile方法用于解除对文件的监听。 createReadStream()createReadStream方法往往用于打开大型的文本文件，创建一个读取操作的数据流。所谓大型文本文件，指的是文本文件的体积很大，读取操作的缓存装不下，只能分成几次发送，每次发送会触发一个data事件，发送结束会触发end事件。 123456let input = fs.createReadStream(fileName);input.on('start',() =&gt; &#123;console.log('start')&#125;)input.on('data',(data) =&gt;&#123; console.log(data)&#125;)input.on('end', () =&gt; &#123;console.log('end')&#125;) createWriteStream方法创建一个写入数据流对象，该对象的write方法用于写入数据，end方法用于结束写入操作。 12345var out = fs.createWriteStream(fileName, &#123; encoding: 'utf8'&#125;);out.write(str);out.end(); createWriteStream方法和createReadStream方法配合，可以实现拷贝大型文件。 123456789101112function fileCopy(filename1, filename2, done) &#123; var input = fs.createReadStream(filename1); var output = fs.createWriteStream(filename2); input.on('data', (d) =&gt; &#123; output.write(d); &#125;); input.on('error', (err) =&gt; &#123; throw err; &#125;); input.on('end', () =&gt; &#123; output.end(); if (done) done(); &#125;);&#125; pipe()就像可以把两个水管串成一个更长的水管一样，两个流也可以串起来。一个Readable流和一个Writable流串起来后，所有的数据自动从Readable流进入Writable流，这种操作叫pipe。 在Node.js中，Readable流有一个 pipe() 方法，就是用来干这件事的。 让我们用 pipe() 把一个文件流和另一个文件流串起来，这样源文件的所有数据就自动写入到目标文件里了，所以，这实际上是一个复制文件的程序： 123456const fs = require('fs');const rs = fs.createReadStream('sample.txt');const ws = fs.createWriteStream('copied.txt');rs.pipe(ws); 默认情况下，当Readable流的数据读取完毕，end事件触发后，将自动关闭Writable流。如果我们不希望自动关闭Writable流，需要传入参数： 1readable.pipe(writable, &#123; end: false &#125;);]]></content>
      <tags>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[命令行程序开发教程]]></title>
    <url>%2F2017%2F04%2F26%2F%E5%89%8D%E7%AB%AF%2F%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[可执行脚本首先，使用 JavaScript 语言，写一个可执行脚本 hello（注意这里是hello不是hello.js） 12#!/usr/bin/env nodeconsole.log('hello world'); 然后，修改 hello 的权限 1$ chmod 755 hello 在当前目录下新建 package.json ，写入下面的内容 123456&#123; "name": "hello", "bin": &#123; "hello": "hello" &#125;&#125; 然后执行 npm link 命令。 1$ npm link 现在再执行 hello ，就不用输入路径了。 12$ hellohello world 命令行参数的原始写法命令行参数可以用系统变量 process.argv 获取。 下面是一个脚本 hello 12#!/usr/bin/env nodeconsole.log('hello ', process.argv[2]); 执行时，直接在脚本文件后面，加上参数即可。 12$ hello tomhello tom 上面代码中，实际上执行的是 node ./hello tom ，对应的 process.argv 是 [&#39;node&#39;, &#39;/path/to/hello&#39;, &#39;tom&#39;] 新建进程脚本可以通过 child_process 模块新建子进程，从而执行 Unix 系统命令。 12345678#!/usr/bin/env nodevar name = process.argv[2];var exec = require('child_process').exec;var child = exec('echo hello ' + name, function(err, stdout, stderr) &#123; if (err) throw err; console.log(stdout);&#125;); 用法如下。 12$ ./hello tomhello tom shelljs 模块shelljs 模块重新包装了 child_process，调用系统命令更加方便。它需要安装后使用。 1npm install --save shelljs 然后，改写脚本。 12345#!/usr/bin/env nodevar name = process.argv[2];var shell = require("shelljs");shell.exec("echo hello " + name); shelljs不赞成全局引入，那样会污染变量。 所以我们应该如此使用shell： 12345678910111213141516171819202122232425var shell = require('shelljs');if (!shell.which('git')) &#123; shell.echo('Sorry, this script requires git'); shell.exit(1);&#125;// Copy files to release dirshell.rm('-rf', 'out/Release');shell.cp('-R', 'stuff/', 'out/Release');// Replace macros in each .js fileshell.cd('lib');shell.ls('*.js').forEach(function (file) &#123; shell.sed('-i', 'BUILD_VERSION', 'v0.1.2', file); shell.sed('-i', /^.*REMOVE_THIS_LINE.*$/, '', file); shell.sed('-i', /.*REPLACE_LINE_WITH_MACRO.*\n/, shell.cat('macro.js'), file);&#125;);shell.cd('..');// Run external tool synchronouslyif (shell.exec('git commit -am "Auto-commit"').code !== 0) &#123; shell.echo('Error: Git commit failed'); shell.exit(1);&#125; 命令参考yargs 模块shelljs 只解决了如何调用 shell 命令，而 yargs 模块能够解决如何处理命令行参数。它也需要安装。 1$ npm install --save yargs yargs 模块提供 argv 对象，用来读取命令行参数。请看改写后的 hello 。 1234#!/usr/bin/env nodevar argv = require('yargs').argv;console.log('hello ', argv.name); 使用时，下面两种用法都可以。 12$ hello --name=tomhello tom 12$ hello --name tomhello tom 也就是说，process.argv 的原始返回值如下。 1234$ node hello --name=tom[ 'node', '/path/to/myscript.js', '--name=tom' ] 参考链接Node.js 命令行程序开发教程]]></content>
      <tags>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[koa2学习系列教程8 mysql数据库]]></title>
    <url>%2F2017%2F04%2F25%2Fkoa%2Fkoa2%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B8mysql%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[在命令行使用mysql安装mysql对于mysql，我们除了在官网下载安装包以外，还可以使用XAMPP建站集成环境进行安装，安装完成后打开MySQL模块即可使用 添加环境变量找到mysql的bin文件夹，将其添加到环境变量路径，方便在cmd中使用mysql 添加方法：使用win自带的搜索功能，搜索环境变量，进入编辑环境变量后选择用户环境变量，选择PATH后编辑，选择新建，输入mysql的bin路径，比如我的是 E:\xampp\mysql\bin 编辑PATH完成后，cmd重启生效 命令行的基本操作连接数据库 1$ mysql -h localhost -u root -p 要求输入password时，假如未设定，直接回车即可 显示用户名下的所有数据库 1$ show databases; 注意sql语句要求以’;’结尾，执行命令后，会输出形如下列的表格 123456789+--------------------+| Database |+--------------------+| information_schema || mynote || mysql || performance_schema || phpmyadmin |+--------------------+ 进入某个数据库 1$ use databasename 退出mysql 1$ exit; MYSQL 与 ORM如果直接使用mysql包提供的接口，我们编写的代码就比较底层，例如，查询代码： 12345connection.query('SELECT * FROM users WHERE id = ?', ['123'], function(err, rows) &#123; for (let row in rows) &#123; processRow(row); &#125;&#125;); 考虑到数据库表是一个二维表，包含多行多列，例如一个pets的表： 12345678mysql&gt; select * from pets;+----+--------+------------+| id | name | birth |+----+--------+------------+| 1 | Gaffey | 2007-07-07 || 2 | Odie | 2008-08-08 |+----+--------+------------+2 rows in set (0.00 sec) 每一行可以用一个JavaScript对象表示，例如第一行： 12345&#123; &quot;id&quot;: 1, &quot;name&quot;: &quot;Gaffey&quot;, &quot;birth&quot;: &quot;2007-07-07&quot;&#125; 这就是传说中的ORM技术：Object-Relational Mapping，把关系数据库的表结构映射到对象上 但是由谁来做这个转换呢？所以ORM框架应运而生。 ORM框架 Sequelize我们选择Node的ORM框架Sequelize来操作数据库。这样，我们读写的都是JavaScript对象，Sequelize帮我们把对象变成数据库中的行。 用Sequelize查询pets表，代码像这样： 12345678Pet.findAll() .then(function (pets) &#123; for (let pet in pets) &#123; console.log(`$&#123;pet.id&#125;: $&#123;pet.name&#125;`); &#125; &#125;).catch(function (err) &#123; // error &#125;); 因为Sequelize返回的对象是Promise，所以我们可以用then()和catch()分别异步响应成功和失败。 但是用then()和catch()仍然比较麻烦。有没有更简单的方法呢？ 可以用ES7的await来调用任何一个Promise对象，这样我们写出来的代码就变成了： 123(async () =&gt; &#123; var pets = await Pet.findAll();&#125;)(); 真的就是这么简单！ 考虑到koa的处理函数都是async函数，所以我们实际上将来在koa的async函数中直接写await访问数据库就可以了！ 这也是为什么我们选择Sequelize的原因：只要API返回Promise，就可以用await调用，写代码就非常简单！ 使用 Sequelize12npm install sequelize --savenpm install mysql --save 注意mysql是驱动，我们不直接使用，但是sequelize会用 配置config.js,他是一个简单的配置文件 123456789var config = &#123; database: 'test', // 使用哪个数据库 username: 'www', // 用户名 password: 'www', // 口令 host: 'localhost', // 主机名 port: 3306 // 端口号，MySQL默认3306&#125;;module.exports = config; 第一步，创建一个sequelize对象实例： 123456789101112const Sequelize = require('sequelize');const config = require('./config');var sequelize = new Sequelize(config.database, config.username, config.password, &#123; host: config.host, dialect: 'mysql', pool: &#123; max: 5, min: 0, idle: 30000 &#125;&#125;); 第二步，定义模型Pet，告诉Sequelize如何映射数据库表： 1234567891011121314var Pet = sequelize.define('pet', &#123; id: &#123; type: Sequelize.STRING(50), primaryKey: true &#125;, name: Sequelize.STRING(100), gender: Sequelize.BOOLEAN, birth: Sequelize.STRING(10), createdAt: Sequelize.BIGINT, updatedAt: Sequelize.BIGINT, version: Sequelize.BIGINT&#125;, &#123; timestamps: false &#125;); 用sequelize.define()定义Model时，传入名称pet，默认的表名就是pets。第二个参数指定列名和数据类型，如果是主键，需要更详细地指定。第三个参数是额外的配置，我们传入{ timestamps: false }是为了关闭Sequelize的自动添加timestamp的功能。所有的ORM框架都有一种很不好的风气，总是自作聪明地加上所谓“自动化”的功能，但是会让人感到完全摸不着头脑。 插入数据123456789101112(async () =&gt; &#123; var dog = await Pet.create(&#123; id: 'd-' + now, name: 'Odie', gender: false, birth: '2008-08-08', createdAt: now, updatedAt: now, version: 0 &#125;); console.log('created: ' + JSON.stringify(dog));&#125;)(); 显然await代码更胜一筹。 查询数据1234567891011(async () =&gt; &#123; var pets = await Pet.findAll(&#123; where: &#123; name: 'Gaffey' &#125; &#125;); console.log(`find $&#123;pets.length&#125; pets:`); for (let p of pets) &#123; console.log(JSON.stringify(p)); &#125;&#125;)(); 更新数据，可以对查询到的实例调用save()方法：1234567(async () =&gt; &#123; var p = await queryFromSomewhere(); p.gender = true; p.updatedAt = Date.now(); p.version ++; await p.save();&#125;)(); 删除数据，可以对查询到的实例调用destroy()方法：1234(async () =&gt; &#123; var p = await queryFromSomewhere(); await p.destroy();&#125;)(); Model为了避免格式上的混乱，我们需要一个统一的模型，强迫所有Model都遵守同一个规范，这样不但实现简单，而且容易统一风格。 我们首先要定义的就是Model存放的文件夹必须在models内，并且以Model名字命名，例如：Pet.js，User.js等等。 其次，每个Model必须遵守一套规范： 统一主键，名称必须是id，类型必须是STRING(50)； 主键可以自己指定，也可以由框架自动生成（如果为null或undefined）； 所有字段默认为NOT NULL，除非显式指定； 统一timestamp机制，每个Model必须有createdAt、updatedAt和version，分别记录创建时间、修改时间和版本号。其中，createdAt和updatedAt以BIGINT存储时间戳，最大的好处是无需处理时区，排序方便。version每次修改时自增。 数据库配置接下来，我们把简单的config.js拆成3个配置文件： config-default.js：存储默认的配置; config-override.js：存储特定的配置; config-test.js：存储用于测试的配置; 例如，默认的config-default.js可以配置如下： 1234567891011var config = &#123; dialect: 'mysql', database: 'nodejs', username: 'www', password: 'www', host: 'localhost', port: 3306&#125;;module.exports = config; 而config-override.js可应用实际配置： 12345678var config = &#123; database: 'production', username: 'www', password: 'secret-password', host: '192.168.1.199'&#125;;module.exports = config; config-test.js可应用测试环境的配置： 1234var config = &#123; database: 'test'&#125;;module.exports = config; 读取配置的时候，我们用config.js实现不同环境读取不同的配置文件： 1234567891011121314151617181920212223242526const defaultConfig = './config-default.js';// 可设定为绝对路径，如 /opt/product/config-override.jsconst overrideConfig = './config-override.js';const testConfig = './config-test.js';const fs = require('fs');var config = null;if (process.env.NODE_ENV === 'test') &#123; console.log(`Load $&#123;testConfig&#125;...`); config = require(testConfig);&#125; else &#123; console.log(`Load $&#123;defaultConfig&#125;...`); config = require(defaultConfig); try &#123; if (fs.statSync(overrideConfig).isFile()) &#123; console.log(`Load $&#123;overrideConfig&#125;...`); config = Object.assign(config, require(overrideConfig)); &#125; &#125; catch (err) &#123; console.log(`Cannot load $&#123;overrideConfig&#125;.`); &#125;&#125;module.exports = config; 具体的规则是： 先读取config-default.js；如果不是测试环境，就读取config-override.js，如果文件不存在，就忽略。如果是测试环境，就读取config-test.js。这样做的好处是，开发环境下，团队统一使用默认的配置，并且无需config-override.js。部署到服务器时，由运维团队配置好config-override.js，以覆盖config-override.js的默认设置。测试环境下，本地和CI服务器统一使用config-test.js，测试数据库可以反复清空，不会影响开发。 配置文件表面上写起来很容易，但是，既要保证开发效率，又要避免服务器配置文件泄漏，还要能方便地执行测试，就需要一开始搭建出好的结构，才能提升工程能力。 使用Model要使用Model，就需要引入对应的Model文件，例如：User.js。一旦Model多了起来，如何引用也是一件麻烦事。 自动化永远比手工做效率高，而且更可靠。我们写一个model.js，自动扫描并导入所有Model： 1234567891011121314151617181920const fs = require('fs');const db = require('./db');let files = fs.readdirSync(__dirname + '/models');let js_files = files.filter((f)=&gt;&#123; return f.endsWith('.js');&#125;, files);module.exports = &#123;&#125;;for (let f of js_files) &#123; console.log(`import model from file $&#123;f&#125;...`); let name = f.substring(0, f.length - 3); module.exports[name] = require(__dirname + '/models/' + f);&#125;module.exports.sync = () =&gt; &#123; return db.sync();&#125;; 这样，需要用的时候，写起来就像这样： 1234567const model = require('./model');let Pet = model.Pet, User = model.User;var pet = await Pet.create(&#123; ... &#125;); 工程结构最终，我们创建的工程model-sequelize结构如下： 12345678910111213141516model-sequelize/||-.vscode| |-launch.json # VSCode 配置文件|-models # 存放所有Model| |-Pet.js # Pet| |-User.js # User|-config.js # 配置文件入口|-config-default.js # 默认配置文件|-config-test.js # 测试配置文件|-db.js # 如何定义Model|-model.js # 如何导入Model|-init-db.js # 初始化数据库|-app.js # 业务代码|-package.json # 项目描述文件|-node_modules/ # npm安装的所有依赖包 注意到我们其实不需要创建表的SQL，因为Sequelize提供了一个sync()方法，可以自动创建数据库。这个功能在开发和生产环境中没有什么用，但是在测试环境中非常有用。测试时，我们可以用sync()方法自动创建出表结构，而不是自己维护SQL脚本。这样，可以随时修改Model的定义，并立刻运行测试。开发环境下，首次使用sync()也可以自动创建出表结构，避免了手动运行SQL的问题。 init-db.js的代码非常简单，作用是创建Pets和User表，只需要执行一次即可。 12345678910const model = require('./model.js');model.sync().then(()=&gt;&#123; console.log('sync done'); process.exit(0);&#125;).catch((e)=&gt;&#123; console.log('failed with: '+e); process.exit(0);&#125;);console.log('init db ok.'); 参考文档 廖雪峰的官方网站Sequelize官方文档]]></content>
      <tags>
        <tag>nodejs</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[koa2学习系列教程4 context应用上下文]]></title>
    <url>%2F2017%2F04%2F25%2Fkoa%2Fkoa2%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B4_context%E4%B8%8A%E4%B8%8B%E6%96%87%2F</url>
    <content type="text"><![CDATA[context对象的全局属性 request：指向Request对象 response：指向Response对象 req：指向Node的request对象 res：指向Node的response对象 app：指向App对象 state：用于在中间件传递信息。 state 对象1cyx.state.user = await User.find(id); 上面代码中，user属性存放在this.state对象上面，可以被另一个中间件读取。 Request与Response 对象我们可以使用ctx.request，ctx.response获取请求头与响应头 1234app.use( async(ctx) =&gt; &#123; ctx.request; // is a koa Request ctx.response; // is a koa Response&#125;); 尝试输出以上信息，输出结果如下 123456789101112131415161718# ctx.request&#123; method: &apos;GET&apos;, url: &apos;/&apos;, header: &#123; host: &apos;127.0.0.1:3000&apos;, connection: &apos;keep-alive&apos;, &apos;upgrade-insecure-requests&apos;: &apos;1&apos;, &apos;user-agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36&apos;, accept: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&apos;, &apos;accept-encoding&apos;: &apos;gzip, deflate, sdch, br&apos;, &apos;accept-language&apos;: &apos;zh-CN,zh;q=0.8&apos; &#125; &#125;# ctx.response&#123; status: 200, message: &apos;OK&apos;, header: &#123; &apos;content-type&apos;: &apos;text/plain; charset=utf-8&apos;, &apos;content-length&apos;: &apos;9&apos; &#125;, body: &apos;Hello Koa&apos; &#125; Koa 的上下文封装了 request 与 response 对象至一个对象中，并提供了一些帮助开发者编写业务逻辑的方法。 Request 对象 ctx.header ctx.method ctx.method= ctx.url ctx.url= ctx.path ctx.path= ctx.query ctx.query= ctx.querystring ctx.querystring= ctx.length ctx.host ctx.fresh ctx.stale ctx.socket ctx.protocol ctx.secure ctx.ip ctx.ips ctx.subdomains ctx.is() ctx.accepts() ctx.acceptsEncodings() ctx.acceptsCharsets() ctx.acceptsLanguages() ctx.get() Response 对象 ctx.body ctx.body= ctx.status ctx.status= ctx.length= ctx.type ctx.type= ctx.headerSent ctx.redirect() ctx.attachment() ctx.set() ctx.remove() ctx.lastModified= ctx.etag= context对象的全局方法 throw()：抛出错误，直接决定了HTTP回应的状态码。 assert()：如果一个表达式为false，则抛出一个错误。 ctx.throw()ctx.throw(msg, [status]) 抛出常规错误的辅助方法，默认 status 为 500。 以下几种写法都有效： 1234ctx.throw(403)ctx.throw('name required', 400)ctx.throw(400, 'name required')ctx.throw('something exploded') 实际上，ctx.throw(&#39;name required&#39;, 400) 是此代码片段的简写方法： 123var err = new Error('name required');err.status = 400;throw err; 需要注意的是，ctx.throw 创建的错误，均为用户级别错误（标记为err.expose），会被返回到客户端。 ctx.assert()12345// 格式ctx.assert(value, [msg], [status], [properties])// 例子this.assert(this.user, 401, 'User not found. Please login!');]]></content>
      <tags>
        <tag>nodejs</tag>
        <tag>koa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[koa2学习系列教程3 路由与koa-router]]></title>
    <url>%2F2017%2F04%2F24%2Fkoa%2Fkoa2%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B3_%E8%B7%AF%E7%94%B1%E4%B8%8Ekoa-router%2F</url>
    <content type="text"><![CDATA[路由的基本用法可以通过this.path属性，判断用户请求的路径，从而起到路由作用。 123456789101112131415app.use(async (ctx,next)=&gt;&#123; if (ctx.path === '/') &#123; ctx.body = 'we are at home!'; &#125; else &#123; await next; &#125;&#125;)app.use(async (ctx,next)=&gt;&#123; if (ctx.path === '/404') &#123; ctx.body = 'page not found'; &#125; else &#123; await next; &#125;&#125;) 上面代码中，每一个中间件负责一个路径，如果路径不符合，就传递给下一个中间件。 复杂的路由需要安装koa-router插件。 koa-router由于api更新等问题，使用时如果未达到预想效果，可以到npm查看官方文档 koa-router 基本的使用123456789101112131415var Koa = require('koa');var Router = require('koa-router');var app = new Koa();var router = new Router();router.get('/', async (ctx, next) =&gt;&#123; ctx.body = 'we are at home!';&#125;);app .use(router.routes()) .use(router.allowedMethods());app.listen(3000) 路径匹配的时候，不会把查询字符串考虑在内。比如，/index?param=xyz 匹配路径为 /index HTTP动词方法Koa-router实例提供一系列动词方法，即一种HTTP动词对应一种方法。 12345678router .get('/', async (ctx, next) =&gt; &#123; ctx.body = 'Hello World!'; &#125;) .post('/users', async (ctx, next) =&gt; &#123;...&#125;) .put('/users/:id', async (ctx, next) =&gt; &#123;...&#125;) .del('/users/:id', async (ctx, next) =&gt; &#123;...&#125;) .all('/users/:id', async (ctx, next) =&gt; &#123;...&#125;) router.all()用于表示上述所有的动词方法 123router.get('/', async (ctx,next) =&gt; &#123; ctx.body = 'Hello World!';&#125;); 上面代码中，router.get方法的第一个参数是根路径，第二个参数是对应的函数方法。 路由参数我们可以通过ctx.params得到URL参数 1234router.get('/:category/:title', function (ctx, next) &#123; console.log(ctx.params); // =&gt; &#123; category: 'programming', title: 'how-to-node' &#125;&#125;); 支持多个中间件12345678910111213router.get( '/users/:id', function (ctx, next) &#123; return User.findOne(ctx.params.id).then(function(user) &#123; ctx.user = user; return next(); &#125;); &#125;, function (ctx) &#123; console.log(ctx.user); // =&gt; &#123; id: 17, name: "Alex" &#125; &#125;); 路由前缀123456var router = new Router(&#123; prefix: '/users'&#125;);router.get('/', ...); // responds to "/users"router.get('/:id', ...); // responds to "/users/:id" 重构每个 url 对应一个规则，如果全部放在 app.js 中会造成代码紊乱且难以理解。因此重构整个项目，项目文件结构如下： 12345|-controller # 用于存放路由规则| |-index.js # 首页路由规则| |-user.js # user路由规则|-controller.js # 自动导入controller下的所有路由规则|-app.js # 入口文件，用于启动koa服务器 index.js 内容如下 1234567const homepage = async (ctx, next) =&gt;&#123; ctx.body = 'we are at home!';&#125;module.exports = &#123; 'GET /': homepage&#125;; 当 require(‘controller/index’) 时，会得到一个包含了规则的对象 {&#39;GET /&#39;: homepage} 其中，GET 表示 GET 方法 / 表示解析路径，homepage 是针对这个路径所做的操作。解析规则由 controller.js 的 add_rule 方法实现 controller.js 内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142const fs = require('fs')const Router = require('koa-router');const router = new Router();// 解析规则 &#123;'GET /': homepage&#125;function add_rule(router, rule) &#123; for (let key in rule) &#123; // key = 'GET /' rule = &#123;'GET /': homepage&#125; if (key.startsWith('GET ')) &#123; let path = key.substring(4); router.get(path, rule[key]); console.log(`register URL mapping: GET $&#123;path&#125;`); &#125; else if (key.startsWith('POST ')) &#123; let path = key.substring(5); router.post(path, rule[key]); console.log(`register URL mapping: POST $&#123;path&#125;`); &#125; else &#123; console.log(`invalid URL: $&#123;key&#125;`); &#125; &#125;&#125;//自动导入controller文件夹下所有的路由规则function add_rules(router) &#123; // 得到 /controller 所有以js结尾的文件 let files = fs.readdirSync(__dirname + '/controller'); let js_files = files.filter((f) =&gt; &#123; return f.endsWith('.js'); &#125;); // 添加规则 for (let f of js_files) &#123; console.log(`process controller: $&#123;f&#125;...`); let rule = require(__dirname + '/controller/' + f); add_rule(router, rule); &#125;&#125;module.exports = function () &#123; add_rules(router); return router.routes();&#125;; app.js 内容如下: 12345678const Koa = require('koa');const app = new Koa();const controller = require('./controller');//app.use(router.routes())app.use(controller())app.listen(3000) koa-router源码解读廖雪峰的官方网站]]></content>
      <tags>
        <tag>nodejs</tag>
        <tag>koa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[koa2学习系列教程2 中间件]]></title>
    <url>%2F2017%2F04%2F24%2Fkoa%2Fkoa2%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B2_%E4%B8%AD%E9%97%B4%E4%BB%B6%2F</url>
    <content type="text"></content>
      <tags>
        <tag>nodejs</tag>
        <tag>koa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[koa2学习系列教程1 Hello Koa]]></title>
    <url>%2F2017%2F04%2F24%2Fkoa%2Fkoa2%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%95%99%E7%A8%8B1_Hello_Koa%2F</url>
    <content type="text"><![CDATA[安装koa依赖于node v7.6.0或者更高的node版本，如果版本未达到要求，请升级 1$ npm install koa Hello koa123456789const Koa = require('koa');const app = new Koa();// responseapp.use(ctx =&gt; &#123; ctx.body = 'Hello Koa';&#125;);app.listen(3000); ctx.body = ‘Hello World’这行代码表示设置response.body的值为’Hello World, 一个Koa应用就是一个对象，包含了一个middleware数组，这个数组由一组同步或异步函数组成。 这些函数负责对HTTP请求进行各种加工，比如生成缓存、指定代理、请求重定向等等。 变量app就是一个Koa应用。它监听3000端口，返回一个内容为Hello Koa的网页 app.use方法用于向middleware数组添加相应函数。 listen方法指定监听端口，并启动当前应用。 异步编程终极解决方案 asyncasync是js异步执行的最佳方案，await无法单独出现，总是需要与async配合使用; async 函数返回一个 Promise 对象，可以使用 then 方法添加回调函数。当函数执行的时候，一旦遇到 await 就会先返回，等到触发的异步操作完成，再接着执行函数体内后面的语句。 12345678async function foo ()=&gt;&#123; console.log('foo start') await sub() console.log('foo end')&#125;async function sub ()=&gt;&#123; console.log('sub')&#125; async表示这个函数是异步的 await sub() 表示等待 sub() 执行完毕,并返回结果后，继续向下执行 输出结果如下： 123foo startsubfoo end 中间件 (async需要node v7.6+)koa是一个可以使用两种函数作为中间件的中间件框架 async function common function 123456789101112131415//异步的app.use(async (ctx, next) =&gt; &#123; console.log('first middleware start') await next(); await sub(); console.log('first middleware end')&#125;);//同步的app.use(()=&gt;&#123; console.log('second middleware')&#125;)const sub = ()=&gt;&#123; console.log('sub')&#125; 输出结果如下： 1234first middleware startsecond middlewaresubfirst middleware end app.use的参数就是中间件，可以看出，通过await next()可以调用下一个中间件，通过await func()可以调用其他函数 参考文档koa 中文文档koa实战]]></content>
      <tags>
        <tag>nodejs</tag>
        <tag>koa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node.js框架之express与koa对比分析]]></title>
    <url>%2F2017%2F04%2F24%2Fnodejs%2FNode-js%E6%A1%86%E6%9E%B6%E4%B9%8Bexpress%E4%B8%8Ekoa%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[学习了Node.js框架之express与koa对比分析这篇文章，个人对于express以及koa有了初步的了解，综合考虑，决定使用koa进行后台开发。以下为文章正文。 提到Node.js开发，不得不提目前炙手可热的2大框架express和koa。Express诞生已有时日，是一个简洁而灵活的web开发框架，使用简单而功能强大。Koa相对更为年轻，是Express框架原班人马基于ES6新特性重新开发的敏捷开发框架，现在可谓风头正劲，大有赶超Express之势。 Express和koa都是服务端的开发框架，服务端开发的重点是对HTTP Request和HTTP Response两个对象的封装和处理，应用的生命周期维护以及视图的处理等。 以下将主要通过这些方面，对两者进行一个对比介绍，看看到底有什么区别。 Express主要基于Connect中间件框架，功能丰富，随取随用，并且框架自身封装了大量便利的功能，比如路由、视图处理等等。而koa主要基于co中间件框架，框架自身并没集成太多功能，大部分功能需要用户自行require中间件去解决，但是由于其基于ES6 generator特性的中间件机制，解决了长期诟病的“callback hell”和麻烦的错误处理的问题，大受开发者欢迎。 Express和koa初印象先来一个Hello World，各自认识一下吧 12345678910111213141516171819202122232425//Expressvar express = require('express')var app = express() //创建一个APP实例//建一个项目根目录的get请求路由，回调方法中直接输出字符串Hello World!app.get('/', function (req, res) &#123; res.send('Hello World!')&#125;);//监听端口，启动服务app.listen(3000);//Koavar koa = require('koa');var route = require('koa-route'); //koa默认没有集成route功能，引入中间件var app = koa(); //创建一个APP实例//建一个项目根目录的get请求路由，回调方法中直接输出字符串Hello World!，就是挂载一个中间件app.use(route.get('/', function *()&#123; this.body = 'Hello World';&#125;));//监听端口，启动服务app.listen(3000); 可以看出来，两者创建一个基础的Web服务都非常简单，可以说几行代码就解决了问题。两者的写法也基本相同，最大的区别是路由处理Express是自身集成的，而koa需要引入中间件。以下是Koa官方文档对于两者特性的一个对比： 重要功能对比介绍 Feature Koa Express ConnectMiddleware KernelRoutingTemplatingSending FilesJSONP 通过后续的比较，大家其实可以看出，虽然koa看上去比express少集成了很多功能，但是使用起来其实基本一致，因为中间件非常丰富全面，需要什么require进来就行了（不一定要像express那样先帮你require好），使用起来反而更加灵活。 应用生命周期和上下文我们在项目过程中，经常需要用到在整个应用生命周期中共享的配置和数据对象，比如服务URL、是否启用某个功能特性、接口配置、当前登录用户数据等等。属于比较基础的功能，两者都非常方便，koa的application context感觉使用起来更方便一点。 123456789101112131415161718192021222324252627282930//Express//共享配置，express提供了很多便利的方法app.set('enableCache', true)app.get('enableCache')//trueapp.disable('cache')app.disabled('cache')//trueapp.enable('cache')app.enabled('cache')//true//应用共享数据：app.localsapp.locals.user = &#123;name:"Samoay", id:1234&#125;;//Koa//配置，直接使用koa context即可app.enableCache = true;app.use(function *(next)&#123; console.log(this.app.enableCache); //true this.app.enableCache = false; //just use this this.staticPath = 'static'; yield *next;&#125;);//应用共享数据：ctx.statethis.state.user = &#123;name:"Samoay", id:1234&#125;; 请求 HTTP Request服务器端需要进行什么处理，怎么处理以及处理的参数都依赖客户端发送的请求，两个框架都封装了HTTP Request对象，便于对这一部分进行处理。以下主要举例说明下对请求参数的处理，其它例如头信息、Cookie等请参考官方文档。两者除了写法上稍有区别，没太大区别。GET参数都可以直接通过Request对象获取，POST参数都需要引入中间件先parse，再取值。 12345678910111213141516171819202122232425262728293031323334353637383940414243// Express// 获取QueryString参数// GET /shoes?order=desc&amp;shoe[color]=bluereq.query.order// =&gt; "desc"req.query.shoe.color// =&gt; "blue"// 通过路由获取Restful风格的URL参数app.get('/user/:id?', function userIdHandler(req, res) &#123; console.log(req.params.id); res.send('GET');&#125;)//获取POST数据:需要body-parser中间件var bodyParser = require('body-parser');app.use(bodyParser.urlencoded(&#123; extended: true &#125;));app.post('/', function (req, res) &#123; console.log(req.body); res.json(req.body);&#125;)// 获取QueryString参数// GET /?action=delete&amp;id=1234this.request.query// =&gt; &#123; action: 'delete', id: '1234' &#125;// 通过路由获取Restful风格的URL参数var route = require('koa-route');app.use(route.get('/post/:id', function *(id)&#123; console.log(id); // =&gt; 1234&#125;));// 获取POST数据:需要co-body中间件// Content-Type: application/x-www-form-urlencoded// title=Test&amp;content=This+is+a+test+postvar parse = require('co-body');app.use(route.post('/post/new', function *()&#123; var post = yield parse(this.request);//this console.log(post); // =&gt; &#123; title: 'Test', content: 'This is a test post' &#125;&#125;)); 路由 Route收到客户端的请求，服务需要通过识别请求的方法（HTTP Method: GET, POST, PUT…）和请求的具体路径(path)来进行不同的处理。这部分功能就是路由（Route）需要做的事情，说白了就是请求的分发，分发到不同的回调方法去处理。 12345678910111213141516171819202122232425262728293031323334353637383940// Express// app.all表示对所有的路径和请求方式都要经过这些回调方法的处理，可以逗号方式传入多个app.all('*', authentication, loadUser);// 也可以多次调用app.all('*', requireAuthentication)app.all('*', loadUser);// 也可以针对某具体路径下面的所有请求app.all('/api/*', requireAuthentication);// app.get GET方式的请求app.get('/user/:id', function(req, res) &#123; res.send('user ' + req.params.id);&#125;);// app.post POST方式的请求app.post('/user/create', function(req, res) &#123; res.send('create new user');&#125;);这里需要说明2个问题，首先是 app.get ，在应用生命周期中也有一个 app.get 方法，用于获取项目配置。Express内部就是公用的一个方法，如果传入的只有1个参数就获取配置，2个参数就作为路由处理。其次是 app.use('*', cb) 与 app.all('*', cb) 的区别，前者是中间件方式，调用是有顺序的，不一定会执行到；后者是路由方式，肯定会执行到。// Koa// 和Express不同，koa需要先引入route中间件var route = require('koa-route');//引入中间件之后支持的写法差不多，只是路径传入route，然后把route作为中间件挂载到appapp.use(route.get('/', list));app.use(route.get('/post/new', add));app.use(route.get('/post/:id', show));app.use(route.post('/post', create));//链式写法var router = require('koa-router')();router.get('/', list) .get('/post/new', add) .get('/post/:id', show) .post('/post', create);app.use(router.routes()) .use(router.allowedMethods()); 视图 ViewsExpress框架自身集成了视图功能，提供了consolidate.js功能，可以是有几乎所有Javascript模板引擎，并提供了视图设置的便利方法。Koa需要引入co-views中间件，co-views也是基于consolidate.js，支持能力一样强大。 12345678910111213141516171819202122232425262728293031323334353637// Express// 这只模板路径和默认的模板后缀app.set('views', __dirname + '/tpls');app.set('view engine', 'html');//默认，express根据template的后缀自动选择模板//引擎渲染，支持jade和ejs。如果不使用默认扩展名app.engine(ext, callback)app.engine('html', require('ejs').renderFile);//如果模板引擎不支持(path, options, callback)var engines = require('consolidate');app.engine('html', engines.handlebars);app.engine('tpl', engines.underscore);app.get('list', function(res, req)&#123; res.render('list', &#123;data&#125;);&#125;);//Koa//需要引入co-views中间件var views = require('co-views');var render = views('tpls', &#123; map: &#123; html: 'swig' &#125;,//html后缀使用引擎 default: "jade"//render不提供后缀名时&#125;);var userInfo = &#123; name: 'tobi', species: 'ferret'&#125;;var html;html = render('user', &#123; user: userInfo &#125;);html = render('user.jade', &#123; user: userInfo &#125;);html = render('user.ejs', &#123; user: userInfo &#125;); 返回 HTTP Response获取完请求参数、处理好了具体的请求、视图也准备就绪，下面就该返回给客户端了，那就是HTTP Response对象了。这部分也属于框架的基础部分，各种都做了封装实现，显著的区别是koa直接将输出绑定到了ctx.body属性上，另外输出JSON或JSONP需要引入中间件。 12345678910111213141516171819202122232425262728293031323334353637383940// Express//输出普通的htmlres.render('tplName', &#123;data&#125;);//输出JSONres.jsonp(&#123; user: 'Samoay' &#125;);// =&gt; &#123; "user": "Samoay" &#125;//输出JSONP ?callback=foores.jsonp(&#123; user: 'Samoay' &#125;);// =&gt; foo(&#123; "user": "Samoay" &#125;);//res.send([body]);res.send(new Buffer('whoop'));res.send(&#123; some: 'json' &#125;);res.send('&lt;p&gt;some html&lt;/p&gt;');//设定HTTP Status状态码res.status(200);//koa直接set ctx的status和bodyapp.use(route.get('/post/update/:id', function *(id)&#123; this.status = 404; this.body = 'Page Not Found';&#125;));var views = require('co-views');var render = views('tpls', &#123; default: "jade"//render不提供后缀名时&#125;);app.use(route.get('/post/:id', function *(id)&#123; var post = getPost(id); this.status = 200;//by default, optional this.body = yield render('user', post);&#125;));//JSONvar json = require('koa-json');app.use(route.get('/post/:id', function *(id)&#123; this.body = &#123;id:1234, title:"Test post", content:"..."&#125;;&#125;)); 中间件 Middleware对比了主要的几个框架功能方面的使用，其实区别最大，使用方式最不同的地方是在中间件的处理上。Express由于是在ES6特性之前的，中间件的基础原理还是callback方式的；而koa得益于generator特性和co框架（co会把所有generator的返回封装成为Promise对象），使得中间件的编写更加优雅。 123456789101112131415161718192021222324252627282930// req 用于获取请求信息， ServerRequest 的实例// res 用于响应处理结果， ServerResponse 的实例// next() 函数用于将当前控制权转交给下一步处理，// 如果给 next() 传递一个参数时，表示出错信息var x = function (req, res, next) &#123; // 对req和res进行必要的处理 // 进入下一个中间件 return next(); // 传递错误信息到下一个中间件 return next(err); // 直接输出，不再进入后面的中间件 return res.send('show page');&#125;;// koa 一切都在ctx对象上+generatorapp.use(function *()&#123; this; // is the Context this.request; // is a koa Request this.response; // is a koa Response this.req;// is node js request this.res;// is node js response //不再进入后面的中间件, 回溯upstream return;&#125;); 本文转载自：Node.js框架之express与koa对比分析]]></content>
      <tags>
        <tag>nodejs 开发框架 服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python协程与异步IO]]></title>
    <url>%2F2017%2F04%2F23%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E5%8D%8F%E7%A8%8B%E4%B8%8E%E5%BC%82%E6%AD%A5IO%2F</url>
    <content type="text"><![CDATA[协程协程的用途在于IO，遇到IO就挂起，并发执行另一个任务，比如http请求，可以实现同时请求若干个网站的目的但是很多经典库比如requests并不支持协程 生成器与协程 PEP-0492 通过使用 async 关键字显式的对生成器和协程做了区分。 实际上，使用async显示声明协程（函数），并在函数内使用wait代替yield，一个协程变诞生了 生成器 yield的功能类似于return，但是不同之处在于它返回的是生成器 生成器是通过一个或多个yield表达式构成的函数，每一个生成器都是一个迭代器(for循环或while循环) 如果一个函数包含yield关键字，这个函数就会变为一个生成器。 生成器并不会一次返回所有结果，而是每次遇到yield关键字后返回相应结果，并保留函数当前的运行状态，等待下一次的调用。 由于生成器也是一个迭代器，那么它就应该支持next方法来获取下一个值 1234567891011121314151617181920212223242526def func(): n = 0 print(n) while 1: print(n,'a') n = yield n #可以通过send函数向n赋值 print(n,'b')f = func()print(f)# 生成一个&lt;generator object func at 0x030489F0&gt;f.send(None) # 启动生成器# f.send(None)遇到yield 中断00 af.send(1)# n被赋值为1，循环遇到yield中断1 b1 af.send(2)# f.send(2)，n被赋值为2，循环遇到yield中断2 b2 a 生产者-消费者模型传统的生产者-消费者模型是一个线程写消息，一个线程取消息，通过锁机制控制队列和等待，但一不小心就可能死锁。如果改用协程，生产者生产消息后，直接通过yield跳转到消费者开始执行，待消费者执行完毕后，切换回生产者继续生产，效率极高 123456789101112131415161718192021def consumer(): r = '' while True: n = yield r if not n: return print('[CONSUMER] Consuming %s...' % n) r = '200 OK'def produce(c): c.send(None) n = 0 while n &lt; 5: n = n + 1 print('[PRODUCER] Producing %s...' % n) r = c.send(n) print('[PRODUCER] Consumer return: %s' % r) c.close()c = consumer()produce(c) 执行过程分析 c = consumer(),生成generator c.send(None)其实等价于next(c),第一次执行时其实只执行到n = yield r就停止了，然后把r的值返回给调用者。用于启动生成器 yield r是一个表达式，通过send(msg)被赋值，而send(msg)是有返回值的，返回值为：下一个yield r表达式的参数，即为r。 produce一旦生产了东西，通过c.send(n)切换到consumer执行。consumer通过yield拿到消息，处理，又通过yield把结果传回。也就是说，c.send(1) 不但会给 c 传送一个数据，它还会等着下次 yield 从 c 中返回一个数据，它是有返回值的，一去一回才算完，拿到了返回的数据(200 OK)才继续下面执行。 整个流程无锁，由一个线程执行，produce和consumer协作完成任务，所以称为“协程”，而非线程的抢占式多任务。 asyncioasyncio是Python 3.4版本引入的标准库，直接内置了对异步IO的支持 12345678910111213import timeimport asyncio@asyncio.coroutinedef hello(): print('Hello world!') yield from asyncio.sleep(1) print('Hello end!')tasks = [hello(), hello()]loop = asyncio.get_event_loop()loop.run_until_complete(asyncio.wait(tasks))loop.close() 两个hello()是由同一个线程并发执行的,返回结果为 12345678# 单线程依次执行print('Hello world!')Hello world!Hello world!# 执行异步操作asyncio.sleep(1)暂停1s# 单线程依次执行print('Hello end!')Hello end!Hello end! async/awaitasync/await是自python 3.5开始后asyncio实现的新语法 把@asyncio.coroutine替换为async； 把yield from替换为await。 1234async def hello(): print("Hello start!") r = await asyncio.sleep(1) print("Hello end!") aiohttpasyncio可以实现单线程并发IO操作。如果仅用在客户端，发挥的威力不大。 如果把asyncio用在服务器端，例如Web服务器，由于HTTP连接就是IO操作，因此可以用单线程+coroutine实现多用户的高并发支持。 asyncio实现了TCP、UDP、SSL等协议，aiohttp则是基于asyncio实现的HTTP框架。 1pip install aiohttp 1234567891011121314151617181920212223import asynciofrom aiohttp import webasync def index(request): await asyncio.sleep(0.5) # 模拟io阻塞 return web.Response(body=b'&lt;h1&gt;Index&lt;/h1&gt;',content_type='text/html')async def hello(request): await asyncio.sleep(0.5) text = '&lt;h1&gt;hello, %s!&lt;/h1&gt;' % request.match_info['name'] return web.Response(body=text.encode('utf-8'),content_type='text/html')async def init(loop): app = web.Application(loop=loop) app.router.add_route('GET', '/', index) app.router.add_route('GET', '/hello/&#123;name&#125;', hello) srv = await loop.create_server(app.make_handler(), '127.0.0.1', 8000) print('Server started at http://127.0.0.1:8000...') return srvloop = asyncio.get_event_loop()loop.run_until_complete(init(loop))loop.run_forever() python并发之协程python:利用asyncio进行快速抓取玩转 Python 3.5 的 await/asyncPython 3.5 协程究竟是个啥]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python实现windows系统下自动拨号]]></title>
    <url>%2F2017%2F04%2F23%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E5%AE%9E%E7%8E%B0windows%E7%B3%BB%E7%BB%9F%E4%B8%8B%E8%87%AA%E5%8A%A8%E6%8B%A8%E5%8F%B7%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617class Adsl(object): def __init__(self): # 分别填写adsl名称，用户名与密码，dsl名称一般为宽带连接 self.name = "宽带连接" self.username = 'adsl_uname' self.password = 'adsl_upwd' def connect(self): cmd_str = "rasdial %s %s %s" % (self.name, self.username, self.password) os.system(cmd_str) time.sleep(5) def disconnect(self): cmd_str = "rasdial %s /disconnect" % self.name os.system(cmd_str) time.sleep(5) def reconnect(self): self.disconnect() self.connect()]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[requests速查手册]]></title>
    <url>%2F2017%2F04%2F23%2Fpython%2Frequests%E9%80%9F%E6%9F%A5%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[get请求网址与传递参数 12payload = &#123;'key1': 'value1', 'key2': 'value2'&#125;r = requests.get(url, params=payload) 定制请求头 12headers = &#123;'user-agent': 'my-app/0.0.1'&#125;r = requests.get(url, headers=headers) 超时 1r = requests.get(url, timeout=0.001) 短链接 1r = requests.get(url,headers=&#123;'Connection':'close'&#125;) post发送表单数据 12data = &#123;'key':'value'&#125;r = requests.post(url, data=data) 发送json数据 123# requests会自动序列化datadata = &#123;'key':'value'&#125;r = requests.post(url, json=data) 处理返回结果12# 得到jsonr.json() 代理ip12345678import requestsproxies = &#123; 'http': 'http://10.10.1.10:3128', 'https': 'http://10.10.1.10:1080',&#125;requests.get('http://example.org', proxies=proxies) 编码与解码问题我们可以通过 r.encoding 查看requests抓取文件的编码 12r = requests.get(url)print (r.encoding) 如果 Requests 检测不到正确的编码，那么你告诉它正确的是什么： 12response.encoding = 'gbk'print (response.text) 字符串在Python内部的表示是 unicode 编码，因此，在做编码转换时，通常需要以 unicode 作为中间编码，即先将其他编码的字符串解码 decode 成 unicode ，再从 unicode 编码 encode 成另一种编码。 decode 的作用是将其他编码的字符串转换成unicode编码，如 str1.decode(&#39;gb2312&#39;)，表示将 gb2312 编码的字符串 str1 转换成 unicode 编码。 encode 的作用是将 unicode 编码转换成其他编码的字符串，如 str2.encode(&#39;gb2312&#39;)，表示将 unicode 编码的字符串 str2 转换成 gb2312编码。 如果一个字符串已经是 unicode 了，再进行解码则将出错，因此通常要对其编码方式是否为 unicode 进行判断，用非 unicode 编码形式的str来 encode 会报错 保存图片123456image_url = 'http://wiki.jikexueyuan.com/project/learn-road-qt/images/32.png'import requestsresponse = requests.get(image_url)with open ('simple.png','wb') as file: file.write(response.content)]]></content>
      <tags>
        <tag>python</tag>
        <tag>requests</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VPS与虚拟主机]]></title>
    <url>%2F2017%2F04%2F23%2Flinux%2FVPS%E4%B8%8E%E8%99%9A%E6%8B%9F%E4%B8%BB%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[独立服务器： 顾名思义，就是一个躺在机房的实实在在的物理服务器 VPS： Virtual Private Server 虚拟专用服务器,一般是将一个独立服务器通过虚拟化技术虚拟成多个虚拟专用服务器 云服务器 ECS： 云服务器是一个计算，网络，存储的组合。简单点说就是通过多个CPU，内存，硬盘组成的计算池和存储池和网络的组合 虚拟主机 Vhost 虚拟主机是通过，物理服务器，VPS或者云服务器安装例如CPanel，Plesk等面板搭建的]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[difflib字符串相似性对比]]></title>
    <url>%2F2017%2F04%2F23%2Fpython%2F2.python%E6%A0%87%E5%87%86%E5%BA%93%2Fdifflib%2F</url>
    <content type="text"><![CDATA[12345import difflibstr1 = '拥有一拳就能打倒任何怪人设定的斗篷光头男的名字是'str2 = '拥有quit拳q^it就能打倒任何怪人设定的斗篷光头男的名字是'result = difflib.SequenceMatcher(None, str1, str2).ratio()print(result)]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将Python代码打包放到PyPI上]]></title>
    <url>%2F2017%2F04%2F23%2Fpython%2F4.python%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%2F%E5%B0%86Python%E4%BB%A3%E7%A0%81%E6%89%93%E5%8C%85%E6%94%BE%E5%88%B0PyPI%E4%B8%8A%2F</url>
    <content type="text"><![CDATA[什么是PyPIPyPI(Python Package Index)是python官方的第三方库的仓库。所有人都可以下载第三方库或上传自己开发的库到PyPI。PyPI推荐使用pip包管理器来下载第三方库 包的文件结构123456789101112131415foo|-- bin/ #存放项目的一些可执行文件| |-- foo|-- foo/ # 所有模块、包都应该放在此目录，程序的入口最好命名为main.py| |-- tests/ # 存放单元测试代码；| | |-- __init__.py| | |-- test_main.py| |-- __init__.py| |-- main.py|-- docs/ # 用于存放文档| |-- conf.py| |-- abc.rst|-- setup.py # 来管理代码的打包、安装、部署问题|-- requirements.txt # 存放软件依赖的外部Python包列表|-- README.rst # 项目说明文件 setup文件1234567891011121314151617181920212223242526272829303132333435363738import codecsimport osimport systry: from setuptools import setupexcept: from distutils.core import setupdef read(fname): return codecs.open(os.path.join(os.path.dirname(__file__), fname)).read()NAME = "project_name"PACKAGES = ["somefunctions",]DESCRIPTION = "package description."LONG_DESCRIPTION = read("README.rst")KEYWORDS = "test python package"AUTHOR = "your_name"AUTHOR_EMAIL = "youremail@email.com"URL = "http://your_blog/"VERSION = "1.0.1"LICENSE = "MIT"CLASSFIERS = ['License :: OSI Approved :: MIT License','Programming Language :: Python','Intended Audience :: Developers','Operating System :: OS Independent']setup( name = NAME, version = VERSION, description = DESCRIPTION, long_description =LONG_DESCRIPTION, classifiers = CLASSFIERS, keywords = KEYWORDS, author = AUTHOR, author_email = AUTHOR_EMAIL, url = URL, license = LICENSE, packages = PACKAGES, include_package_data=True, zip_safe=True,) 打包上传使用check命令查看是否存在语法问题，使用sdist进行打包。 12python setup.py checkpython setup.py sdist 发布前，需要到pypi官网注册一个账号，并在用户目录新建文件 ~/.pypirc ，并键入以下内容 123456[distutils]index-servers = pypi[pypi]repository: https://pypi.python.org/pypiusername: yournamepassword: yourpwd 执行以下内容进行打包上传，服务器返回Server response (200): OK表示上传成功 1python setup.py sdist upload 发布成功后就可以使用pip install安装你自己的python包了！ 参考资料Invalid or non-existent authentication information]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python使用requirement.txt实现自动打包部署]]></title>
    <url>%2F2017%2F04%2F23%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2FPython%E4%BD%BF%E7%94%A8requirement.txt%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E6%89%93%E5%8C%85%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[不要在本地打包依赖python一些模块是下载源码然后本机编译的。如果本机打包模块发布在别的机器上可能会出现兼容性问题。所以，统一使用pip进行模块安装打包。 使用pip进行依赖部署在开发环境中，统一运行以下命令安装依赖 1sudo pip install xx 在开发环境中，运行以下命令导出依赖 1pip freeze &gt; requirement.txt 在生产环境中，执行以下命令安装依赖 1pip install -r requirement.txt]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python规范目录结构]]></title>
    <url>%2F2017%2F04%2F23%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fpython%E8%A7%84%E8%8C%83%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[假设项目名称为Foo 123456789101112131415Foo/|-- bin/ #存放项目的一些可执行文件| |-- foo|-- foo/ # 所有模块、包都应该放在此目录，程序的入口最好命名为main.py| |-- tests/ # 存放单元测试代码；| | |-- __init__.py| | |-- test_main.py| |-- __init__.py| |-- main.py|-- docs/ # 用于存放文档| |-- conf.py| |-- abc.rst|-- setup.py # 来管理代码的打包、安装、部署问题|-- requirements.txt # 存放软件依赖的外部Python包列表|-- README # 项目说明文件 python基础6–目录结构]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Python和win32编程范例]]></title>
    <url>%2F2017%2F04%2F23%2Fpython%2F4.python%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%2F%E5%88%A9%E7%94%A8Python%E5%92%8Cwin32%E7%BC%96%E7%A8%8B%E8%8C%83%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[学习pywin32之前，我们先要了解一些概念 句柄是一个32位整数，在windows中标记对象用，类似一个dict中的key 消息是windows应用的重要部分，比如给一个按钮发送BN_CLICKED,按钮就会知道自己被点击了 为了方面查找目标窗口的句柄，可以下载一个微软自家的Spy++ 使用pip安装pywin32 1pip install pypiwin32 快速开始 123import win32apiimport win32conwin32api.MessageBox(win32con.NULL, 'Python 你好！', '你好', win32con.MB_OK) 运行以上程序你会得到一个python版本的弹窗，是不是非常简单呢 参考文档按需定制一个按键精灵Python在Windows下系统编程初步]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何区分真实点击与js的click]]></title>
    <url>%2F2017%2F04%2F23%2F%E5%89%8D%E7%AB%AF%2F%E5%A6%82%E4%BD%95%E5%8C%BA%E5%88%86%E7%9C%9F%E5%AE%9E%E7%82%B9%E5%87%BB%E4%B8%8Ejs%E7%9A%84click%2F</url>
    <content type="text"><![CDATA[区分真实与虚拟点击使用event.isTrusted以及e.clientX, e.pageX可以区分真实点击与js虚拟点击 1234elem = document.querySelector('#elem_id')elem.addEventListener('click',function(e)&#123; console.log(e.isTrusted,e.clientX,e.clientY,e.pageX,e.pageY)&#125;) 鼠标真实点击，控制台输出了 true 60 60通过js控制的elem.click()，控制台输出false 0 0 使用javascript尝试破解对于坐标，可以通过自定义事件解决，但是坐标范围比较麻烦而且对于浏览器底层的event.isTrusted无解所以总体来说，js无法解决此问题 12345678//event = new MouseEvent(typeArg, mouseEventInit);var event = new MouseEvent('click', &#123; 'screenX': 10, 'screenY': 10, 'clientX': 10, 'clientY': 10&#125;);elem.dispatchEvent(event) 输出结果为 false 10 10 10 10 使用selinium模拟真实点击通过自动化测试软件selinium，注入检测用js代码并执行点击操作 1browser.find_element_by_id("elem_id").click() 查看控制台输出结果为：true 633 358 633 458 网页中，鼠标点击与javascript的click事件怎么区分javascript自定义事件(event)MouseEvent()]]></content>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nodejs路径略解与path模块]]></title>
    <url>%2F2017%2F04%2F23%2Fnodejs%2Fnodejs%E8%B7%AF%E5%BE%84%E7%95%A5%E8%A7%A3%E4%B8%8Epath%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[绝对路径与相对路径nodejs中有三种绝对路径和两种相对路径 __dirname: 总是返回被执行的 js 所在文件夹的绝对路径 __filename: 总是返回被执行的 js 文件的绝对路径 process.cwd(): 总是返回运行 node 命令时所在的文件夹的绝对路径 ./ ../ 举个例子，我有一个模块，文件结构如下： 12345app/ -lib/ -common.js -model -task.js 在app文件夹执行cmd命令 node task.js 结果如下： 123__dirname : /Users/app/model__filename : /Users/app/model/task.jsprocess.cwd() : /Users/app 在model文件夹执行cmd命令 node task.js 结果如下： 123__dirname : /Users/app/model__filename : /Users/app/model/task.jsprocess.cwd() : /Users/app 对于 ./ 有如下结论： 在 require() 中使用是跟 __dirname 的效果相同，不会因为启动脚本的目录不一样而改变，在其他情况下跟 process.cwd() 效果相同，是相对于启动脚本所在目录的路径。 因此只有在 require() 时才使用相对路径(./, ../) 的写法，其他地方一律使用绝对路径，如下： 1234// 当前目录下path.dirname(__filename) + '/task.js';// 相邻目录下path.resolve(__dirname, '../lib/common.js'); 参考资料浅析 NodeJs 的几种文件路径]]></content>
      <tags>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[if __name__ == '__main__'浅析]]></title>
    <url>%2F2017%2F04%2F23%2Fpython%2F1.python%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2Fif%20__name__%20%3D%3D%20'__main__'%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[1234def main(): print "we are in %s"%__name__if __name__ == '__main__': main() 其中 __name__ == &#39;__main__&#39;: 的作用是：当该文件作为脚本运行时，main()函数可以运行当该文件作为模块被其他文件导入时，main()函数不会运行,避免了模块重复运行]]></content>
      <tags>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[npm常用命令与nodejs版本更新]]></title>
    <url>%2F2017%2F04%2F23%2Fnodejs%2Fnpm%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%B8%8Enodejs%E7%89%88%E6%9C%AC%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[更新nodejsnode有一个模块叫n，是专门用来管理node.js的版本的。 12npm install -g nn stable # 升级node.js到最新稳定版 可惜不支持windows，windows直接到官网下载即可 npm 常用命令1234567891011npm -v #显示版本，检查npm 是否正确安装。npm install express #安装express模块npm install -g express #全局安装express模块npm list #列出已安装模块npm show express #显示模块详情npm update #升级当前目录下的项目的所有模块npm update express #升级当前目录下的项目的指定模块npm update -g express #升级全局安装的express模块npm uninstall express #删除指定的模块npm linknpm unlink]]></content>
      <tags>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用hexo搭建博客并托管在github]]></title>
    <url>%2F2017%2F04%2F23%2F%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%2F%E4%BD%BF%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[Hexo简介Hexo 是一个简单地、轻量地、基于Node的一个静态博客框架，可以方便的生成静态网页托管在github和Heroku上。 使用Hexo之前，你首先需要做好准备工作:一个github账号，安装好nodejs，安装好git，当你完成上述内容后，就可以开始安装hexo了。 快速开始(本项目) 安装最新版本的 node.js ，注意安装时必须勾选 add to path 运行npm install hexo-cli -g 安装命令行工具 运行git clone git@github.com:gaianote/gaianote.github.io.git，将source分支clone到本地 cd到项目根目录，运行 npm install 安装相关依赖 hexo server 启动服务器，可以在本地浏览blog 使用http://localhost:4000/admin/访问管理平台，在线编辑blog 云端推送当我们需要更新blog时，运行以下命令即可完成推送本地blog到github中 12$ hexo generate$ hexo deploy 通过以上命令，系统会自动执行以下操作： 生成html静态文件 将生成的静态文件发布到master分支上(无需使用git切换到master分支) 将blog源文件分支提交到source分支上(shelljs实现) 注意事项本博客程序中的package依赖固定的版本号，当你直接更新某个主题或者依赖包时，可能会导致运行故障。因此，在博客展示正常的情况下，不建议对版本进行更新。 安装hexo打开cmd，依次执行以下命令 1234npm install hexo-cli -ghexo init blogcd blognpm install 基本使用新建文章 1hexo new 文章标题 启动服务器 1hexo server 执行hexo server后,可以在 http://localhost:4000/ 查看hexo创建好的博客 发布到github 发布到服务器前，需要通过hexo generate命令对所有的文章做静态化处理，生成html，css，js等文件 推送到github需要安装hexo-deployer，并且配置好_config.yml中的Deployment项，然后使用hexo deploy推送到github 1npm install hexo-deployer-git -S 12hexo generatehexo deploy 常用命令总结 1234hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy Hexo的配置文件结构12345678910111213gaianote.github.io||-public # 存放hexo生成的html文件|-dcaffolds # 模板文件夹，新建文章时，Hexo 会根据 scaffold 来建立文件| |-draft.md| |-page.md| |-post.md # 默认模板post|-source # 资源文件夹是存放用户资源的地方，Markdown 和 HTML 文件会被解析并放到 public 文件夹，而其他文件会被拷贝过去| |-_post # 文件箱，新建的文章存放在这里,除 _posts 文件夹之外，开头命名为 _ (下划线)的文件/ 文件夹和隐藏的文件将会被忽略|-themes # 存放主题文件，Hexo 会根据主题来生成静态页面| |-landscape ：默认的主题文件夹|-_config.yml # 全局配置文件，每次更改要重启服务| 配置文件 _config.yml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# Site 站点配置title: 李云鹏的个人博客 # 网站标题subtitle:description:author: 李云鹏language: zh-CNtimezone:# URL 链接配置url: http://gaianote.github.ioroot: /permalink: :year/:month/:day/:title/permalink_defaults:# Directory 目录配置source_dir: source # 资源文件夹，这个文件夹用来存放内容public_dir: public # 公共文件夹，这个文件夹用于存放生成的站点文件tag_dir: tags # 标签文件夹archive_dir: archives # 归档文件夹category_dir: categories # 分类文件夹code_dir: downloads/code # Include code 文件夹i18n_dir: :lang # 国际化文件夹skip_render: # 跳过指定文件的渲染，您可使用 glob 来配置路径# Writing 写作配置new_post_name: :title.md # 新文章的文件名称default_layout: # 默认布局titlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0 # 把文件名称转换为 1 小写或 2 大写render_drafts: false # 显示草稿post_asset_folder: false # 是否启动资源文件夹relative_link: false # 把链接改为与根目录的相对位址future: truehighlight: # 代码块的设置 enable: true line_number: true auto_detect: false tab_replace:# Category &amp; Tag # 分类 &amp; 标签default_category: uncategorized # 默认分类category_map: # 分类别名tag_map: # 标签别名# Date / Time format 时间和日期date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination 分页per_page: 10 # 每页显示的文章量 (0 = 关闭分页功能)pagination_dir: page # 分页目录# Extensions 扩展theme: hexo-theme-yilia # 当前主题名称# Deployment # 部署到githubdeploy: type: git repo: https://github.com/gaianote/gaianote.github.io.git branch: master 主题配置123456789101112gaianote.github.io||-themes # 存放主题文件，Hexo 会根据主题来生成静态页面| |-landscape # 默认的主题文件夹| |-hexo-theme-yilia # 笔者使用的主题| |-languages| |-layout| |-source| |-source-src| |-_config.yml # 主题的全局配置| |-README.md # 主题的使用说明| 在使用主题过程中，我们结合 _config.yml和README.md 对主题进行设置即可 头像与favoicon放在public文件夹内即可生效 文章与草稿属性文章可以拥有如下属性,在文章的开头使用 ===== 与正文分隔 12345678910===========layout Layout ：post或pagetitle 文章的标题date 创建日期 ：文件的创建日期updated 修改日期 ：文件的修改日期comments 是否开启评论 ：true|falsetags 标签categories 分类permalink url中的名字 : 文件名=========== 分类和标签 1234567===========categories:- 日记tags:- Hexo- node.js=========== 草稿当你撰写好一篇文章，并未打算发布时，需要使用草稿功能 新建草稿 1hexo new draft "new draft" 新建草稿会在source/_drafts目录下生成一个new-draft.md文件。但是这个文件不被显示在页面上，链接也访问不到。 预览草稿 1hexo server --drafts 发布草稿 1hexo publish [layout] &lt;filename&gt; 保存 Hexo 博客源码到 GitHub因为 master 分支只保存了public 文件夹中的静态文件，所以需要创建一个分支来保存你的博客的源代码 使用git管理source分支 在github上新建一个分支source，并将source设置为默认分支，通过source分支使用git管理源文件 配置好 .gitnore 文件，添加规则 public，因为public使用hexo deploy管理，无需重复添加 将远程仓库克隆到本地，然后连接远程仓库 12git clone https://github.com/gaianote/gaianote.github.iogit remote add origin https://github.com/gaianote/gaianote.github.io 以后每次使用时，直接键入以下命令即可 123git add .git commit -m "update"git push origin source 使用 hexo deploy 管理master分支 使用 hexo deploy 管理master分支，直接将静态文件发布到master分支上(无需使用git切换到master分支) 1hexo deploy 实现流程自动化hexo写博客非常享受，但是有可以改进的地方，比如每次新建文章时，都需要在post文件中查找新建立的文档，并且每次写完文章需要使用hexo和git备份两次，比较繁琐，我们可以使用shelljs实现自动化备份 通过查阅Hexo文档 ，找到了Hexo的主要事件，见下表： 事件名 事件发生时间 deployBefore 在部署完成前发布 deployAfter 在部署成功后发布 exit 在 Hexo 结束前发布 generateBefore 在静态文件生成前发布 generateAfter 在静态文件生成后发布 new 在文章文件建立后发布 于是我们就可以通过监听Hexo的 deployAfter 事件，待上传完成之后自动运行Git备份命令，从而达到自动备份的目的。 通过监听Hexo 的 new 事件，新建文章同时使用编辑器打开文档 首先，我们需要安装模块shelljs 1npm install --save shelljs 待到模块安装完成，在博客根目录的 scripts 文件夹下新建一个js文件，文件名随意取；如果没有 scripts 目录，请新建一个。 然后在脚本内键入以下内容 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950require('shelljs/global');var path = require('path');/* hexo deploy 时自动执行git push */try &#123; hexo.on('deployAfter', function() &#123;//当deploy完成后执行备份 run(); &#125;);&#125; catch (e) &#123; console.log("产生了一个错误&lt;(￣3￣)&gt; !，错误详情为：" + e.toString());&#125;function run() &#123; if (!which('git')) &#123; echo('Sorry, this script requires git'); exit(1); &#125; else &#123; echo("======================Auto Backup Begin==========================="); cd(process.cwd()); if (exec('git add --all').code !== 0) &#123; echo('Error: Git add failed'); exit(1); &#125; if (exec('git commit -m "update"').code !== 0) &#123; echo('Error: Git commit failed'); exit(1); &#125; if (exec('git push origin source').code !== 0) &#123; echo('Error: Git push failed'); exit(1); &#125; echo("==================Auto Backup Complete============================") &#125;&#125;/* 新建文章自动打开编辑器 */try &#123; hexo.on('new', function(data) &#123; exec(data.path) &#125;);&#125; catch (e) &#123; console.log("产生了一个错误&lt;(￣3￣)&gt; !，错误详情为：" + e.toString());&#125; 这样，我们在使用Hexo命令时就可以自动触发git以及打开文章的操作了！ 集成搜索服务Next官网–&gt;第三方服务–&gt;搜索服务 中已经有详细的介绍了,主题内置了4种搜索方式: Swiftype 微搜索 Local Search Algolia 由于自己的博客主要用于工作日志的记录,以及工作时的快速定位查询,因此选择了第三种方式Local Search,即本地搜索. 安装搜索插件在Hexo根目录执行如下命令 1$ npm install hexo-generator-searchdb --save 编辑站点配置文件在站点配置文件(Hexo根目录的配置文件)_config.yml中添加 12345search: path: search.xml field: post format: html limit: 10000 编辑主题配置文件,启用本地搜索在主题配置文件(Next主题根目录的配置文件)_config.yml中启用 Local search12local_search: enable: true 注:以上内容在Next官网本地搜索模块有详细介绍,在此只为记录. hexo yilia主题添加文章访问量统计hexo添加访问量统计功能可以用百度的站长统计、leancloud，还有不蒜子，这里我用的不蒜子，感觉挺简单的。 引入不蒜子1&lt;script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"&gt;&lt;/script&gt; 这段代码可以写在footer.ejs里或者header.ejs里或者layout.ejs里，我用的yilia主题，放在/themes/yilia/layout/_partial/footer.ejs 添加站点访问量通常站点的总访问量会显示在footer的位置，所以我们可以在footer.ejs里加上如下标签： 1&lt;span id="busuanzi_container_site_uv"&gt; 本站访客数&lt;span id="busuanzi_value_site_uv"&gt;&lt;/span&gt;人次&lt;/span&gt; 计算访问量的方法有两种：算法a：pv的方式，单个用户连续点击n篇文章，记录n次访问量。算法b：uv的方式，单个用户连续点击n篇文章，只记录1次访客数。我用的是uv的方式，大家自行选择即可。 添加文章访问量文章的访问量显示在文章里面，所以在article.ejs里加上文章访问量的标签： 1&lt;span id="busuanzi_container_page_pv"&gt; 本文总阅读量&lt;span id="busuanzi_value_page_pv"&gt;&lt;/span&gt;次&lt;/span&gt; 我们直接就这样放在yilia主题中，首页也会显示该网页的访问量，没法正常使用，所以我加一个判断，如果是首页不显示该文章的访问量，下面这段代码添加在/themes/yilia/layout/_partial/article.ejs的header的日期后面： 1234567891011&lt;%if ( !index )&#123; %&gt;&lt;span class="archive-article-date"&gt; 阅读量 &lt;span id="busuanzi_value_page_pv"&gt;&lt;/span&gt;&lt;/span&gt;&lt;% &#125;%&gt; VPS gengrator 命令导致 hexo 被 killed 解决方案12345free -m dd if=/dev/zero of=/swap bs=4096 count=1572864mkswap /swapswapon /swapecho &quot;LABEL=SWAP-sda /swap swap swap defaults 0 0&quot; &gt;&gt; /etc/fstab 这样会在根目录建立一个 swap 文件,用于虚拟内存使用 dd if=/dev/zero of=/swap bs=4096 count=1572864命令时,会卡一会,等待即可,然后就可以正常使用 hexo g 命令了 为blog添加评论功能 next + gitalk配置方法1.我们先来找到Hexo\themes\next\layout\_third-party\comments这个目录下，新建一个叫gitalk.swig的文件，里面写入下列代码： 1234567891011121314151617&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/gitalk/dist/gitalk.css&quot;&gt; &lt;script src=&quot;https://unpkg.com/gitalk/dist/gitalk.min.js&quot;&gt;&lt;/script&gt; //这两句是调用作者的css和js文件 &lt;script type=&quot;text/javascript&quot;&gt; var gitalk = new Gitalk(&#123; //这里面的参数我们会在另一个文件中配置 clientID: &apos;&#123;&#123;theme.gitalk.clientID&#125;&#125;&apos;, clientSecret: &apos;&#123;&#123;theme.gitalk.clientSecret&#125;&#125;&apos;, repo: &apos;&#123;&#123;theme.gitalk.repo&#125;&#125;&apos;, owner: &apos;&#123;&#123;theme.gitalk.owner&#125;&#125;&apos;, admin: &apos;&#123;&#123;theme.gitalk.admin&#125;&#125;&apos;, id: location.pathname, distractionFreeMode: &apos;&#123;&#123;theme.gitalk.distractionFreeMode&#125;&#125;&apos;, &#125;) gitalk.render(&apos;gitalk-container&apos;) &lt;/script&gt;&#123;% endif %&#125; 同目录下的index.swig文件中，添加以下代码：1&#123;% include &apos;gitalk.swig&apos; %&#125; 目的是使主页面包含上面新建的文件 2.然后找到\Hexo\themes\next\layout\_partials文件夹的comments.swig文件：这是评论模块的配置文件。在倒数第二层的if-else判断里加入： 12&#123;% elseif theme.gitalk.enable %&#125; &lt;div id=&quot;gitalk-container&quot;&gt;&lt;/div&gt; 3.修改next的_config.yaml文件，添加以下内容: 12345678gitalk: enable: true clientID: 5a3c536053971... # your client id clientSecret: 2c5c5bde68afec70ae... # your client secret repo: gaianote.github.io # your github repo owner: gaianote # your github id admin: gaianote # your github id pagerDirection: first 解决 Error: Validation Failed 报错问题最近github更新过后，issue的名字不能超过50，所以会出现Error: Validation Failed报错,解决方法如下: 在 themes\NexT\source\js\src\ 下添加 md5.min.js 然后修改修改gitalk.swig，引入md5.min.js，对id值md5编码化 123456789101112131415161718192021&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/gitalk/dist/gitalk.css&quot;&gt; &lt;script src=&quot;https://unpkg.com/gitalk/dist/gitalk.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;/js/src/md5.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; var gitalk = new Gitalk(&#123; clientID: &apos;&#123;&#123; theme.gitalk.ClientID &#125;&#125;&apos;, clientSecret: &apos;&#123;&#123; theme.gitalk.ClientSecret &#125;&#125;&apos;, repo: &apos;&#123;&#123; theme.gitalk.repo &#125;&#125;&apos;, owner: &apos;&#123;&#123; theme.gitalk.githubID &#125;&#125;&apos;, admin: [&apos;&#123;&#123; theme.gitalk.adminUser &#125;&#125;&apos;], id: md5(location.pathname), distractionFreeMode: &apos;&#123;&#123; theme.gitalk.distractionFreeMode &#125;&#125;&apos; &#125;) gitalk.render(&apos;gitalk-container&apos;) &lt;/script&gt;&#123;% endif %&#125; 参考资料hexo官网next官网Hexo 入门指南简洁轻便的博客平台: Hexo详解20分钟教你使用hexo搭建github博客自动备份Hexo博客源文件hexo-yilia主题添加文章访问量统计]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
</search>
